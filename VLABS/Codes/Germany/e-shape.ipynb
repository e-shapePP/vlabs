{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a7613658",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n5/09/2022\\n\\nGetting code ready  to be used on VLABS\\n\\nPdrive: 11204971\\n\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This scripts runs post-processing steps for Eddy covariance data coming\n",
    "in one file in the format of europe-fluxdata.eu. This format is very similar\n",
    "to the ICOS format (the only known difference is the unit of pressure,\n",
    "which is hPa in europe-fluxdata.eu and kPa in ICOS).\n",
    "\n",
    "The script covers the following steps:\n",
    "- spike / outlier detection with mean absolute deviation filter\n",
    "  after Papale et al. (Biogeosci, 2006)\n",
    "- ustar filtering after Papale et al. (Biogeosci, 2006)\n",
    "- carbon flux partitioning with the nighttime method\n",
    "  of Reichstein et al. (Global Change Biolo, 2005) and\n",
    "  the daytime method of Lasslop et al. (Global Change Biolo, 2010)\n",
    "- gap filling with marginal distribution sampling (MDS)\n",
    "  of Reichstein et al. (Global Change Biolo, 2005)\n",
    "- flux error estimates using MDS after Lasslop et al. (Biogeosci, 2008)\n",
    "\n",
    "The script is controlled by a config file in Python's standard configparser\n",
    "format. The config file includes all possible parameters of used routines.\n",
    "Default parameter values follow the package REddyProc where appropriate. See\n",
    "comments in config file for details.\n",
    "\n",
    "The script currently flags on input all NaN values and given *undefined*\n",
    "values. Variables should be set to *undefined* in case of other existing flags\n",
    "before calling the script. Otherwise it should be easy to set the appropriate\n",
    "flags in the pandas DataFrame dff for the flags after its creation around line\n",
    "160.\n",
    "\n",
    "The output file can either have all flagged variables set to *undefined*\n",
    "and/or can include flag columns for each variable (see config file).\n",
    "\n",
    "Note, ustar filtering needs at least one full year.\n",
    "\n",
    "Examples\n",
    "--------\n",
    "python postproc_europe-fluxdata.py hesseflux_example.cfg\n",
    "\n",
    "History\n",
    "-------\n",
    "Written, Matthias Cuntz, April 2020\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "27/09/2021\n",
    "\n",
    "Integration of Footprint predictor model and satellite images from google earth engine \n",
    "to derive empirical remote sensing models and monthly and annual maps.\n",
    "\n",
    "Written, Mario Alberto Fuentes Monjaraz, October 2021\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "5/09/2022\n",
    "\n",
    "Getting code ready  to be used on VLABS\n",
    "\n",
    "Pdrive: 11204971\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "13dbf1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code requires Python 3.6 and the next packages\n",
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "#!pip install hesseflux \n",
    "#!pip install pyproj\n",
    "#!pip install earthengine-api\n",
    "#!pip install statsmodels\n",
    "#!pip install sklearn\n",
    "#!pip install folium\n",
    "#!pip install altair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "723f1236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Python packages used in the code\n",
    "from __future__ import division, absolute_import, print_function\n",
    "import time as ptime\n",
    "import sys\n",
    "import configparser\n",
    "import os.path\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hesseflux as hf\n",
    "import altair as alt\n",
    "import math\n",
    "from pyproj import Proj\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import ee\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "import folium\n",
    "from folium import plugins\n",
    "from IPython.display import Image\n",
    "from datetime import date, datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pyjams as pj\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "23f20c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1a)  Opening configuration file\n",
      "1b)  Reading configuration file\n"
     ]
    }
   ],
   "source": [
    "#Function to identify columns with specific beggining \n",
    "def _findfirststart(starts, names):\n",
    "    \"\"\"\n",
    "    Function that finds variables on the head of a table indicating the name or label of the variable  \n",
    "    and creates a list of the variables located. The \n",
    "    \"\"\"\n",
    "    hout = []\n",
    "    for hh in starts:\n",
    "        for cc in names:\n",
    "            if cc.startswith(hh):\n",
    "                hout.append(cc)\n",
    "                break\n",
    "    return hout\n",
    "\n",
    "#The workflow start here\n",
    "if __name__ == '__main__':\n",
    "    t1 = ptime.time()\n",
    "    \n",
    "    #*********************************************************************************************************************************************************************\n",
    "    #1)   Read configuration file\n",
    "    print('1a)  Opening configuration file')\n",
    "    \n",
    "    #1.a) Read from command-line interpreter (It must include in the cosole \"GPP.py Configs.cfg\" located in the same file)\n",
    "    #if len(sys.argv) <= 1:\n",
    "    #raise IOError('Input configuration file must be given.')\n",
    "    #configfile = sys.argv[1]                                                                          #Change 1. Read configuration (different methods)    \n",
    "                                                                                                       #For this option a Configs folder is required in the main directory\n",
    "    #1.b)Read from directory path\n",
    "    #configfile = 'C:/Users/Administrador/OneDrive/Documentos/MSc Thesis/Configs/DNP_e_shape_configuration.cfg'  \n",
    "    #configfile = 'C:/Users/Usuario/Documents/Pdrive/Final python codes Anna/Codes/'\n",
    "    configfile = 'Configs/Configuration_file.cfg'                                                \n",
    "    \n",
    "    #1.c)Read from gui window\n",
    "    #configfile = hf.files_from_gui(initialdir='.', title='configuration file')\n",
    "    \n",
    "    config = configparser.ConfigParser(interpolation=None)                                             #Constructor\n",
    "    config.read(configfile)                                                                            #Read configuration file with the constructor\n",
    "\n",
    "    #1b) Read file to retrieve file directories and model's marameters \n",
    "    print('1b)  Reading configuration file')\n",
    "    \n",
    "    # file path\n",
    "    ID            = config['GENERAL'].get('ID',  \".\")\n",
    "    inputdir      = config['GENERAL'].get('inputdir',  \".\")                                            #Change 2. Add inputdir to read folder from Data folder\n",
    "    outputdir     = config['GENERAL'].get('outputdir', \".\")\n",
    "    \n",
    "    # meteorological data\n",
    "    meteo_file    = config['METHEO'].get('meteo_file', \".\")\n",
    "    \n",
    "    # program switches                                                                                 #Activates each module of the workflow)\n",
    "    #------------------------------------------------------------\n",
    "    outlier   = config['POSTSWITCH'].getboolean('outlier',   True)\n",
    "    ustar     = config['POSTSWITCH'].getboolean('ustar',     True)\n",
    "    ustar_non_annual  = config['POSTSWITCH'].getboolean('ustar_non_annual',     True)                  #Change 3. Add method ustar_non_annual  to compute the u* filter with a given threshold.\n",
    "    partition = config['POSTSWITCH'].getboolean('partition', True)                                     #ustar_non_annual  method has to be used instead of ustar when there is not data \n",
    "    fill      = config['POSTSWITCH'].getboolean('fill',      True)                                     #for a full date to calculate automatically a threshold\n",
    "    fluxerr   = config['POSTSWITCH'].getboolean('fluxerr',   True)\n",
    "    #------------------------------------------------------------\n",
    "    daily_gpp              =  config['POSTSWITCH'].getboolean('daily_gpp',                      True)  #Change 4. All the modules \n",
    "    #------------------------------------------------------------\n",
    "    climatological_footprint  =  config['POSTSWITCH'].getboolean('climatological_footprint ',   True) \n",
    "    #------------------------------------------------------------\n",
    "    vegetation_indices        =  config['POSTSWITCH'].getboolean('vegetation_indices',          True)\n",
    "    #------------------------------------------------------------\n",
    "    environmental_variables_station     =  config['POSTSWITCH'].getboolean('environmental_variables_station',           True)\n",
    "    environmental_variables_satellite   =  config['POSTSWITCH'].getboolean('environmental_variables_satellite',         True)\n",
    "    tower_observations                  =  config['POSTSWITCH'].getboolean('tower_observations',                        True)\n",
    "    rei_gpp_switch                      =  config['POSTSWITCH'].getboolean('rei_gpp_switch ',                           True)\n",
    "    fal_gpp_switch                      =  config['POSTSWITCH'].getboolean('fal_gpp_switch ',                           False)\n",
    "    las_gpp_switch                      =  config['POSTSWITCH'].getboolean('las_gpp_switch ',                           False)\n",
    "    \n",
    "    df_rainfall_station_switch = config['POSTSWITCH'].getboolean('df_rainfall_station_switch',      True)\n",
    "    df_meteo_station_switch    = config['POSTSWITCH'].getboolean('df_meteo_station_switch',         True)\n",
    "    df_rainfall_CHIRPS_switch  = config['POSTSWITCH'].getboolean('df_rainfall_CHIRPS_switch',       True)\n",
    "    df_temp_MODIS_switch       = config['POSTSWITCH'].getboolean('df_temp_MODIS_switch',            True)\n",
    "    df_meteo_tower_switch      = config['POSTSWITCH'].getboolean('df_meteo_tower_switch',           True)\n",
    "    \n",
    "    #------------------------------------------------------------\n",
    "    correlation_analysis        =  config['POSTSWITCH'].getboolean('correlation_analysis',          True)\n",
    "    correlation_analysis_simple =  config['POSTSWITCH'].getboolean('correlation_analysis',          True)\n",
    "    calibration_validation      =  config['POSTSWITCH'].getboolean('calibration_validation',        True) \n",
    "    #------------------------------------------------------------\n",
    "    timeseries_thirty           =  config['POSTSWITCH'].getboolean('timeseries_thirty',        True) \n",
    "    timeseries_fifteen          =  config['POSTSWITCH'].getboolean('timeseries_fifteen',       True) \n",
    "    mapping_GPP                 =  config['POSTSWITCH'].getboolean('mapping_GPP',              True)\n",
    "    classification_maps         =  config['POSTSWITCH'].getboolean('classification_maps',      True)\n",
    "    mapping_GPP_thirty          =  config['POSTSWITCH'].getboolean('mapping_GPP_thirty',       True)\n",
    "    mapping_GPP_fifteen         =  config['POSTSWITCH'].getboolean('mapping_GPP_fifteen',      True)\n",
    "     \n",
    "    # input file format\n",
    "    eufluxfile  = config['POSTIO'].get('eufluxfile',  '')\n",
    "    timeformat  = config['POSTIO'].get('timeformat', '%Y%m%d%H%M')\n",
    "    sep         = config['POSTIO'].get('sep',        ',')\n",
    "    skiprows    = config['POSTIO'].get('skiprows',   '')\n",
    "    undef       = config['POSTIO'].getfloat('undef', -9999.)\n",
    "    swthr       = config['POSTIO'].getfloat('swthr', 10.)\n",
    "    outputfile  = config['POSTIO'].get('outputfile'  '')\n",
    "    outundef       = config['POSTIO'].getboolean('outundef',    False)\n",
    "    outflagcols    = config['POSTIO'].getboolean('outflagcols', False)\n",
    "\n",
    "    # input file variables \n",
    "    carbonflux     = config['POSTVAR'].get('carbonflux',        'FC')                                  #Change 4. Add variable to identify the name of the carbon fluxes to compute \n",
    "                                                                                                       #Carbon flux name to process in the code (e.g. NEE, FC, FC_1)                                                                                              #This change can be done for all the column names of the input file \n",
    "    # remove information on a variable \n",
    "    remove_SW_IN   = config['POSTVAR'].getboolean('remove_SW_IN', False)                               #remove_SW_IN is only useful for DNP where the SW data was not \n",
    "                                                                                                       #for the year of analysis.\n",
    "    # mad parameters\n",
    "    nscan = config['POSTMAD'].getint('nscan', 15)\n",
    "    nfill = config['POSTMAD'].getint('nfill',  1)\n",
    "    z     = config['POSTMAD'].getfloat('z',    7)\n",
    "    deriv = config['POSTMAD'].getint('deriv',  2)\n",
    "    \n",
    "    # ustar parameters\n",
    "    ustarmin       = config['POSTUSTAR'].getfloat('ustarmin',    0.1)\n",
    "    nboot          = config['POSTUSTAR'].getint('nboot',         1)\n",
    "    plateaucrit    = config['POSTUSTAR'].getfloat('plateaucrit', 0.95)\n",
    "    seasonout      = config['POSTUSTAR'].getboolean('seasonout', False)                                #Change 5. Add these parameters in the configuration file                      \n",
    "    applyustarflag = config['POSTUSTAR'].getboolean('applyustarflag', True)\n",
    "\n",
    "    # gap-filling parameters\n",
    "    sw_dev  = config['POSTGAP'].getfloat('sw_dev',  50.)\n",
    "    ta_dev  = config['POSTGAP'].getfloat('ta_dev',  2.5)\n",
    "    vpd_dev = config['POSTGAP'].getfloat('vpd_dev', 5.0)\n",
    "    longgap = config['POSTGAP'].getint('longgap',   60)\n",
    "    \n",
    "    # partitioning parameters \n",
    "    nogppnight = config['POSTPARTITION'].getboolean('nogppnight', False)\n",
    "    \n",
    "    # daily gpp computation parameters\n",
    "    carbonfluxlimit  = config['DAILYGPP'].getint('carbonfluxlimit',         100)                       #Maximun daily GPP (Look in the literature)\n",
    "    respirationlimit = config['DAILYGPP'].getint('respirationlimit',        100)                       #Minimun daily respiration (Look in the literature)\n",
    "    rolling_window_gpp   = config['DAILYGPP'].getint('rolling_window_gpp',    3)\n",
    "    \n",
    "    # climatological footprint parameters\n",
    "    altitude                        = config['CLIMATOLOGICAL'].getfloat('altitude',                      '')                                \n",
    "    latitude                        = config['CLIMATOLOGICAL'].getfloat('latitude',                      '')                        \n",
    "    longitude                       = config['CLIMATOLOGICAL'].getfloat('longitude',                     '')                        \n",
    "    canopy_height                   = config['CLIMATOLOGICAL'].getfloat('canopy_height ',                '')                           \n",
    "    displacement_height             = config['CLIMATOLOGICAL'].getfloat('displacement_height',           '')                          \n",
    "    roughness_lenght                = config['CLIMATOLOGICAL'].getfloat('roughness_lenght ',             '')                           \n",
    "    instrument_height_anenometer    = config['CLIMATOLOGICAL'].getfloat('instrument_height_anenometer',  '')\n",
    "    instrument_height_gas_analyzer  = config['CLIMATOLOGICAL'].getfloat('instrument_height_gas_analyzer','')\n",
    "    \n",
    "    domaint_var  = config['CLIMATOLOGICAL'].get('domaint_var','-2000,2000.,-2000.,2000.').split(',')\n",
    "    nxt_var      = config['CLIMATOLOGICAL'].get('nxt_var',                        '1000').split(',')\n",
    "    rst_var      = config['CLIMATOLOGICAL'].get('rst_var',             '20.,40.,60.,80.').split(',')\n",
    "    for i in range(0, len(domaint_var)):\n",
    "        domaint_var[i] = float(domaint_var[i])\n",
    "    for i in range(0, len(rst_var)):\n",
    "        rst_var[i] = float(rst_var[i])\n",
    "    \n",
    "    projection_site_UTM_zone        = config['CLIMATOLOGICAL'].get('projection_site_UTM_zone', '')          #Change 5.1 Projection of climatological footprint\n",
    "    #projection_site                 = '+proj=utm +zone='+projection_site_UTM_zone+' +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs'       \n",
    "    projection_site                 = '+proj=utm +zone='+projection_site_UTM_zone+' +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs'       \n",
    "    boundary_layer_height           = config['CLIMATOLOGICAL'].getfloat('boundary_layer_height',     '1500')\n",
    "    \n",
    "    # vegetation indices parameters \n",
    "    max_cloud_coverage              = config['VI'].getint('max_cloud_coverage',         100)                 #Default: No filter, all images available.\n",
    "    crs                             = config['VI'].get('crs',                            '')                 #EPSG:4326\n",
    "    ndviMask                        = config['VI'].getfloat('ndviMask',                -100)                 #Default: No mask\n",
    "    mndviMask                       = config['VI'].getfloat('mndviMask',               -100)                 #Default: No mask\n",
    " \n",
    "    # environmental vegetation parameters\n",
    "    rolling_window_ev_meteo         = config['EV'].getint('rolling_window_ev_meteo',            3)\n",
    "    rolling_window_ev_meteo_sat     = config['EV'].getint('rolling_window_ev_meteo_sat',        3)\n",
    "    rolling_window_gpp_MODIS        = config['EV'].getint('rolling_window_gpp_MODIS',           3)\n",
    "    precipitation_data              = config['EV'].get('precipitation_data',                   '').split(',')\n",
    "    \n",
    "    # mapping parameters\n",
    "    ecosystem_extension             = config['MAPS'].getint('ecosystem_extension ',          5000) \n",
    "    number_clusters                 = config['MAPS'].getint('number_clusters ',                 4) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f731118c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2)   Formatting data frames\n",
      "      Read data:  \\NEE\\NEE_data_Germany.txt\n",
      "      Formating data:  ['Input\\\\NEE\\\\NEE_data_Germany.txt']\n",
      "     Computation setting data frames in  0 [seconds]\n",
      "3)   Spike detection\n",
      "      Using: ['NEE']\n",
      "     Computation outlier detection in  1 [seconds]\n",
      "4)   u* filtering (less than 1-year data)\n",
      "      Using: ['NEE', 'USTAR', 'TA']\n",
      "      Using: ['NEE']\n",
      "     Computation u* filtering detection in  0 [seconds]\n",
      "5)   Flux partitioning\n",
      "      Using: ['NEE', 'SW_IN', 'TA', 'VPD']\n",
      "      Nighttime partitioning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\minpack.py:829: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\minpack.py:829: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\minpack.py:829: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\minpack.py:829: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Falge method\n",
      "      Daytime partitioning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n",
      "C:\\Users\\fuentesm\\Anaconda3\\envs\\eShape\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:519: RuntimeWarning: invalid value encountered in true_divide\n",
      "  J_transposed[i] = df / dx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Computation flux partitioning detection in  9 [seconds]\n",
      "6)   Gap-filling\n",
      "      Using: ['NEE', 'GPP_NEE_rei', 'RECO_NEE_rei', 'GPP_NEE_fal', 'RECO_NEE_fal', 'GPP_NEE_las', 'RECO_NEE_las', 'SW_IN', 'TA', 'VPD']\n",
      "  Filling  NEE\n",
      "  Filling  GPP_NEE_rei\n",
      "  Filling  RECO_NEE_rei\n",
      "  Filling  GPP_NEE_fal\n",
      "  Filling  RECO_NEE_fal\n",
      "  Filling  GPP_NEE_las\n",
      "  Filling  RECO_NEE_las\n",
      "     Computation filling gaps detection in  1 [seconds]\n",
      "8)   Outputfile\n",
      "      Write output  Output\\NEE_output\\NEE_corrected_with_flags.txt\n",
      "      Add flag columns for gap-filled variables.\n",
      "      Write.\n",
      "      Creating output file in  0 [seconds]\n",
      "9)   Daily GPP\n",
      "     Computed daily GPP in  0 [seconds]\n",
      "Total time of correction of data  4.7 [minutes]\n"
     ]
    }
   ],
   "source": [
    "    #*********************************************************************************************************************************************************************\n",
    "    #2)   Setting data frames\n",
    "    print('2)   Formatting data frames')\n",
    "    t01 = ptime.time()\n",
    "    \n",
    "    #------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    #2.a)   Read eddy covariance files (eufluxfiles)\n",
    "    print('      Read data: ', eufluxfile)\n",
    "    \n",
    "    # Assert iterable                                                                                  #This process reads the names of the eufluxfiles and adds the directory information. The loop allows to introduce n number of files for different years\n",
    "    if ',' in eufluxfile:\n",
    "        eufluxfile     = eufluxfile.split(',')\n",
    "        \n",
    "        eufluxfile     = [ inputdir  + ee.strip() for ee in eufluxfile ]                               #Change 6. Add inputdir                                                                                 \n",
    "    else:                                                                                              #If the inputdir  is not added in this section, the input file need to be in the main directory and not in the inputdir file          \n",
    "        if eufluxfile:                                                                                   \n",
    "            eufluxfile = [inputdir  + eufluxfile]\n",
    "        else:\n",
    "            try:\n",
    "                eufluxfile = hf.files_from_gui(                                                        #If any direction to the files is given, the files has to be selected through a pup up window\n",
    "                    initialdir='.', title='europe-fluxdata.eu file(s)')\n",
    "            except:\n",
    "                raise IOError(\"GUI for europe-fluxdata.eu file(s) failed.\")\n",
    "                \n",
    "    # Identify rows in the dataframe to skipt              \n",
    "    if skiprows == 'None':                                                                             #This process allows to identify the rows to skipt in the data frames\n",
    "        skiprows = ''\n",
    "    if skiprows:\n",
    "        import json  # to analyse int or list, tuple not working\n",
    "        skiprows = json.loads(skiprows.replace('(', '[').replace(')', ']'))\n",
    "        \n",
    "    # Read input files into Panda data frame and check variable availability\n",
    "    parser = lambda date: dt.datetime.strptime(date, timeformat)                               \n",
    "\n",
    "    infile = eufluxfile[0]                                                                             #Loads the first file in the eufluxfile list                                                                                          \n",
    "    df = pd.read_csv(infile, sep, skiprows=skiprows, parse_dates=[0], \n",
    "                     date_parser=parser, index_col=0, header=0)\n",
    "    if len(eufluxfile) > 1:                                                                            #Iterate to integrate all the files in case of data for different years is available \n",
    "        for infile in eufluxfile[1:]:                    \n",
    "            df_aux_a = pd.read_csv(infile, sep, skiprows=skiprows, parse_dates=[0],\n",
    "                              date_parser=parser, index_col=0, header=0)\n",
    "            df       = df.append(df_aux_a, sort=False)\n",
    "            \n",
    "    #------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ \n",
    "    # Process to ensure constant 30-minute frequency in the datasets\n",
    "    # identify beggining and end of the time series\n",
    "    df_time_aux = df.copy()\n",
    "    df_time_aux = df_time_aux.reset_index()\n",
    "    time1_aux   = df_time_aux.iloc[0, 0]\n",
    "    time2_aux   = df_time_aux.iloc[df.shape[0] -1,0]\n",
    "    \n",
    "    # create time series with daily frequency (Not needed if usinf gap filled variables)\n",
    "    time_series_aux = pd.date_range(time1_aux, time2_aux, freq=\"30min\")\n",
    "    time_series_aux = pd.DataFrame(time_series_aux).rename(columns={0: 'TIMESTAMP_START'})\n",
    "    time_series_aux ['TIMESTAMP_END']   = time_series_aux['TIMESTAMP_START'] + dt.timedelta(minutes = 30)\n",
    "    time_series_aux.set_index('TIMESTAMP_START',inplace=True)\n",
    "    df_time_aux.set_index('TIMESTAMP_START',inplace=True)\n",
    "    df_time_aux.drop('TIMESTAMP_END', axis =1, inplace=True)\n",
    "    df_time_aux_final = pd.merge(left= time_series_aux, right = df_time_aux,\n",
    "                     how=\"left\", left_index = True , right_index = True)\n",
    "    df_time_aux_final.replace(np.nan, undef, inplace=True)\n",
    "    df = df_time_aux_final.copy(deep=True)\n",
    "    \n",
    "    #------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------        \n",
    "    #2.b)   Formatting the input file    \n",
    "    print('      Formating data: ', eufluxfile)\n",
    "    \n",
    "    # Fill the undef values (e.g. -9999.) with null values (NaN)\n",
    "    #df.fillna(undef, inplace=True)\n",
    "        \n",
    "    # Flag.                                                                                            #Create file with flags\n",
    "    dff              = df.copy(deep=True)\n",
    "    dff[:]           = 0\n",
    "    dff[df == undef] = 2                                                                               #(Flag 2) for null values\n",
    "    #dff[df.isna()]   = 2\n",
    "    \n",
    "    # day / night\n",
    "    #isday = df['SW_IN'] > swthr                                                                       #This column in the data frame indicates the short wave radiation which\n",
    "    hsw = ['SW_IN']                                                                                    #can be use to identify difference between day an night. Threshold is set in the configuration file\n",
    "    hout = _findfirststart(hsw, df.columns)                                                            #Change 7. Use _findfirststart method to look for the SW_IN column\n",
    "    isday = df[hout[0]] >= swthr\n",
    "    \n",
    "    if remove_SW_IN:                                                                                   # Remove 'SW_IN' data from the data frame to not be used in the GPP flux partitioning in case the data is not available.  \n",
    "        df['SW_IN']=-9999.                                                                             #Change 7.1 Remove SW_IN. This change is just relevant for the study case of Doana\n",
    "        df['SW_IN'].replace(-9999., np.nan, inplace=True) \n",
    "        \n",
    "    # Check Ta in Kelvin\n",
    "    hta = ['TA']                                                                                       #Change 8. Change TA_ for TA. Allows more flexibility in the column names of the input file\n",
    "    hout = _findfirststart(hta, df.columns)                                                            #This process identifies if the temperature is in kelvin or in celcious                                                \n",
    "    if df[hout[0]].max() < 100.:\n",
    "        tkelvin = 273.15\n",
    "    else:\n",
    "        tkelvin = 0.\n",
    "        \n",
    "    # Add tkelvin only where not flagged\n",
    "    df.loc[dff[hout[0]] == 0, hout[0]] += tkelvin\n",
    "\n",
    "    # Add vpd if not given\n",
    "    hvpd = ['VPD']\n",
    "    hout = _findfirststart(hvpd, df.columns)\n",
    "    if len(hout) == 0:\n",
    "        hvpd = ['TA', 'RH']                                                                            #Change 9. Change TA_ and RH_ for TA and RH                                                                               \n",
    "        hout = _findfirststart(hvpd, df.columns)\n",
    "        if len(hout) != 2:\n",
    "            raise ValueError('Cannot calculate VPD.')\n",
    "        ta_id = hout[0]\n",
    "        rh_id = hout[1]\n",
    "        if df[ta_id].max() < 100.:\n",
    "            tk = df[ta_id] + 273.15\n",
    "        else:\n",
    "            tk = df[ta_id]\n",
    "        if df[rh_id].max() > 10.:\n",
    "            rh = df[rh_id] / 100.\n",
    "        else:\n",
    "            rh = df[rh_id]\n",
    "        list_1    = 1. - rh\n",
    "        list_2    =  pj.esat(tk)\n",
    "        list_1_df = list_1.to_frame().reset_index()\n",
    "        list_2_df = list_2.to_frame().rename(columns={0: 'TK'})\n",
    "        list_1_df['VPD_Total'] = list_1_df['RH'] * list_2_df['TK']\n",
    "        list_final = list_1_df.set_index('TIMESTAMP_START')\n",
    "        vpd_id = 'VPD'\n",
    "        df[vpd_id] = list_final['VPD_Total']\n",
    "        df[vpd_id].where((df[ta_id] != undef) | (df[rh_id] != undef),\n",
    "                         other=undef, inplace=True)\n",
    "        dff[vpd_id] = np.where((dff[ta_id] + dff[rh_id]) > 0, 2, 0)                                    #(Flag 2) in 'VPD_CALC'  where ta or rh is not available\n",
    "        df.loc[dff[vpd_id] == 0, vpd_id] /= 100.                                                       #Converts from hPa to Pa \n",
    "\n",
    "    # Check VPD in Pa\n",
    "    hvpd = ['VPD']\n",
    "    hout = _findfirststart(hvpd, df.columns)\n",
    "    if df[hout[0]].max() < 10.:     # kPa\n",
    "        vpdpa = 1000.\n",
    "    elif df[hout[0]].max() < 100.:  # hPa\n",
    "        vpdpa = 100.\n",
    "    else:\n",
    "        vpdpa = 1.                  # Pa\n",
    "    df.loc[dff[hout[0]] == 0, hout[0]] *= vpdpa  \n",
    "    \n",
    "    # Time stepping                                                                                    #Derives the number of datapoints per day\n",
    "    dsec  = (df.index[1] - df.index[0]).seconds\n",
    "    ntday = np.rint(86400 / dsec).astype(np.int)                                                       #Indicates the number of records per day\n",
    "    #------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    t02   = ptime.time()                                                                               #Change 9. Change legend of computation time\n",
    "    strin = ( '{:.1f} [minutes]'.format((t02 - t01) / 60.)                                           \n",
    "              if (t02 - t01) > 60.\n",
    "              else '{:d} [seconds]'.format(int(t02 - t01))\n",
    "            )\n",
    "    print('     Computation setting data frames in ', strin , end='\\n')    \n",
    "    \n",
    "    #********************************************************************************************************************************************************************* \n",
    "    # 3)   Outlier detection\n",
    "    if outlier:\n",
    "        print('3)   Spike detection')\n",
    "        t11 = ptime.time()\n",
    "\n",
    "        # Finds carbon flux data (e.g. NEE or FC)\n",
    "        houtlier = [carbonflux]                                                                        #Change 10. Process only the carbonflux variable (H and LE could be processed in the same way)                                            \n",
    "        hout = _findfirststart(houtlier, df.columns)\n",
    "        print('      Using:', hout)\n",
    "        \n",
    "        # Applies the spike detection. Only one call to mad for all variables                          #carbonflux variable can be a list with NEE, H, LE, etc. and the .madspike() requires to be called only once for alll the variables\n",
    "        sflag = hf.madspikes(df[hout], flag=dff[hout], isday=isday,                                    #This function creates flags with value 2 for outliers that are translated to flags 3 in the dff file\n",
    "                             undef=undef, nscan=nscan * ntday,                                 \n",
    "                             nfill=nfill * ntday, z=z, deriv=deriv, plot=False)\n",
    "        \n",
    "        for ii, hh in enumerate(hout):\n",
    "            dff.loc[sflag[hh] == 2, hh]   = 3                                                          #(Flag 3) for outlieres. The tool flags outliers even when there are emptyspaces\n",
    "            #dff.loc[df[hh] == undef , hh] = 2                                                         #It cannot assign flag 3 if there was not data available in the carbonflux data. It is a correction of the flag                                                        \n",
    "\n",
    "        t12   = ptime.time()                                                                           #Change 11. Change legend of computation time  \n",
    "        strin = ( '{:.1f} [minutes]'.format((t12 - t11) / 60.)                                                \n",
    "                  if (t12 - t11) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t12 - t11))\n",
    "                )\n",
    "        print('     Computation outlier detection in ', strin)  \n",
    "        \n",
    "    #********************************************************************************************************************************************************************* \n",
    "    # 4) u* filtering (data for a full year)\n",
    "    if  ustar:                                                                                         #This method requires a data set with data for a full year\n",
    "        print('4)   u* filtering')\n",
    "        t21 = ptime.time()\n",
    "        \n",
    "        #Looking for carbonflux, u*, and temperature data\n",
    "        hfilt = [carbonflux, 'USTAR', 'TA']                                                            #Change 12. Change 'NEE' for carbonflux variable\n",
    "        hout  = _findfirststart(hfilt, df.columns)\n",
    "    \n",
    "        assert len(hout) == 3, 'Could not find CO2 flux (NEE or FC), USTAR or TA in input file.'\n",
    "        print('      Using:', hout)\n",
    "        \n",
    "        #Saves a copy of the flags of the carbonflux data\n",
    "        ffsave = dff[hout[0]].to_numpy()\n",
    "        \n",
    "        #Sets a temporal flag \n",
    "        #iic    = np.where((~isday) & (df[hout[0]] < 0.))[0] \n",
    "        #dff.iloc[iic, list(df.columns).index(hout[0])] = 4                                            #(Flag 4). Temporal flag for data with negative values in the carbon fluxes during night\n",
    "                                                                                                       #Assigns flag 4 for cases of GPP negative during night time.\n",
    "        dff.loc[(~isday) & (df[hout[0]] < 0.), hout[0]] = 4 \n",
    "        # Applies the u* filtering\n",
    "        #ustars, flag = hf.ustarfilter(df[hout], flag=dff[hout],                                       #Check method to identify why the temporal flag is required in the ustarfilter\n",
    "        #                              isday=isday, undef=undef,                                       #ustarfilter function creates flags with value 2 for outliers that are translated to flags 3 in the dff file                 \n",
    "        #                              ustarmin=ustarmin, nboot=nboot,\n",
    "        #                              plateaucrit=plateaucrit,\n",
    "        #                              seasonout=seasonout,\n",
    "        #                              plot=False)\n",
    "        \n",
    "        ustars, flag = hf.ustarfilter(df[hout], flag=dff[hout],\n",
    "                                      isday=isday, undef=undef,\n",
    "                                      ustarmin=ustarmin, nboot=nboot,\n",
    "                                      plateaucrit=plateaucrit,\n",
    "                                      seasonout=False, plot=False)\n",
    "        \n",
    "        dff[hout[0]] = ffsave                                                                          #Return to original flags file without 4-flag\n",
    "        df  = df.assign(USTAR_TEST=flag)                                                               #Change 14. Change 'USTAR_TEST_1_1_1' column name for 'USTAR_TEST'                                                                \n",
    "        dff = dff.assign(USTAR_TEST=np.zeros(df.shape[0], dtype=np.int))                               #This line adds a column in the dataframe of flags \n",
    "        \n",
    "        if applyustarflag:\n",
    "            hustar = [carbonflux]                                                                      #Change 15. Process only the carbonflux variable (H and LE could be processed in the same way)   \n",
    "            hout = _findfirststart(hustar, df.columns)\n",
    "            print('      Using:', hout)\n",
    "            for ii, hh in enumerate(hout):\n",
    "                dff.loc[flag [hh] == 2, hh] = 5                                                        #(Flag 5) for carbon fluxes with ustar(friction velocity) below calculated threshold\n",
    "                #dff.loc[df[hh] == undef, hh] = 2                                  \n",
    "                \n",
    "        t22   = ptime.time()                                                                           #Change 16. Change legend of computation time.\n",
    "        strin = ( '{:.1f} [minutes]'.format((t22 - t21) / 60.)                                           \n",
    "                  if (t22 - t21) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t22 - t21))\n",
    "                )\n",
    "        print('     Computation u* filtering detection in ', strin) \n",
    "\n",
    "    #------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # 4)   u* filtering (data for partial year)\n",
    "    if  ustar_non_annual :                                                                             #Change 17. ustar_non_annual  is a copy of ustar without the 'ustarfilter'\n",
    "        print('4)   u* filtering (less than 1-year data)')                                             #The ustar_noN_annual method is simple approach to manually set a ustar threshold when \n",
    "        t21 = ptime.time()                                                                             #there is no data for a full year required to compute ustar\n",
    "        \n",
    "        #Looking for carbonflux, u*, and temperature data\n",
    "        hfilt = [carbonflux, 'USTAR', 'TA']                                                            \n",
    "        hout  = _findfirststart(hfilt, df.columns)\n",
    "        assert len(hout) == 3, 'Could not find CO2 flux (NEE or FC), USTAR or TA in input file.'\n",
    "        print('      Using:', hout)\n",
    "        \n",
    "        flag = sflag.copy().multiply(0)\n",
    "        \n",
    "        #flag.loc[(df['USTAR'] < ustarmin) & (dff[carbonflux] == 2), carbonflux] = 2.\n",
    "        #flag.loc[(df['USTAR'] < ustarmin) & (dff[carbonflux] == 3), carbonflux] = 2.\n",
    "        #flag.loc[(df['USTAR'] < ustarmin), carbonflux] = 2.\n",
    "        #flag.loc[(df['USTAR'] < ustarmin) & (dff['USTAR'] != 2) & (dff[carbonflux] != 2), carbonflux] = 2.\n",
    "        #Flags when the USTAR is below ustarmin and when there is carbonflux data available for the same timestep. \n",
    "        flag.loc[(df['USTAR'] < ustarmin) & (df[carbonflux] != undef), carbonflux] = 2.\n",
    "        #flag.loc[(df['USTAR'] < ustarmin), carbonflux] = 2.\n",
    "                               \n",
    "        df  = df.assign(USTAR_TEST=flag)               \n",
    "        dff = dff.assign(USTAR_TEST=np.zeros(df.shape[0], dtype=np.int))\n",
    "\n",
    "        if applyustarflag:\n",
    "            hustar = [carbonflux]\n",
    "            hout = _findfirststart(hustar, df.columns)\n",
    "            print('      Using:', hout)\n",
    "            for ii, hh in enumerate(hout):\n",
    "                dff.loc[flag[hh] == 2, hh] = 5 \n",
    "                #dff.loc[df[hh] == undef, hh] = 2 \n",
    "\n",
    "        t22   = ptime.time()                                                                           \n",
    "        strin = ( '{:.1f} [minutes]'.format((t22 - t21) / 60.)                                           \n",
    "                  if (t22 - t21) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t22 - t21))\n",
    "                )\n",
    "        print('     Computation u* filtering detection in ', strin) \n",
    "\n",
    "    #********************************************************************************************************************************************************************* \n",
    "    # 5)   Flux partitioning\n",
    "    if partition:\n",
    "        print('5)   Flux partitioning')\n",
    "        t31 = ptime.time()\n",
    "        \n",
    "        #Looking for carbon flux, global radiation, temperature and vpd data\n",
    "        hpart = [carbonflux, 'SW_IN', 'TA', 'VPD']                                                     #Change 18. Change 'NEE' for carbonflux variable                                                                      \n",
    "        hout  = _findfirststart(hpart, df.columns)\n",
    "        assert len(hout) == 4, 'Could not find CO2 flux (NEE or FC), SW_IN, TA, or VPD in input file.'\n",
    "        print('      Using:', hout)\n",
    "\n",
    "        suff = hout[0]                                                                                 #Change 20. Rename with the carbonflux variable              \n",
    " \n",
    "        # nighttime method\n",
    "        print('      Nighttime partitioning')\n",
    "        dfpartn = hf.nee2gpp(df[hout], flag=dff[hout], isday=isday,\n",
    "                             undef=undef, method='reichstein',\n",
    "                             nogppnight=nogppnight)\n",
    "\n",
    "        dfpartn.rename(columns=lambda c: c + '_' + suff + '_rei', inplace=True)                        #Change 21. Add '_' before suff and change '1' with '_1'\n",
    "            \n",
    "        # falge method                                                                                 #Change 22. Falge method instead of lasslop method\n",
    "        print('      Falge method')\n",
    "        dfpartf = hf.nee2gpp(df[hout], flag=dff[hout], isday=isday,\n",
    "                             undef=undef, method='falge',         \n",
    "                             nogppnight=nogppnight)  \n",
    "        \n",
    "        dfpartf.rename(columns=lambda c: c + '_' + suff + '_fal', inplace=True)\n",
    "        \n",
    "        # daytime method                                                                               #Change 23. Day time method 'lasslop' can be integrated as a third method\n",
    "        print('      Daytime partitioning')\n",
    "        dfpartd = hf.nee2gpp(df[hout], flag=dff[hout], isday=isday,\n",
    "                             undef=undef, method='lasslop',\n",
    "                             nogppnight=nogppnight)\n",
    "        \n",
    "        dfpartd.rename(columns=lambda c: c  + '_' + suff + '_las', inplace=True) \n",
    "\n",
    "        df = pd.concat([df, dfpartn, dfpartf, dfpartd],  axis=1)\n",
    "\n",
    "        # take flags from NEE or FC same flag\n",
    "        for dn in ['rei', 'fal', 'las']:\n",
    "            for gg in ['GPP', 'RECO']:                                                                 #Change 24. Adds '_' between labels\n",
    "                dff[gg + '_' + suff + '_'+ dn] = dff[hout[0]]                                          #Takes flags from the carbonflux variable\n",
    "                \n",
    "        # flag GPP and RECO if they were not calculated\n",
    "        for dn in ['rei', 'fal', 'las']:\n",
    "            for gg in ['GPP', 'RECO']:                                                                 #Change 25. This method flags with 2 value the 'RECO' columns when 'GPP was not calculated\n",
    "                dff.loc[df['GPP' + '_' + suff + '_'+ dn] == undef, gg + '_' + suff + '_'+ dn ] = 2     #('GPP' == undef)\n",
    "          \n",
    "        t32   = ptime.time()\n",
    "        strin = ( '{:.1f} [minutes]'.format((t32 - t31) / 60.)                                         #Change 26. Change legend of computation time.         \n",
    "                  if (t32 - t31) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t32 - t31))\n",
    "                )\n",
    "        print('     Computation flux partitioning detection in ', strin)  \n",
    "        \n",
    "    #********************************************************************************************************************************************************************* \n",
    "    # 6)   Gap-filling\n",
    "    if fill:        \n",
    "        print('6)   Gap-filling')\n",
    "        t41 = ptime.time()\n",
    "        \n",
    "        #Looking for meteorological data\n",
    "        hfill = ['SW_IN', 'TA', 'VPD']\n",
    "        hout  = _findfirststart(hfill, df.columns)\n",
    "        assert len(hout) == 3, 'Could not find SW_IN, TA or VPD in input file.'\n",
    "\n",
    "        # if available\n",
    "        rei_gpp = 'GPP_'+carbonflux+'_rei'\n",
    "        rei_res = 'RECO_'+carbonflux+'_rei'\n",
    "        fal_gpp = 'GPP_'+carbonflux+'_fal'\n",
    "        fal_res = 'RECO_'+carbonflux+'_fal'\n",
    "        las_gpp = 'GPP_'+carbonflux+'_las'\n",
    "        las_res = 'RECO_'+carbonflux+'_las'\n",
    "        \n",
    "        hfill = [ carbonflux,                                                                          #Change 27. Change names of columns to process\n",
    "                  rei_gpp,rei_res,fal_gpp,fal_res,las_gpp,las_res,\n",
    "                  'SW_IN', 'TA', 'VPD']\n",
    "        \n",
    "        hout  = _findfirststart(hfill, df.columns)\n",
    "        print('      Using:', hout)\n",
    "        \n",
    "        df_f, dff_f = hf.gapfill(df[hout], flag=dff[hout],\n",
    "                                 sw_dev=sw_dev, ta_dev=ta_dev, vpd_dev=vpd_dev,\n",
    "                                 longgap=longgap, undef=undef, err=False,\n",
    "                                 verbose=1)\n",
    "        \n",
    "        #hdrop = ['SW_IN', 'TA', 'VPD']                           \n",
    "        #hout = _findfirststart(hdrop, df.columns)\n",
    "        #df_f.drop(columns=hout,  inplace=True)\n",
    "        #dff_f.drop(columns=hout, inplace=True)\n",
    "\n",
    "        def _add_f(c):\n",
    "            return '_'.join(c.split('_')[:-3] + c.split('_')[-3:]  + ['f'])                            #Change 28. 'f' of fill till the end of the name of the column names\n",
    "        df_f.rename(columns=_add_f,  inplace=True)\n",
    "        dff_f.rename(columns=_add_f, inplace=True)    \n",
    "        \n",
    "        df  = pd.concat([df,  df_f],  axis=1)\n",
    "        dff = pd.concat([dff, dff_f], axis=1)\n",
    "        \n",
    "        t42   = ptime.time()                                                                           #Change 29. Change legend of computation time.\n",
    "        strin = ( '{:.1f} [minutes]'.format((t42 - t41) / 60.)                                           \n",
    "                  if (t42 - t41) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t42 - t41))\n",
    "                )\n",
    "        print('     Computation filling gaps detection in ', strin) \n",
    "        \n",
    "    #********************************************************************************************************************************************************************* \n",
    "    # 7)   Error estimate\n",
    "    if fluxerr:\n",
    "        print('7)   Flux error estimates')\n",
    "        t51 = ptime.time()\n",
    "        \n",
    "        #Looking for meteorological data\n",
    "        hfill = ['SW_IN', 'TA', 'VPD']\n",
    "        hout  = _findfirststart(hfill, df.columns)\n",
    "        assert len(hout) == 3, 'Could not find SW_IN, TA or VPD in input file.'\n",
    "\n",
    "        # if available \n",
    "        rei_gpp = 'GPP_'+carbonflux+'_rei'\n",
    "        rei_res = 'RECO_'+carbonflux+'_rei'\n",
    "        fal_gpp = 'GPP_'+carbonflux+'_fal'\n",
    "        fal_res = 'RECO_'+carbonflux+'_fal'\n",
    "        las_gpp = 'GPP_'+carbonflux+'_las'\n",
    "        las_res = 'RECO_'+carbonflux+'_las'\n",
    "        \n",
    "        hfill = [ carbonflux,                                                                          #Change 30. Change names of columns to process\n",
    "                  rei_gpp,rei_res,fal_gpp,fal_res,las_gpp,las_res,\n",
    "                  'SW_IN', 'TA', 'VPD']\n",
    "        \n",
    "        hout  = _findfirststart(hfill, df.columns)\n",
    "        print('      Using:', hout)\n",
    "        \n",
    "        df_e = hf.gapfill(df[hout], flag=dff[hout],\n",
    "                          sw_dev=sw_dev, ta_dev=ta_dev, vpd_dev=vpd_dev,\n",
    "                          longgap=longgap, undef=undef, err=True, \n",
    "                          verbose=1)\n",
    "        \n",
    "        hdrop = ['SW_IN', 'TA', 'VPD']\n",
    "        hout = _findfirststart(hdrop, df.columns)\n",
    "        df_e.drop(columns=hout, inplace=True)\n",
    "\n",
    "        def _add_e(c):                                                                                 #Change 31. Create _add_e instead of reusing _add_f\n",
    "            return '_'.join(c.split('_')[:-3] + c.split('_')[-3:] + ['e'])\n",
    "\n",
    "        # rename the variables with e (error)\n",
    "        colin  = list(df_f.columns)\n",
    "        df_e.rename(columns=_add_e,  inplace=True)\n",
    "        colout = list(df_f.columns)                                                                    #List with updated names to display in the dff file\n",
    "        df     = pd.concat([df, df_f], axis=1)\n",
    "        \n",
    "        # take flags of non-error columns with the same label\n",
    "        for cc in range(len(colin)):\n",
    "            dff[colout[cc]] = dff[colin[cc]]\n",
    "\n",
    "        t52   = ptime.time()                                                                           #Change 32. Change legend of computation time.\n",
    "        strin = ( '{:.1f} [minutes]'.format((t52 - t51) / 60.)                                           \n",
    "                  if (t52 - t51) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t52 - t51))\n",
    "                )\n",
    "        print('     Computation flux error estimates in ', strin) \n",
    "        \n",
    "    #********************************************************************************************************************************************************************* \n",
    "    # 8)   Output\n",
    "    print('8)   Outputfile')\n",
    "    t61 = ptime.time()\n",
    "\n",
    "    if not outputfile:\n",
    "        try:\n",
    "            outputdir = hf.directory_from_gui(initialdir='.',\n",
    "                                              title='Output directory')\n",
    "        except:\n",
    "            raise IOError(\"GUI for output directory failed.\")\n",
    "            \n",
    "        outputfile = configfile[:configfile.rfind('.')]                                                #Takes the name from the configurtion file\n",
    "        outputfile = outputdir + '/' + os.path.basename(outputfile + '.csv')                           #Change 33. Change outdir for outputdir to select directly the output folder\n",
    "    else:\n",
    "        outputfile = outputdir + outputfile                                                            #Change 34. Create outputfile in case outputfile and outputdir are available \n",
    "        \n",
    "    print('      Write output ', outputfile)\n",
    "\n",
    "    # Back to original units\n",
    "    hta = ['TA']\n",
    "    hout = _findfirststart(hta, df.columns)\n",
    "    df.loc[dff[hout[0]] == 0, hout[0]] -= tkelvin\n",
    "    hvpd = ['VPD']\n",
    "    hout = _findfirststart(hvpd, df.columns)\n",
    "    df.loc[dff[hout[0]] == 0, hout[0]] /= vpdpa\n",
    "\n",
    "    if outundef:\n",
    "        print('      Set flags to undef.')\n",
    "        for cc in df.columns:\n",
    "            if cc.split('_')[-1] != 'f' and cc.split('_')[-1] != 'e':  # exclude gap-filled columns    #Change 35. Change [-4] for [-1] and exclude error 'e' columns\n",
    "                df[cc].where(dff[cc] == 0, other=undef, inplace=True)  # Is the & working?             #This line writes undef (-9999.) for all the flagged data\n",
    "\n",
    "    if outflagcols:\n",
    "        print('      Add flag columns.')\n",
    "\n",
    "        def _add_flag(c):\n",
    "            return 'flag_' + c\n",
    "        dff.rename(columns=_add_flag, inplace=True)\n",
    "        \n",
    "        # no flag columns for flags\n",
    "        dcol = []\n",
    "        for hh in dff.columns:\n",
    "            if '_TEST' in hh:                                                                          #Avoids bringing _TEST data from dff which is an data columnd only with zero values not connected to the datasets                                                                 #Change 36. Change '_TEST_' for '_TEST'\n",
    "                dcol.append(hh)\n",
    "        if dcol:\n",
    "            dff.drop(columns=dcol, inplace=True)                                                       #Remove the TEST columns\n",
    "        df = pd.concat([df, dff], axis=1)\n",
    "        \n",
    "    else:\n",
    "        print('      Add flag columns for gap-filled variables.')\n",
    "        occ = []\n",
    "        for cc in df.columns:\n",
    "            if cc.split('_')[-1] == 'f' or cc.split('_')[-1] == 'e':                                   #Change 37. Add the error columns 'e' in the condition\n",
    "                occ.append(cc)                                                                         #Selects only flags for _f or _e\n",
    "        dff1 = dff[occ].copy(deep=True)                                                               \n",
    "        dff1.rename(columns=lambda c: 'flag_' + c, inplace=True)\n",
    "        df = pd.concat([df, dff1], axis=1)\n",
    "    \n",
    "    print('      Write.')\n",
    "    df.to_csv(outputfile, sep=sep, na_rep=str(undef), index=True,\n",
    "              date_format=timeformat)\n",
    "    \n",
    "    t62   = ptime.time()\n",
    "    strin = ( '{:.1f} [minutes]'.format((t62 - t61) / 60.)                                             #Change 37. Change legend of computation time.          \n",
    "              if (t62 - t61) > 60.\n",
    "              else '{:d} [seconds]'.format(int(t62 - t61))\n",
    "            )\n",
    "    print('      Creating output file in ', strin) \n",
    "    \n",
    "    #********************************************************************************************************************************************************************* \n",
    "    # Next elements are complement modules to compute Remote Sensing empirical models of GPP           #Change 39. All below code is extra code to derive empirical models\n",
    "    #*********************************************************************************************************************************************************************\n",
    "    # 9)   Daily estimations \n",
    "    if daily_gpp:                                                                                       \n",
    "        print('9)   Daily GPP')\n",
    "        t71 = ptime.time()\n",
    "\n",
    "        # Daily GPP and enviromental drivers\n",
    "        gpp = df.copy()\n",
    "        \n",
    "        gpp = gpp[(gpp[carbonflux+'_f'] < carbonfluxlimit) & (gpp[carbonflux+'_f'] > -carbonfluxlimit)]                                                           #These parameters are set to avoid having fluxing carboxes beyond specific established limits.\n",
    "        gpp = gpp[(gpp[rei_res+'_f'] < respirationlimit) & (gpp[rei_res+'_f'] > -respirationlimit)] \n",
    "        \n",
    "        gpp_mean = gpp[['TA_f','VPD_f','SW_IN_f']]\n",
    "        gpp_sum  = gpp[[carbonflux+'_f',rei_gpp+'_f',rei_res+'_f',fal_gpp+'_f',fal_res+'_f',las_gpp+'_f',las_res+'_f']] * 12 * 30 * 60 /1000000\n",
    "\n",
    "        gpp_mean = gpp_mean.reset_index()\n",
    "        gpp_sum  = gpp_sum.reset_index()\n",
    "\n",
    "        gpp_mean['date']  =  gpp_mean['TIMESTAMP_START'].dt.date\n",
    "        gpp_sum ['date']  =  gpp_sum['TIMESTAMP_START'].dt.date\n",
    "        \n",
    "        gpp_mean.replace(undef, np.nan, inplace=True)\n",
    "        gpp_sum.replace(undef, np.nan, inplace=True) \n",
    "\n",
    "        gpp_mean_daily = gpp_mean.groupby('date').mean()\n",
    "        gpp_sum_daily  = gpp_sum.groupby('date').sum()\n",
    "\n",
    "        df_gpp = pd.concat([gpp_mean_daily, gpp_sum_daily], axis=1)\n",
    "\n",
    "        # identify beggining and end of the time series\n",
    "        df_time = df_gpp.reset_index()\n",
    "        time1 = df_time.iloc[0, 0]\n",
    "        time2 = df_time.iloc[df_gpp.shape[0] -1,0]\n",
    "\n",
    "        # create time series with daily frequency (Not needed if usinf gap filled variables)\n",
    "        time_series = pd.date_range(time1, time2, freq=\"D\")\n",
    "        time_series = pd.DataFrame(time_series).rename(columns={0: 'date'}).set_index('date')\n",
    "        df_gpp_time = pd.merge(left= time_series, right = df_gpp,\n",
    "                                 how=\"left\", left_index = True , right_index = True)\n",
    "\n",
    "        # smoth time series  \n",
    "        #df_gpp_smoth  = df_gpp_smoth.interpolate(method='akima', order=1, limit_direction ='backward')\n",
    "        df_gpp_smoth  = df_gpp_time.interpolate(method='akima', order=1, limit_direction ='forward')\n",
    "        df_gpp_smoth  = df_gpp_smoth.rolling(rolling_window_gpp, center=True, min_periods=1).mean()\n",
    "        \n",
    "        # save file of daily GPP\n",
    "        df_gpp_smoth.to_csv(outputdir + \"/GPP_output/\" + ID + \"_GPP_daily.txt\")\n",
    "\n",
    "        t72   = ptime.time()\n",
    "        strin = ( '{:.1f} [minutes]'.format((t72 - t71) / 60.)                                                      \n",
    "                  if (t72 - t71) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t72 - t71))\n",
    "                )\n",
    "        print('     Computed daily GPP in ', strin) \n",
    "        \n",
    "    #********************************************************************************************************************************************************************* \n",
    "    # Finish Correction of Data with the Hesseflux package \n",
    "    t2   = ptime.time()                                                                                #Change 38. Change legend of computation time.\n",
    "    strin = ( '{:.1f} [minutes]'.format((t2 - t1) / 60.)                                            \n",
    "              if (t2 - t1) > 60.\n",
    "              else '{:d} [seconds]'.format(int(t2 - t1))\n",
    "            )\n",
    "    print('Total time of correction of data ', strin) \n",
    "    #********************************************************************************************************************************************************************* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa670c7",
   "metadata": {},
   "source": [
    "# Plot eddy covariance fluxes and GPP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0188d699",
   "metadata": {},
   "source": [
    "Daily GPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e5d72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print time series\n",
    "model = '_rei_f'\n",
    "import altair as alt\n",
    "\n",
    "data_ec        = df_gpp_smoth.reset_index()\n",
    "data_co_fluxes = data_ec[['date','GPP_'+carbonflux+ model]].copy()\n",
    "data_co_fluxes = data_co_fluxes.rename(columns={\n",
    "    'GPP_'+carbonflux + model: 'GPP (gC m-2 day-1)',\n",
    "})\n",
    "data_co_fluxes.head(10)\n",
    "\n",
    "alt.Chart(data_co_fluxes).mark_bar(size=1).encode(\n",
    "    x='date:T',\n",
    "    y='GPP (gC m-2 day-1):Q',\n",
    "    color=alt.Color(\n",
    "        'GPP (gC m-2 day-1):Q', scale=alt.Scale(scheme='redyellowgreen', domain=(0, 14))),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('date:T', title='Date'),\n",
    "        alt.Tooltip('GPP (gC m-2 day-1):Q', title='GPP (gC m-2 day-1)')\n",
    "    ]).properties(width=600, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f97490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes=plt.subplots(figsize=(13,7))\n",
    "\n",
    "axes.plot(df_gpp_smoth[carbonflux+'_f'], marker='.', linestyle=':', ms = 6, color = 'RED', \n",
    "             label='Serie original')\n",
    "axes.plot(df_gpp_smoth[rei_gpp+'_f'], marker='.', linestyle=':', ms = 6, color = 'GRAY', \n",
    "             label='Serie original')\n",
    "axes.plot(df_gpp_smoth[rei_res+'_f'], marker='.', linestyle=':', ms = 6, color = 'BLACK', \n",
    "             label='Serie original')\n",
    "\n",
    "axes.set_ylim(-20, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f632c30",
   "metadata": {},
   "source": [
    "30-minute filled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d3eacd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes=plt.subplots(figsize=(13,7))\n",
    "\n",
    "axes.plot(df[carbonflux+'_f'], marker='.', linestyle=':', ms = 6, color = 'orange', \n",
    "             label='Serie original')\n",
    "\n",
    "axes.plot(df[rei_gpp+'_f'], marker='.', linestyle=':', ms = 6, color = 'yellow', \n",
    "             label='Serie original')\n",
    "\n",
    "axes.plot(df[fal_gpp+'_f'], marker='.', linestyle=':', ms = 6, color = 'blue', \n",
    "             label='Serie original')\n",
    "\n",
    "axes.plot(df[rei_res], marker='.', linestyle=':', ms = 6, color = 'red', \n",
    "             label='Serie original')\n",
    "\n",
    "axes.plot(df[fal_res], marker='.', linestyle=':', ms = 6, color = 'gray', \n",
    "             label='Serie original')\n",
    "\n",
    "axes.set_ylim(-50, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c327e75",
   "metadata": {},
   "source": [
    "30-minute non-filled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae7554",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes=plt.subplots(figsize=(13,7))\n",
    "\n",
    "axes.plot(df[carbonflux], marker='.', linestyle=':', ms = 6, color = 'orange', \n",
    "             label='Serie original')\n",
    "\n",
    "axes.plot(df[rei_gpp], marker='.', linestyle=':', ms = 6, color = 'yellow', \n",
    "             label='Serie original')\n",
    "\n",
    "axes.plot(df[fal_gpp], marker='.', linestyle=':', ms = 6, color = 'blue', \n",
    "             label='Serie original')\n",
    "\n",
    "axes.plot(df[rei_res], marker='.', linestyle=':', ms = 6, color = 'red', \n",
    "             label='Serie original')\n",
    "\n",
    "axes.plot(df[fal_res], marker='.', linestyle=':', ms = 6, color = 'gray', \n",
    "             label='Serie original')\n",
    "\n",
    "axes.set_ylim(-100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7caf63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools to get GPP for specific periods (mean and sum)\n",
    "date1 = '2019-01-01'\n",
    "date2 = '2019-01-31'\n",
    "\n",
    "gpp_analysis_A = df_gpp_smoth.loc[(df_gpp_smoth.index >= date1) & (df_gpp_smoth.index < date2) ]\n",
    "\n",
    "gpp_analysis_B = gpp_analysis_A.describe()\n",
    "\n",
    "gpp_analysis_C = gpp_analysis_A.copy().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b2fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #*********************************************************************************************************************************************************************     \n",
    "    if climatological_footprint:\n",
    "        \n",
    "        print('10)   Climatological footprint')\n",
    "        t81 = ptime.time()\n",
    "        \n",
    "        #-------------------------------------------------------------------------------------------------------------------\n",
    "        # load carbon flux file  \n",
    "        \n",
    "        df_carbonflux = df.loc[dff[carbonflux]==0].copy(deep=True)\n",
    "        df_carbonflux.replace(undef, np.nan, inplace=True)\n",
    "        df_carbonflux = df.loc[df['USTAR']>0.1]\n",
    "        \n",
    "        #-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # metadata parameters\n",
    "        fetch = 100*(instrument_height_anenometer - displacement_height) #Fetch to height ratio https://www.mdpi.com/2073-4433/10/6/299\n",
    "                                                                         #https://nicholas.duke.edu/people/faculty/katul/Matlab_footprint.html  \n",
    "        # footprint variables \n",
    "        #domaint_var  = [-2000., 2000., -2000., 2000.]\n",
    "        #nxt_var      = 1000\n",
    "        #rst_var      = [20.,40.,60.,80.]   \n",
    "        \n",
    "        domaint_var  = domaint_var\n",
    "        nxt_var      = nxt_var\n",
    "        rst_var      = rst_var\n",
    "        \n",
    "        # function to add date variables to DataFrame.\n",
    "        def add_date_info(df):\n",
    "\n",
    "            df['Timestamp'] = pd.to_datetime(df['TIMESTAMP_END']) \n",
    "            df['Year'] = pd.DatetimeIndex(df['Timestamp']).year\n",
    "            df['Month'] = pd.DatetimeIndex(df['Timestamp']).month\n",
    "            df['Day'] = pd.DatetimeIndex(df['Timestamp']).day\n",
    "            df['Hour'] = pd.DatetimeIndex(df['Timestamp']).hour\n",
    "            df['Minute'] = pd.DatetimeIndex(df['Timestamp']).minute\n",
    "\n",
    "            return df\n",
    "        \n",
    "        # function to add ffp data to the data frame\n",
    "        def add_ffp_info(df):\n",
    "\n",
    "            df['zm']              = instrument_height_anenometer\n",
    "            df['d']               = displacement_height\n",
    "            df['z0']              = roughness_lenght    \n",
    "            df_ec_variables       = ['Year','Month','Day','Hour','Minute',\n",
    "                                     'zm','d','z0','WS','MO_LENGTH','V_SIGMA','USTAR','WD']\n",
    "            ffp_variables         = ['yyyy','mm','day','HH_UTC','MM',\n",
    "                                     'zm','d','z0','u_mean','L','sigma_v','u_star','wind_dir']\n",
    "            df = df.loc[:,df_ec_variables].set_axis(ffp_variables , axis=1)\n",
    "\n",
    "            return df\n",
    "        \n",
    "        # function to create files for the online tool \n",
    "        def online_format_by_year(df, year):\n",
    "            \n",
    "            # select only selected year\n",
    "            df = df[df['yyyy']==year].copy(deep=True) \n",
    "            \n",
    "            # compute mean velocity friction\n",
    "            df_u_mean = df['u_mean'].mean()\n",
    "            df['u_mean'] = df_u_mean\n",
    "            \n",
    "            # remove index info from file\n",
    "            df_online = df.set_index('yyyy')\n",
    "            \n",
    "            # save file\n",
    "            filepath = outputdir +'/Footprint_output/'\n",
    "            df_online.to_csv(filepath + ID + \"_ffp_online_data_\"+ str(year) + \".csv\")\n",
    "            \n",
    "            return df_online\n",
    "        \n",
    "        # function to create a climatological footprint per year\n",
    "        def python_tool_by_year(df, year):\n",
    "        \n",
    "            df = df[df['yyyy']==year].copy(deep=True) \n",
    "            df_u_mean = df['u_mean'].mean()\n",
    "            df['u_mean'] = df_u_mean\n",
    "            \n",
    "            # brings a code with the FPP model \n",
    "            %run FFP_Python/calc_footprint_FFP_climatology.py\n",
    "        \n",
    "            # defines h (boundary layer height) in convective conditions \n",
    "            df['h_convective'] = boundary_layer_height\n",
    "            \n",
    "            # function that adds boundary layer height info for stable conditions\n",
    "            def add_bl(df): \n",
    "                \n",
    "                angular_velocity = 7.29e-05               # angular velocity of the Earths rotation\n",
    "                radianes = (latitude* math.pi)/180\n",
    "                f = 2*angular_velocity*math.sin(radianes) # coriolis parameter\n",
    "                df['h_stable'] = df['L']/3.8*(-1+( 1 + 2.28 * df['u_star']/(f*df['L']))**0.5)\n",
    "                return df  \n",
    "            \n",
    "            # defines h (boundary layer height) in stable conditions \n",
    "            df = add_bl(df)\n",
    "            \n",
    "            # functions to differenciate between stable and convective conditions with the L parameter \n",
    "            def stable(L): \n",
    "                if L < 10:\n",
    "                    bl = 0\n",
    "                else:\n",
    "                    bl = 1\n",
    "                return bl\n",
    "            \n",
    "            def convective(L): \n",
    "                if L < 10:\n",
    "                    bl = 1\n",
    "                else:\n",
    "                    bl = 0\n",
    "                return bl\n",
    "            \n",
    "            df['stable']     = df['L'].apply(stable)\n",
    "            df['convective'] = df['L'].apply(convective)\n",
    "            \n",
    "            df.replace(np.nan, -999, inplace=True) \n",
    "            \n",
    "            # add h (boundary layer height) parameter\n",
    "            df['h'] =  df['h_convective']*df['convective']\n",
    "            df['h'] =  df['h'] + df['h_stable']*df['stable']\n",
    "        \n",
    "            # function to convert dataframe columns into list (used in the tool)\n",
    "            def variablesToList(df):\n",
    "\n",
    "                zmt      = df['zm'].to_numpy().tolist()             #0\n",
    "                z0t      = df['z0'].to_numpy().tolist()             #1\n",
    "                u_meant  = df['u_mean'].to_numpy().tolist()         #2\n",
    "                ht       = df['h'].to_numpy().tolist()              #3\n",
    "                olt      = df['L'].to_numpy().tolist()              #4\n",
    "                sigmavt  = df['sigma_v'].to_numpy().tolist()        #5\n",
    "                ustart   = df['u_star'].to_numpy().tolist()         #6\n",
    "                wind_dirt= df['wind_dir'].to_numpy().tolist()       #7\n",
    "                domaint  = domaint_var\n",
    "                nxt      = nxt_var\n",
    "                rst      = rst_var\n",
    "                return zmt,z0t,u_meant,ht,olt,sigmavt,ustart,wind_dirt,domaint,rst\n",
    "            \n",
    "            ffp = variablesToList(df)\n",
    "            \n",
    "            # function to calcuate the footprint\n",
    "            def calculateFootprint(list):\n",
    "                l = list\n",
    "                FFP = FFP_climatology (zm=l[0], z0=None, umean = l[2], h=l[3], ol=l[4], sigmav=l[5],\n",
    "                ustar=l[6], wind_dir=l[7], domain=l[8], nx=None, rs=l[9],smooth_data=1, fig=1)\n",
    "                return FFP\n",
    "            \n",
    "            footprint = calculateFootprint(ffp)\n",
    "\n",
    "            return footprint  \n",
    "        \n",
    "        # add date and labels info\n",
    "        df_ffp  = add_date_info(df_carbonflux.reset_index())\n",
    "        df_ffp  = add_ffp_info(df_ffp).dropna(subset=['yyyy'])\n",
    "        \n",
    "        # create a only file per year identified in the input files\n",
    "        years = df_ffp['yyyy'].unique().tolist()\n",
    "        \n",
    "        for i in years:\n",
    "            globals()['df_online_%s' % i] = online_format_by_year(df_ffp, i)\n",
    "            print('      File: df_online_%s' % i)\n",
    "        \n",
    "        # create aclimatological footprint per year\n",
    "        for i in years:\n",
    "            \n",
    "            print('  \\n      Footprint for the year ', i)\n",
    "            globals()['df_python_footprint_%s' % i] = python_tool_by_year(df_ffp, i)\n",
    "            \n",
    "            fp = globals()['df_python_footprint_%s' % i] \n",
    "            \n",
    "            filename = 'df_python_footprint_' + str(i)\n",
    "            print('      Created file: ', filename)\n",
    "            \n",
    "            # transforme x,y values in geographical coordinates\n",
    "            projection = projection_site                                                                    #This is an input with the format of QGIS\n",
    "            pro = Proj(projection)\n",
    "\n",
    "            lat = latitude     \n",
    "            lon = longitude    \n",
    "\n",
    "            UTMlon, UTMlat     = pro(lon, lat)\n",
    "            geo_lon, geo_lat   = pro(UTMlon, UTMlat , inverse=True)\n",
    "            \n",
    "            # tranformation per contour line \n",
    "            for n in range(len(rst_var)):\n",
    "\n",
    "                    print('      Contour line:', rst_var[n])\n",
    "                    \n",
    "                    # create a data frame with the x,y data per contour line\n",
    "                    x = fp['xr'][n]\n",
    "                    y = fp['yr'][n]\n",
    "\n",
    "                    d_ffp = {'x': x, 'y': y}\n",
    "\n",
    "                    ffp = pd.DataFrame(data=d_ffp)\n",
    "\n",
    "                    # transform x,y data into UTM\n",
    "                    ffp['UTMlon'] = UTMlon\n",
    "                    ffp['UTMlat'] = UTMlat\n",
    "                    \n",
    "                    ffp['X'] = ffp['x'] + ffp['UTMlon'] \n",
    "                    ffp['Y'] = ffp['y'] + ffp['UTMlat']\n",
    "                    \n",
    "                    # transform UTM into geographical coordinates\n",
    "\n",
    "                    ffp['lon'], ffp['lat'] = pro(ffp['X'].values, ffp['Y'].values, inverse=True)\n",
    "                    globals()['footprint_%s_%d' %(int(rst_var[n]),i)] = ffp[['lon','lat']]\n",
    "                    \n",
    "                    # export file\n",
    "                    ffp_export = globals()['footprint_%s_%d' %(int(rst_var[n]),i)]\n",
    "                    filepath = outputdir +'/Footprint_output/'\n",
    "                    ffp_export.to_csv(filepath + ID + \"_footprint_\" + str(int(rst_var[n])) + '_' + str(i)+ \".csv\")\n",
    "                    \n",
    "                    print(\"      Exporting: footprint_\" + str(int(rst_var[n])) + '_' + str(i))\n",
    "                    \n",
    "        t82   = ptime.time()\n",
    "        strin = ( '{:.1f} [minutes]'.format((t82 - t81) / 60.)                                                      \n",
    "                  if (t82 - t81) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t82 - t81))\n",
    "                )\n",
    "        print('     Computation climatological footprint in ', strin)    \n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ad5e0",
   "metadata": {},
   "source": [
    "# Plot climatological footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1754436",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 14)) \n",
    "\n",
    "ax.scatter(footprint_20_2019['lon'], footprint_20_2019['lat'],\n",
    "               c='black', alpha=0.2, label='20')\n",
    "ax.scatter(footprint_40_2019['lon'], footprint_40_2019['lat'],\n",
    "               c='black', alpha=0.2, label='40')\n",
    "ax.scatter(footprint_60_2019['lon'], footprint_60_2019['lat'],\n",
    "               c='black', alpha=0.2, label='60')\n",
    "ax.scatter(footprint_80_2019['lon'], footprint_80_2019['lat'],\n",
    "               c='black', alpha=0.2, label='80')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dae070",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    #*********************************************************************************************************************************************************************     \n",
    "    if vegetation_indices:\n",
    "        \n",
    "        print('11)   Vegetation indices time series')\n",
    "        t91 = ptime.time()\n",
    "\n",
    "        import ee\n",
    "        ee.Authenticate() #For authentifications we require a Google Account registered in GEE (https://earthengine.google.com/)\n",
    "        ee.Initialize()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0438988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # create climatological footprint as ee object\n",
    "        for i in years:\n",
    "            print('      Creating climatological footprint for %d'%i) \n",
    "            # create geometries \n",
    "            for n in range(len(rst_var)):\n",
    "                df = globals()['footprint_%s_%d' %(int(rst_var[n]),i)].to_numpy().tolist()\n",
    "                globals()['df_%s_%d_poly' %(int(rst_var[n]),i)] = ee.Geometry.Polygon(coords = df)\n",
    "                print('      Transforming df_%s_%d_poly' %(int(rst_var[n]),i))\n",
    "\n",
    "            lon_lat         =  [longitude, latitude]\n",
    "            point = ee.Geometry.Point(lon_lat)\n",
    "            globals()['df_fetch_%s_poly' %i]   = point.buffer(fetch)\n",
    "            print('      Transforming df_fetch_%s_poly' %i)\n",
    "            \n",
    "            # create areas\n",
    "            globals()['area_%s_%d' %(int(rst_var[0]),i)] = globals()['df_%s_%d_poly' %(int(rst_var[0]),i)]\n",
    "            print('      Creating area_%s_%d' %(int(rst_var[0]),i))\n",
    "            \n",
    "            \n",
    "            for n in range(len(rst_var)-1):\n",
    "                globals()['area_%s_%d' %(int(rst_var[n+1]),i)]  = globals()['df_%s_%d_poly' %(int(rst_var[n+1]),i)].difference(globals()['df_%s_%d_poly' %(int(rst_var[n]),i)])\n",
    "                print('      Creating area_%s_%d' %(int(rst_var[n+1]),i))  \n",
    "\n",
    "            globals()['area_100_%d' %(i)]  = globals()['df_fetch_%s_poly' %i].difference(globals()['df_%s_%d_poly' %(int(rst_var[-1]),i)])\n",
    "            print('      Creating area_100_%d ' %(i))\n",
    "            print('\\n') \n",
    "\n",
    "        # create range according to data in the input datafiles   \n",
    "        start   = '%s-01-01'   %years[0]                                               #2017-05-12 starts frequency of 10 days                                               \n",
    "        end     = '%s-12-31'   %years[-1]                                              #2017-12-18 starts frequency of 5 days\n",
    "        timeSD  = [start, end]\n",
    "\n",
    "        # create coordinates of the eddy covariance tower\n",
    "        lon_lat         =  [longitude, latitude]         \n",
    "        point = ee.Geometry.Point(lon_lat)\n",
    "\n",
    "        # collections google earth engine    \n",
    "        COPERNICUS_S2_L2A = 'COPERNICUS/S2_SR'                   #Multi-spectral surface reflectances (https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR)\n",
    "        MODIS_temp        = 'MODIS/006/MOD11A1'                  #Land surface temperature (https://developers.google.com/earth-engine/datasets/catalog/MODIS_006_MOD11A1)\n",
    "        USAID_prec        = 'UCSB-CHG/CHIRPS/DAILY'              #InfraRed Precipitation with Station dat (https://developers.google.com/earth-engine/datasets/catalog/UCSB-CHG_CHIRPS_DAILY)\n",
    "        MODIS_GPP         = 'MODIS/006/MOD17A2H'                 #Gross primary productivity(https://developers.google.com/earth-engine/datasets/catalog/MODIS_006_MOD17A2H)\n",
    "        MODIS_NPP         = 'MODIS/006/MOD17A3HGF'               #Net primary productivity (https://developers.google.com/earth-engine/datasets/catalog/MODIS_006_MOD17A3HGF)\n",
    "        \n",
    "        # bands of the EO products used in the analysis\n",
    "        # image.bandNames().getInfo() can be used to request bands of colections as well\n",
    "        COPERNICUS_S2_bands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', 'AOT', 'WVP', 'SCL', 'TCI_R', 'TCI_G', 'TCI_B', 'QA10', 'QA20', 'QA60']\n",
    "        MODIS_temp_bands    = ['LST_Day_1km','QC_Day','Day_view_time','Day_view_angle','LST_Night_1km','QC_Night','Night_view_time','Night_view_angle','Emis_31','Emis_32','Clear_day_cov','Clear_night_cov']\n",
    "        USAID_prec_bands    = ['precipitation']\n",
    "        MODIS_GPP_bands     = ['Gpp', 'PsnNet', 'Psn_QC']\n",
    "        MODIS_NPP_bands     = ['Npp', 'Npp_QC']\n",
    "\n",
    "        # function to load data set with specified period and location\n",
    "        def load_catalog(catalog, time, location, bands):\n",
    "            dataset = ee.ImageCollection(catalog).filterDate(time[0],time[1]).filterBounds(location).select(bands)\n",
    "            return dataset\n",
    "\n",
    "        # cloud coverage filter function\n",
    "        def cloud_filter(collection, cloud_coverage_metadata_name, threshold):\n",
    "            collection_cf = collection.filterMetadata(cloud_coverage_metadata_name,'less_than', threshold)\n",
    "            return collection_cf\n",
    "        \n",
    "        # function to derive VIs\n",
    "        def calculateVI(image):\n",
    "            '''This method calculates different vegetation indices in a image collection and adds their values as new bands'''\n",
    "\n",
    "            # defining dictionary of bands Sentinel-2 \n",
    "            dict_bands = {\n",
    "\n",
    "                \"blue\"  :  'B2',                              #Blue band                        \n",
    "                \"green\" :  'B3',                              #Green band\n",
    "                \"red\"   :  'B4',                              #Red band\n",
    "\n",
    "                \"red1\"  :  'B5',                              #Red-edge spectral band   \n",
    "                \"red2\"  :  'B6',                              #Red-edge spectral band\n",
    "                \"red3\"  :  'B7',                              #Red-edge spectral band    \n",
    "                \"NIR\"   :  'B8',                              #Near-infrared band\n",
    "                \"NIRn\"  :  'B8A',                             #Near-infrared narrow\n",
    "\n",
    "                \"WV\"    :  'B9',                              #Water vapour\n",
    "                \"SWIR1\" :  'B11',                             #Short wave infrared 1\n",
    "                \"SWIR2\" :  'B12',                             #Short wave infrared 2\n",
    "            }\n",
    "            \n",
    "            # specify bands \n",
    "            dict  = dict_bands\n",
    "            blue  = dict[\"blue\"]                              #Blue band                        \n",
    "            green = dict[\"green\"]                             #Green band\n",
    "            red   = dict[\"red\"]                               #Red band\n",
    "            red1  = dict[\"red1\"]                              #Red-edge spectral band    \n",
    "            red2  = dict[\"red2\"]                              #Red-edge spectral band\n",
    "            red3  = dict[\"red3\"]                              #Red-edge spectral band\n",
    "            NIR   = dict[\"NIR\"]                               #Near-infrared band\n",
    "            NIRn  = dict[\"NIRn\"]                              #Near-infrared band\n",
    "            \n",
    "            WV    = dict[\"WV\"]                                #Water vapour\n",
    "            SWIR1 = dict[\"SWIR1\"]                             #Short wave infrared 1\n",
    "            SWIR2 = dict[\"SWIR2\"]                             #Short wave infrared 2\n",
    "\n",
    "            bands_for_expressions = {\n",
    "                \n",
    "                'blue'  : image.select(blue).divide(10000),\n",
    "                'green' : image.select(green).divide(10000), \n",
    "                'red'   : image.select(red).divide(10000),\n",
    "                'red1'  : image.select(red1).divide(10000), \n",
    "                'red2'  : image.select(red2).divide(10000),\n",
    "                'red3'  : image.select(red3).divide(10000), \n",
    "                'NIR'   : image.select(NIR).divide(10000),\n",
    "                'NIRn'  : image.select(NIRn).divide(10000),\n",
    "                'WV'    : image.select(WV).divide(10000),\n",
    "                'SWIR1' : image.select(SWIR1).divide(10000),\n",
    "                'SWIR2' : image.select(SWIR2).divide(10000)}\n",
    "\n",
    "            # greeness related indices\n",
    "            # NDVI                                                                            (Rouse et al., 1974)\n",
    "            NDVI  = image.normalizedDifference([NIR, red]).rename(\"NDVI\") \n",
    "            # EVI                                                                             \n",
    "            EVI   = image.expression('2.5*(( NIR - red ) / ( NIR + 6 * red - 7.5 * blue + 1 ))', \n",
    "                    bands_for_expressions).rename(\"EVI\")\n",
    "            # EVI2                                                                            (Jiang et al., 2008)\n",
    "            EVI2  = image.expression('2.5*(( NIR - red ) / ( NIR + 2.4 * red + 1 ))', \n",
    "                    bands_for_expressions).rename(\"EVI2\")\n",
    "            \n",
    "            # greeness related indices with Sentinel-2 narrow bands / Red-edge\n",
    "            # Clr\n",
    "            CLr  = image.expression('(red3/red1)-1', bands_for_expressions).rename(\"CLr\")\n",
    "            # Clg\n",
    "            Clg  = image.expression('(red3/green)-1', bands_for_expressions).rename(\"CLg\")\n",
    "            # MTCI\n",
    "            MTCI = image.expression('(red2-red1)/(red1-red)', bands_for_expressions).rename(\"MTCI\")\n",
    "            # MNDVI                                                                          (Add reference)\n",
    "            MNDVI = image.normalizedDifference([red3, red1]).rename(\"MNDVI\")    \n",
    "            \n",
    "            # water related indices\n",
    "            # MNDWI                                                                          (Add reference)\n",
    "            MNDWI = image.normalizedDifference([green, SWIR1]).rename(\"MNDWI\")    \n",
    "            # NDWI OR LSWI or NDII or NDMI                                                    (Add reference)\n",
    "            LSWI  = image.normalizedDifference([NIR, SWIR1]).rename(\"LSWI\")\n",
    "            # NDII                                                                            (Hunt & Qu, 2013)\n",
    "            NDII   = image.normalizedDifference([NIR, SWIR2]).rename(\"NDII\")\n",
    " \n",
    "            image1 = image.addBands(NDVI).addBands(EVI).addBands(EVI2)\n",
    "            image2 = image1.addBands(CLr).addBands(Clg).addBands(MTCI).addBands(MNDVI)\n",
    "            image3 = image2.addBands(MNDWI).addBands(LSWI).addBands(NDII)\n",
    "\n",
    "            return image3  \n",
    "        \n",
    "        # function for masking non-vegetation areas\n",
    "        def maskS2nonvegetation(image):\n",
    "\n",
    "            qa    = image.select('QA60')\n",
    "            scl   = image.select('SCL')\n",
    "            ndvi  = image.select('NDVI')\n",
    "            mndvi = image.select('MNDVI')\n",
    "\n",
    "            cloudBitMask = 1 << 10\n",
    "            cirrusBitMask = 1 << 11\n",
    "\n",
    "            #vegetationMask1 = 4 # vegetation\n",
    "            #vegetationMask2 = 5 # non-vegetated\n",
    "            #vegetationMask3 = 6 # water\n",
    "            #vegetationMask4 = 7 # unclassified\n",
    "\n",
    "            #ndviMask  = -100;   # set in the config file\n",
    "            #mndviMask = -100; \n",
    "            \n",
    "            # this mask selects vegetation + non-vegetated + water + free cloud areas + areas with VIs (NDVI and MNDVI) greater that a threshold set in the Config file\n",
    "            mask = scl.eq(4).Or(scl.eq(5)).Or(scl.eq(6)).Or(scl.eq(7)).And(qa.bitwiseAnd(cloudBitMask).eq(0)).And(qa.bitwiseAnd(cirrusBitMask).eq(0)).And(ndvi.gte(ndviMask)).And(mndvi.gte(mndviMask))\n",
    "\n",
    "            vegetation = image.updateMask(mask)\n",
    "\n",
    "            return vegetation\n",
    "        \n",
    "        # function to transform ee objects to dataframes pandas objects\n",
    "        # function that transforms arrays into dataframes\n",
    "        def ee_array_to_df(imagecollection, geometry, scale):\n",
    "            \n",
    "            \"\"\"Transforms client-side ee.Image.getRegion array to pandas.DataFrame.\"\"\"\n",
    "            \n",
    "            # select bands from the image collection\n",
    "            filtered = imagecollection.select(bands)\n",
    "            \n",
    "            # function that produce functions to reduce a region with atatistics (mean, max, min, etc.)\n",
    "            def create_reduce_region_function(geometry,\n",
    "                                              reducer=ee.Reducer.mean(),\n",
    "                                              scale=1000,\n",
    "                                              crs=crs,\n",
    "                                              bestEffort=True,\n",
    "                                              maxPixels=1e13,\n",
    "                                              tileScale=4):\n",
    "\n",
    "                  def reduce_region_function(img):\n",
    "\n",
    "                    stat = img.reduceRegion(\n",
    "                        reducer=reducer,\n",
    "                        geometry=geometry,\n",
    "                        scale=scale,\n",
    "                        crs=crs,\n",
    "                        bestEffort=bestEffort,\n",
    "                        maxPixels=maxPixels,\n",
    "                        tileScale=tileScale)\n",
    "\n",
    "                    return ee.Feature(geometry, stat).set({'millis': img.date().millis()})\n",
    "                  return reduce_region_function\n",
    "                \n",
    "            # function to transfer feature properties to a dictionary.\n",
    "            def fc_to_dict(fc):\n",
    "                    prop_names = fc.first().propertyNames()\n",
    "                    prop_lists = fc.reduceColumns(reducer=ee.Reducer.toList().repeat(prop_names.size()),selectors=prop_names).get('list')\n",
    "\n",
    "                    return ee.Dictionary.fromLists(prop_names, prop_lists)\n",
    "            \n",
    "            # creating reduction function (reduce_VI is a function)\n",
    "            reduce_VI = create_reduce_region_function(\n",
    "                geometry= geometry, reducer=ee.Reducer.mean(), scale=10, crs= crs)\n",
    "            \n",
    "            # transform image collection into feature collection (tables)\n",
    "            VI_stat_fc = ee.FeatureCollection(imagecollection.map(reduce_VI)).filter(\n",
    "                ee.Filter.notNull(imagecollection.first().bandNames()))\n",
    "            \n",
    "            # transform feature collection into dictionary object\n",
    "            VI_dict = fc_to_dict(VI_stat_fc).getInfo()\n",
    "\n",
    "            #print(type(VI_dict), '\\n')\n",
    "            \n",
    "            #for prop in VI_dict.keys():\n",
    "            #    print(prop + ':', VI_dict[prop][0:3] + ['...'])\n",
    "            \n",
    "            # transform dictionary into dataframe\n",
    "            VI_df = pd.DataFrame(VI_dict)\n",
    "            \n",
    "            # convert column in datatime type object\n",
    "            #VI_df['datetime'] = pd.to_datetime(VI_df['time'], unit='ms')\n",
    "            VI_df['date']     = pd.to_datetime(VI_df['millis'], unit='ms').dt.date\n",
    "            \n",
    "            # generate a list with the names of each band of the collection \n",
    "            list_of_bands = filtered.first().bandNames().getInfo()\n",
    "            \n",
    "            # remove rows without data inside.\n",
    "            VI_df = VI_df[['date', *list_of_bands]].dropna()\n",
    "            \n",
    "            # convert the data to numeric values.\n",
    "            for band in list_of_bands:\n",
    "                VI_df[band] = pd.to_numeric(VI_df[band], errors='coerce', downcast ='float')\n",
    "                \n",
    "            # convert the time field into a datetime.\n",
    "            #VI_df['datetime'] = pd.to_datetime(VI_df['time'], unit='ms')\n",
    "            #VI_df['date']     = pd.to_datetime(VI_df['time'], unit='ms').dt.date\n",
    "\n",
    "            # keep the columns of interest.\n",
    "            #VI_df = VI_df[['datetime','date',  *list_of_bands]]\n",
    "            \n",
    "            # flag to identify if in the reduction there were pixels, or they were masked-removed\n",
    "            VI_df['flag'] = 100\n",
    "            \n",
    "            # reduction in case there are two pixels from different images for the same day\n",
    "            VI_df = VI_df.groupby('date').mean().reset_index().set_index('date').copy()\n",
    "\n",
    "            return VI_df\n",
    "\n",
    "        # applying functions \n",
    "\n",
    "        # request of catalogues \n",
    "        S2     = load_catalog(COPERNICUS_S2_L2A, timeSD, point, COPERNICUS_S2_bands)\n",
    "        temp   = load_catalog(MODIS_temp,        timeSD, point, MODIS_temp_bands)\n",
    "        prec   = load_catalog(USAID_prec,        timeSD, point, USAID_prec_bands)\n",
    "        gpp_MODIS    = load_catalog(MODIS_GPP,         timeSD, point, MODIS_GPP_bands)\n",
    "        npp_MODIS    = load_catalog(MODIS_NPP,         timeSD, point,  MODIS_NPP_bands)\n",
    "\n",
    "        # filter cloud coverage\n",
    "        cloud_coverage_metadata_name = 'CLOUDY_PIXEL_PERCENTAGE'                     # name of metadata property indicating cloud coverage in %\n",
    "\n",
    "        # applying cloud filter \n",
    "        S2_VI = cloud_filter(S2, cloud_coverage_metadata_name, max_cloud_coverage)   # max cloud coverage defined in the Config file\n",
    "\n",
    "        # calculation of vegetation indices for the collection\n",
    "        S2_VI = S2_VI.map(calculateVI)\n",
    "        \n",
    "        # applying mask \n",
    "        S2_VI = S2_VI.map(maskS2nonvegetation)\n",
    "\n",
    "        # select bands for the analysis of interdependency and regression models \n",
    "        #bands = ['NDVI','EVI','EVI2','CLr','CLg','MTCI','MNDVI','MNDWI','LSWI','NDII']\n",
    "        bands = ['NDVI','EVI','EVI2','CLr','MNDVI','MNDWI','LSWI','NDII']\n",
    "    \n",
    "        # applying funtion ee_array_to_df for each climatological footprint\n",
    "        for i in years:\n",
    "            for n in range(len(rst_var)):\n",
    "\n",
    "                    area = globals()['area_%s_%d' %(int(rst_var[n]),i)]\n",
    "                    globals()['S2_VI_df_area_%s_%d' %(int(rst_var[n]),i)]  = ee_array_to_df(S2_VI, area, 60)          \n",
    "                    print('      Retrieving info from area_%s_%d into dataframe:' %(int(rst_var[n]),i))\n",
    "                    print('      S2_VI_df_area_%s_%d' %(int(rst_var[n]),i))\n",
    "\n",
    "\n",
    "            area = globals()['area_100_%d' %i]\n",
    "            globals()['S2_VI_df_area_100_%d' %i]  = ee_array_to_df(S2_VI, area, 60) \n",
    "            print('      Retrieving info from area_100_%d into dataframe:' %i)\n",
    "            print('      S2_VI_df_area_100_%d' %i)\n",
    "            print('\\n')\n",
    "\n",
    "        # loop to derive a weighted value of each VI per climatological footprint\n",
    "        for i in years:\n",
    "            \n",
    "            # create an empty file\n",
    "            globals()['dfvi_%s'%i] = pd.DataFrame(columns=bands)\n",
    "            \n",
    "            for n in range(len(rst_var)):\n",
    "\n",
    "                    df = globals()['S2_VI_df_area_%s_%d' %(int(rst_var[n]),i)].multiply(0.2)  \n",
    "                    print('      Concat S2_VI_df_area_%s_%d' %(int(rst_var[n]),i))\n",
    "                    globals()['dfvi_%s'%i] = pd.concat([globals()['dfvi_%s'%i], df])\n",
    "\n",
    "            df = globals()['S2_VI_df_area_100_%d' %i].multiply(0.2)\n",
    "            print('      Concat S2_VI_df_area_100_%d' %i)\n",
    "            globals()['dfvi_%s'%i] = pd.concat([globals()['dfvi_%s'%i] , df])\n",
    "            \n",
    "            globals()['S2_VI_ffp_%s'%i] = globals()['dfvi_%s'%i].groupby(globals()['dfvi_%s'%i].index).sum().rename_axis('date')\n",
    "            print('      Creating S2_VI_ffp_%s'%i)\n",
    "            print('\\n') \n",
    "\n",
    "        # loop to derive the mean value of the vegetation indixes per region and per year\n",
    "        for i in years:\n",
    "\n",
    "            globals()['dfvi_mean_%s'%i] = pd.DataFrame(columns=bands)\n",
    "\n",
    "            for n in range(len(rst_var)):\n",
    "\n",
    "                    df = globals()['S2_VI_df_area_%s_%d' %(int(rst_var[n]),i)].describe().loc['mean':'std',bands[0]: bands[-1]]                  \n",
    "                    df.drop(['std'], inplace = True)\n",
    "                    df['Index'] = int(rst_var[n])\n",
    "                    df.set_index('Index', inplace = True)\n",
    "                    globals()['dfvi_mean_%s'%i] = pd.concat([globals()['dfvi_mean_%s'%i], df])\n",
    "\n",
    "            df = globals()['S2_VI_df_area_100_%d' %i].describe().loc['mean':'std',bands[0]: bands[-1]]\n",
    "            df.drop(['std'], inplace = True)\n",
    "            df['Index'] = int(100)\n",
    "            df.set_index('Index', inplace = True)\n",
    "\n",
    "            globals()['dfvi_mean_%s'%i] = pd.concat(([globals()['dfvi_mean_%s'%i] , df]), axis = 0)\n",
    "            globals()['dfvi_mean_%s'%i].to_csv(outputdir + '/VI_output/' + ID + '_dfvi_mean_%s'%i+'.txt')\n",
    "            print('      Creating dfvi_mean_%s'%i)\n",
    "            print('\\n') \n",
    "\n",
    "        # loop to join each S2_VI_df in a single dataframe per year named as df_VI\n",
    "        df_VI = pd.DataFrame(columns=bands)\n",
    "        \n",
    "        ndvi_threshold = -100                                                            # Threshold applied in the time series. Not used in the analysis\n",
    "\n",
    "        for i in years:\n",
    "\n",
    "            globals()['S2_VI_ffp_%s_join'%i] = globals()['S2_VI_ffp_%s'%i][globals()['S2_VI_ffp_%s'%i]['NDVI']>ndvi_threshold]\n",
    "\n",
    "            def add_date_info(df):\n",
    "\n",
    "                df = df.reset_index()\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "                df['Year'] = pd.DatetimeIndex(df['date']).year\n",
    "\n",
    "                def timestamp(df):\n",
    "                    df = df.timestamp()\n",
    "                    return df\n",
    "\n",
    "                df['timestamp'] = df['date'].apply(timestamp)\n",
    "                df = df.set_index('date')\n",
    "                \n",
    "                return df\n",
    "\n",
    "            globals()['S2_VI_ffp_%s_join'%i]  = add_date_info(globals()['S2_VI_ffp_%s_join'%i] )\n",
    "            globals()['S2_VI_ffp_%s_join'%i]  = globals()['S2_VI_ffp_%s_join'%i][globals()['S2_VI_ffp_%s_join'%i]['Year'] == i]\n",
    "\n",
    "            df = globals()['S2_VI_ffp_%s_join'%i]\n",
    "            print('      Concat S2_VI_ffp_%s_join'%i)\n",
    "\n",
    "            df_VI = pd.concat([df_VI, df]).rename_axis('date')\n",
    "        print('      Creating df_VI')\n",
    "        print('\\n') \n",
    "\n",
    "        # filtering VI_eshape by flag. Removes dates when there were areas whitin the climatological footprint that were totally masked \n",
    "        df_VI_filtered = df_VI.copy()\n",
    "        df_VI_filtered = df_VI_filtered[df_VI_filtered['flag']>80].drop(['flag'], axis = 1)\n",
    "\n",
    "        # create time series with daily frequency\n",
    "        time_series = pd.date_range(start=start, end=end, freq=\"D\")\n",
    "        time_series = pd.DataFrame(time_series).rename(columns={0: 'date'}).set_index('date')\n",
    "\n",
    "        # allows to have a time series with daily frequency with gaps when the VI were not calculated or there were not S2 images\n",
    "        df_VI_time = pd.merge(left= time_series, right = df_VI_filtered,\n",
    "                                     how=\"left\", left_index = True , right_index = True)  \n",
    "        \n",
    "        # interpolate values\n",
    "        df_VI_interpolate = df_VI_time.interpolate(method='akima', order=1, limit_direction ='forward')\n",
    "        #df_VI_interpolate_limits = df_VI_interpolate.apply(lambda x: x.interpolate(method=\"spline\", order=6))\n",
    "        df_VI_interpolate_limits = df_VI_interpolate.fillna(method='backfill')\n",
    "        df_VI_interpolate_limits = df_VI_interpolate_limits.fillna(method='ffill')\n",
    "\n",
    "        # file to extrapolate\n",
    "        df_VI_export = df_VI_interpolate_limits.dropna().drop(['Year','timestamp'], axis = 1) \n",
    "        df_VI_export.to_csv(outputdir + '/VI_output/' + ID + \"_Vegetation_indices.txt\")   \n",
    "\n",
    "        print(\"      Exporting: Vegetation_indices.txt\")\n",
    "        print('\\n') \n",
    "\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "\n",
    "        t92   = ptime.time()\n",
    "        strin = ( '{:.1f} [minutes]'.format((t92 - t91) / 60.)                                                      \n",
    "                  if (t92 - t91) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t92 - t91))\n",
    "                )\n",
    "        print('     Computation vegetation indices in ', strin)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d907ab6c",
   "metadata": {},
   "source": [
    "# Plots vegetation indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e737392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display plots of VI\n",
    "def plot_timeseries_vi_multiple(df):\n",
    "    # subplots.\n",
    "    fig, ax = plt.subplots(figsize=(14, 6)) #Indicates the size of the plot\n",
    "\n",
    "    # add scatter plots //Adds the scatter points\n",
    "    ax.plot(df['NDVI'],\n",
    "               c='#00FF00', alpha=1, label='NDVI', lw=2, linestyle = ':')\n",
    "    ax.plot(df['EVI'],\n",
    "               c='red', alpha=1, label='EVI', lw=2, linestyle = ':')\n",
    "    ax.plot(df['EVI2'],\n",
    "               c='yellow', alpha=1, label='EVI-2', lw=2, linestyle = ':')\n",
    "    ax.plot(df['CLr'],\n",
    "               c='green', alpha=1, label='CLr', lw=2, linestyle = ':')\n",
    "    ax.plot(df['MNDVI'],\n",
    "               c='black', alpha=1, label='MNDVI', lw=2, linestyle = ':')\n",
    "    ax.plot(df['MNDWI'],\n",
    "               c='#00FFFF', alpha=0.5, label='MNDWI', lw=2, linestyle = '-.')\n",
    "    ax.plot(df['LSWI'],\n",
    "               c='blue', alpha=0.8, label='LSWI', lw=2, linestyle = '-.')\n",
    "    ax.plot(df['NDII'],\n",
    "               c='#00008B', alpha=0.8, label='NDII', lw=2, linestyle = '-.') #marker=\"x\", markersize=2)\n",
    "\n",
    "    ax.set_title('Vegetation Indices', fontsize=16)\n",
    "    ax.set_xlabel('Date', fontsize=14)\n",
    "    ax.set_ylabel('Vegetation Index', fontsize=14)\n",
    "    ax.set_ylim(-1, 1)\n",
    "    ax.grid(lw=0.5)\n",
    "    ax.legend(fontsize=14, loc='lower right')\n",
    "    \n",
    "    # shrink current axis by 20%\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "    # put a legend to the right of the current axis\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, .8))\n",
    "    plt.savefig(outputdir + '/VI_output/' + ID + '_VI_timeseries.png', dpi=300, format='png', bbox_inches='tight',pad_inches=0.0001)\n",
    "\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e58bae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries_vi_single(df,df_original, VI):\n",
    "    \n",
    "    fig, ay = plt.subplots(figsize=(14, 6)) #Indicates the size of the plot\n",
    "    ay.plot(df[VI], c='#00FF00', alpha=0.2, lw=5, label='Akima interpolation')\n",
    "\n",
    "    df = df.reset_index()\n",
    "    dff= df_original.reset_index()\n",
    "    \n",
    "    dff.plot(kind=\"scatter\",x='date', y=VI, ax=ay, c='GREEN', alpha=0.80, label=VI.upper())\n",
    "\n",
    "    ay.legend(fontsize=12, loc='upper right')\n",
    "\n",
    "    ay.grid(lw=0.5)\n",
    "\n",
    "    ay.set_title(VI, fontsize=16)\n",
    "    ay.set_xlabel('Date', fontsize=14)\n",
    "    ay.set_ylabel(VI, fontsize=14)\n",
    "    \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f9a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_VI_export.copy()\n",
    "plot_timeseries_vi_multiple(df)\n",
    "plot_timeseries_vi_single(df, df_VI_time, 'LSWI')\n",
    "plot_timeseries_vi_single(df, df_VI_time, 'MNDVI')\n",
    "plot_timeseries_vi_single(df, df_VI_time, 'NDVI')\n",
    "plot_timeseries_vi_single(df, df_VI_time, 'EVI')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee367d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display info within specific dates\n",
    "df = df_VI_export.copy()\n",
    "\n",
    "date1 = '2019-01-01'\n",
    "date2 = '2019-12-31'\n",
    "\n",
    "a1 = df.loc[(df.index >= date1) & (df.index < date2)]\n",
    "b1 = a1.describe()\n",
    "c1 = a1.copy().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc871f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #*********************************************************************************************************************************************************************     \n",
    "    # list of dataframes with meteorological information\n",
    "    EV_dataframes = []\n",
    "    EV_dataframes_names = []\n",
    "    #*********************************************************************************************************************************************************************     \n",
    "    if environmental_variables_station:\n",
    "        \n",
    "        print('12a) Environmental variables time series from station')\n",
    "        t101a = ptime.time()\n",
    "        \n",
    "        # function to apply rolling average and interpolation to meteorological data \n",
    "        def interpolation_rolling(df,window):\n",
    "            df = df.interpolate(method='akima', order=1, limit_direction ='forward')\n",
    "            df = df.rolling(window, center=True, min_periods=1).mean()\n",
    "            return df\n",
    "        \n",
    "        # directory with the Data\n",
    "        filepath = os.getcwd() + '\\\\Input'\n",
    "        first_file= meteo_file\n",
    "        \n",
    "        # reading file meteorological station\n",
    "        parser = lambda date: dt.datetime.strptime(date, '%Y-%m-%d')\n",
    "        \n",
    "        # meteorological data\n",
    "        df_meteo_station = pd.read_csv(filepath + first_file,parse_dates=[0], date_parser=parser, index_col = [0])\n",
    "        \n",
    "        \n",
    "        if df_rainfall_station_switch:\n",
    "            \n",
    "            print('      Computing: df_rainfall_station \\n')\n",
    "            \n",
    "            # rainfall data\n",
    "            df_rainfall_station = df_meteo_station.rename(columns={ precipitation_data[0] : 'RAIN'}).loc[:,'RAIN'].copy()  \n",
    "            df_rainfall_station = pd.DataFrame(df_rainfall_station)\n",
    "\n",
    "            # rainfall data with diferrent windows and delays\n",
    "            window = [7,15,30,60,90]\n",
    "            shifts = [7,15,30,60,90,120,150,180]\n",
    "\n",
    "            for w in window:\n",
    "                df_rainfall_station['RAIN_%s'%w]  = df_rainfall_station['RAIN'].rolling(window=w, center=True, min_periods=1).mean()\n",
    "\n",
    "                for sh in shifts:\n",
    "                        df_rainfall_station['RAIN_%s_%s'%(w,sh)]  = df_rainfall_station['RAIN_%s'%w].shift(+sh)\n",
    "                           \n",
    "            EV_dataframes.append(df_rainfall_station)\n",
    "            EV_dataframes_names.append(\"df_rainfall_station\")\n",
    "                        \n",
    "        if df_meteo_station_switch:\n",
    "            \n",
    "            print('      Computing: df_meteo_statio  \\n')\n",
    "            \n",
    "            df_meteo_station   = interpolation_rolling(df_meteo_station,rolling_window_ev_meteo).drop(precipitation_data, axis = 1)\n",
    "            \n",
    "            EV_dataframes.append(df_meteo_station)\n",
    "            EV_dataframes_names.append(\"df_meteo_station\")\n",
    "\n",
    "        t102a   = ptime.time()\n",
    "        strin = ( '{:.1f} [minutes]'.format((t102a - t101a) / 60.)                                                      \n",
    "                  if (t102a - t101a) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t102a - t101a))\n",
    "                )\n",
    "        print('     Computation environmental variables time series from station in', strin)  \n",
    "        print('\\n')\n",
    "\n",
    "    #*********************************************************************************************************************************************************************     \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cf4065",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_meteo = 'TH'\n",
    "\n",
    "# print time series\n",
    "import altair as alt\n",
    "\n",
    "data_meteo        = df_meteo_station.reset_index()\n",
    "data_meteo_one    = data_meteo[['date',variable_meteo]].copy()\n",
    "data_meteo_one    = data_meteo_one.rename(columns={\n",
    "    variable_meteo: 'Variable',\n",
    "})\n",
    "data_co_fluxes.head(10)\n",
    "\n",
    "alt.Chart(data_meteo_one).mark_bar(size=1).encode(\n",
    "    x='date:T',\n",
    "    y='Variable:Q',\n",
    "    color=alt.Color(\n",
    "        'Variable:Q', scale=alt.Scale(scheme='redyellowgreen', domain=(0, 14))),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('date:T', title='Date'),\n",
    "        alt.Tooltip('Variable:Q', title='Variable')\n",
    "    ]).properties(width=600, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf7e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "   if environmental_variables_satellite:\n",
    "        \n",
    "        print('12b) Environmental variables time series from satellite products')\n",
    "        t101b = ptime.time()\n",
    "        \n",
    "        # function to transform ee objects into dataframes. It cannot process high resolution data due to the limitations of memory of ee API \n",
    "        # this method is only practical for coarse resolution Satellite Images. Give information for a specific pixel in the case of Sentinel 2\n",
    "        def ee_array_to_df_coarse(imagecollection, geometry, scale):\n",
    "\n",
    "            \"\"\"Transforms client-side ee.Image.getRegion array to pandas.DataFrame.\"\"\"\n",
    " \n",
    "            # ee objects are transformed into arrays for specific region (point + buffer)\n",
    "            arr = imagecollection.getRegion(geometry, scale).getInfo()\n",
    "\n",
    "            # generate a list with the names of each band of the collection \n",
    "            list_of_bands = imagecollection.first().bandNames().getInfo()\n",
    "\n",
    "            # transforms arrays into dataframes\n",
    "            df = pd.DataFrame(arr)\n",
    "\n",
    "            # rearrange the header.\n",
    "            headers = df.iloc[0]\n",
    "            df = pd.DataFrame(df.values[1:], columns=headers).dropna()\n",
    "            \n",
    "            # convert the time field into a datetime.\n",
    "            df['date']     = pd.to_datetime(df['time'], unit='ms') #.dt.date\n",
    "\n",
    "            # convert the data to numeric values.\n",
    "            for band in list_of_bands:\n",
    "                df[band] = pd.to_numeric(df[band], errors='coerce', downcast ='float')\n",
    "\n",
    "            # keep the columns of interest.\n",
    "            df = df[['date',  *list_of_bands]]\n",
    "            df = df.groupby('date').mean().reset_index().set_index('date')\n",
    "            \n",
    "            # create a time series with daily frequency\n",
    "            time_series = pd.date_range(start, end, freq=\"D\")\n",
    "            time_series = pd.DataFrame(time_series).rename(columns={0: 'date'}).set_index('date')\n",
    "\n",
    "            df  = pd.merge(left= time_series, right = df,\n",
    "                           how=\"left\", left_on =\"date\", right_on =\"date\")  \n",
    "            return df\n",
    "        \n",
    "        # function to convert temperature to C // Important to observe .apply method to apply a function over a list or df\n",
    "        def t_modis_to_celsius(t_modis): \n",
    "            \"\"\"Converts MODIS LST units to degrees Celsius.\"\"\"\n",
    "            t_celsius =  0.02*t_modis - 273.15\n",
    "            return t_celsius\n",
    "        \n",
    "        # function to apply rolling average and interpolation to meteorological data \n",
    "        def interpolation_rolling(df,window):\n",
    "            df = df.interpolate(method='akima', order=1, limit_direction ='forward')\n",
    "            df = df.rolling(window, center=True, min_periods=1).mean()\n",
    "            return df\n",
    "            \n",
    "        if df_rainfall_CHIRPS_switch:\n",
    "            print('      Computing: df_rainfall_CHIRPS')\n",
    "            # applying functions to derive precipitation data from MODIS catalog\n",
    "            prec_dataframe     = ee_array_to_df_coarse(prec, point.buffer(fetch),100)\n",
    "            prec_dataframe     = prec_dataframe.rename(columns={ 'precipitation': 'RAIN_C'})       \n",
    "            df_rainfall_CHIRPS = prec_dataframe.copy()\n",
    "\n",
    "            # rainfall data with diferrent windows and delays\n",
    "            window = [7,15,30,60,90]\n",
    "            shifts = [7,15,30,60,90,120,150,180]\n",
    "\n",
    "            for w in window:\n",
    "                df_rainfall_CHIRPS['RAIN_C_%s'%w]  = df_rainfall_CHIRPS['RAIN_C'].rolling(window=w, center=True, min_periods=1).mean()\n",
    "\n",
    "                for sh in shifts:\n",
    "                        df_rainfall_CHIRPS['RAIN_C_%s_%s'%(w,sh)]  = df_rainfall_CHIRPS['RAIN_C_%s'%w] .shift(+sh)\n",
    "\n",
    "            EV_dataframes.append(df_rainfall_CHIRPS)\n",
    "            EV_dataframes_names.append(\"df_rainfall_CHIRPS\")\n",
    "            \n",
    "        if df_temp_MODIS_switch:\n",
    "            print('      Computing: df_temp_MODIS')\n",
    "            # applying functions to derive temperature data from MODIS catalog\n",
    "            temp_dataframe = ee_array_to_df_coarse(temp.select('LST_Day_1km'), point.buffer(fetch),100)\n",
    "            temp_dataframe = temp_dataframe.rename(columns={'LST_Day_1km': 'TA_MODIS'}).apply(t_modis_to_celsius)\n",
    "            df_temp_MODIS  = interpolation_rolling(temp_dataframe,rolling_window_ev_meteo_sat)\n",
    "            \n",
    "            EV_dataframes.append(df_temp_MODIS)\n",
    "            EV_dataframes_names.append(\"df_temp_MODIS\")\n",
    "            \n",
    "        print('      Computing: df_gpp_MODIS')\n",
    "        print('      Computing: df_npp_MODIS  \\n')\n",
    "        # applying functions to derive gpp data from MODIS catalog\n",
    "        gpp_dataframe  = ee_array_to_df_coarse(gpp_MODIS, point.buffer(fetch),100)\n",
    "        gpp_dataframe  = gpp_dataframe.divide(8).multiply(1000).multiply(0.0001)\n",
    "        gpp_dataframe  = gpp_dataframe.rename(columns={ 'Gpp': 'GPP_MODIS'}).drop(['PsnNet', 'Psn_QC'], axis = 1)\n",
    "        df_gpp_MODIS   = interpolation_rolling(gpp_dataframe,rolling_window_gpp_MODIS)\n",
    "        \n",
    "        # applying functions to derive precipitation data from MODIS catalog\n",
    "        npp_dataframe = ee_array_to_df_coarse(npp_MODIS, point.buffer(fetch),100).multiply(1000).multiply(0.0001)\n",
    "        df_npp_MODIS  = npp_dataframe.rename(columns={ 'Npp': 'NPP_MODIS'}).drop('Npp_QC', axis = 1)\n",
    "\n",
    "        t102b   = ptime.time()\n",
    "        strin = ( '{:.1f} [minutes]'.format((t102b - t101b) / 60.)                                                      \n",
    "                  if (t102b - t101b) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t102b - t101b))\n",
    "                )\n",
    "        print('     Computation environmental variables time series from satellite products in', strin)  \n",
    "        print('\\n')\n",
    "\n",
    "    #*********************************************************************************************************************************************************************     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bf16c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "variable_meteo = 'GPP_MODIS'\n",
    "\n",
    "# print time series\n",
    "import altair as alt\n",
    "\n",
    "data_meteo        = df_gpp_MODIS.reset_index()\n",
    "data_meteo_one    = data_meteo[['date',variable_meteo]].copy()\n",
    "data_meteo_one    = data_meteo_one.rename(columns={\n",
    "    variable_meteo: 'Variable',\n",
    "})\n",
    "data_co_fluxes.head(10)\n",
    "\n",
    "alt.Chart(data_meteo_one).mark_bar(size=1).encode(\n",
    "    x='date:T',\n",
    "    y='Variable:Q',\n",
    "    color=alt.Color(\n",
    "        'Variable:Q', scale=alt.Scale(scheme='redyellowgreen', domain=(0, 14))),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('date:T', title='Date'),\n",
    "        alt.Tooltip('Variable:Q', title='Variable')\n",
    "    ]).properties(width=600, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b3d30b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "  if tower_observations:\n",
    "        \n",
    "        print('12c) GPP and environmental time series from flux tower')\n",
    "        t101c = ptime.time()\n",
    "        \n",
    "        if df_meteo_tower_switch:\n",
    "            \n",
    "            print('      Computing: df_meteo_tower')\n",
    "            df_meteo_tower = df_gpp_smoth.copy(deep=True).drop([\n",
    "              #'TA_f',\n",
    "              #'VPD_f',\n",
    "              #'SW_IN_f',\n",
    "              carbonflux+'_f',\n",
    "              rei_gpp+'_f',\n",
    "              rei_res+'_f',\n",
    "              fal_gpp+'_f',\n",
    "              fal_res+'_f',\n",
    "              las_gpp+'_f',\n",
    "              las_res+'_f'], axis = 1)  \n",
    "\n",
    "            EV_dataframes.append(df_meteo_tower)\n",
    "            EV_dataframes_names.append(\"df_meteo_tower\")\n",
    "\n",
    "        erased_variables = ['TA_f', 'VPD_f', 'SW_IN_f', carbonflux+'_f', rei_res+'_f', fal_res+'_f',  las_res+'_f']   \n",
    "        \n",
    "        if rei_gpp_switch:\n",
    "            erased_variables.append(fal_gpp+'_f')\n",
    "            erased_variables.append(las_gpp+'_f')\n",
    "            gpp_analysis = rei_gpp+'_f'\n",
    "        if fal_gpp_switch:\n",
    "            erased_variables.append(rei_gpp+'_f')\n",
    "            erased_variables.append(las_gpp+'_f')\n",
    "            gpp_analysis = fal_gpp+'_f'\n",
    "        if las_gpp_switch:\n",
    "            erased_variables.append(rei_gpp+'_f')\n",
    "            erased_variables.append(fal_gpp+'_f')\n",
    "            gpp_analysis = las_gpp+'_f'\n",
    "            \n",
    "        print('      Computing: df_gpp_tower\\n')\n",
    "        df_gpp_tower = df_gpp_smoth.copy(deep=True).drop(erased_variables, axis = 1).rename(columns={ gpp_analysis: 'GPP_observations'}) \n",
    "        \n",
    "        t102c   = ptime.time()\n",
    "        strin = ( '{:.1f} [minutes]'.format((t102c - t101c) / 60.)                                                      \n",
    "                  if (t102c - t101c) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t102b - t101c))\n",
    "                )\n",
    "        print('     Computation environmental variables time series from satellite products in', strin)  \n",
    "        print('\\n')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85729eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_meteo = 'SW_IN_f'\n",
    "\n",
    "# print time series\n",
    "import altair as alt\n",
    "\n",
    "data_meteo        = df_meteo_tower.reset_index()\n",
    "data_meteo_one    = data_meteo[['date',variable_meteo]].copy()\n",
    "data_meteo_one    = data_meteo_one.rename(columns={\n",
    "    variable_meteo: 'Variable',\n",
    "})\n",
    "data_co_fluxes.head(10)\n",
    "\n",
    "alt.Chart(data_meteo_one).mark_bar(size=1).encode(\n",
    "    x='date:T',\n",
    "    y='Variable:Q',\n",
    "    color=alt.Color(\n",
    "        'Variable:Q', scale=alt.Scale(scheme='redyellowgreen', domain=(0, 14))),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('date:T', title='Date'),\n",
    "        alt.Tooltip('Variable:Q', title='Variable')\n",
    "    ]).properties(width=600, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4499c837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "variable_meteo = 'GPP_observations'\n",
    "\n",
    "# print time series\n",
    "import altair as alt\n",
    "\n",
    "data_meteo        = df_gpp_tower.reset_index()\n",
    "data_meteo_one    = data_meteo[['date',variable_meteo]].copy()\n",
    "data_meteo_one    = data_meteo_one.rename(columns={\n",
    "    variable_meteo: 'Variable',\n",
    "})\n",
    "data_co_fluxes.head(10)\n",
    "\n",
    "alt.Chart(data_meteo_one).mark_bar(size=1).encode(\n",
    "    x='date:T',\n",
    "    y='Variable:Q',\n",
    "    color=alt.Color(\n",
    "        'Variable:Q', scale=alt.Scale(scheme='redyellowgreen', domain=(0, 14))),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('date:T', title='Date'),\n",
    "        alt.Tooltip('Variable:Q', title='Variable')\n",
    "    ]).properties(width=600, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd18f601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    #*********************************************************************************************************************************************************************     \n",
    "    if correlation_analysis:\n",
    "        \n",
    "        print('13a) Correlation analysis')\n",
    "        t111a = ptime.time()\n",
    "        \n",
    "        # function that derives the analysis of interdependency betwee gpp and multiple variables, and selects the variable with higher correlation\n",
    "        def correlation_single(gpp,variable,Top):\n",
    "                \n",
    "                # merging dataframes\n",
    "                df_gpp = gpp.copy().dropna()\n",
    "                df_gpp = pd.merge(left= df_gpp, right = variable,\n",
    "                                how=\"left\", left_index = True , right_index = True) \n",
    "                \n",
    "                # correlation analysis\n",
    "                correlation = df_gpp.corr()**2\n",
    "                correlation_analysis    = correlation.reset_index().sort_values(by='GPP_observations', ascending=False)\n",
    "                correlation_analysis    = correlation_analysis[['index','GPP_observations']].set_index('index')\n",
    "\n",
    "                if isinstance(Top, int):\n",
    "                    best_model = correlation_analysis.reset_index().iloc[Top,0]\n",
    "                    best_r2    = correlation_analysis.reset_index().iloc[Top,1]\n",
    "                    \n",
    "                    print('The %s higer correlated variable with GPP_observations is %s with R^2 = %.4f'%(Top, best_model,best_r2))\n",
    "                    \n",
    "                else:\n",
    "                    best_model = Top \n",
    "                    best_r2    = correlation_analysis.loc[Top, 'GPP_observations']\n",
    "                    print('The correlation of the %s model with GPP_observations is R^2 = %.4f'%(best_model,best_r2))\n",
    "\n",
    "                # produce de linear model with the variable with higher correlation\n",
    "                lm_1    = smf.ols(formula=\"GPP_observations~%s\"%best_model, data = df_gpp).fit()\n",
    "\n",
    "                pvalues = lm_1.pvalues\n",
    "                params  = lm_1.params\n",
    "                summary = lm_1.summary()\n",
    "                print('The linear model  GPP = m (%s) + b has coefficients b = %.4f with p = %.4e and m= %.4f with p = %.4e' %(best_model,params[0],pvalues[0],params[1],pvalues[1]))\n",
    "                \n",
    "                # plot correlation between gpp and the variable with higher correlation\n",
    "                #%matplotlib inline\n",
    "                #df.plot(kind = \"scatter\", x = \"%s\"%best_model, y =\"GPP_observations\", c='green')\n",
    "\n",
    "                return correlation_analysis, params, pvalues, summary, best_model, best_r2 \n",
    "  \n",
    "        # function that evaluates the correlation between independat variables to identify multicollinearity\n",
    "        def multicollinearity(variable1,variable2,Top1, Top2):\n",
    "                \n",
    "                # merging dataframes\n",
    "                df = variable1[Top1].copy().dropna()\n",
    "                df = pd.merge(left= df, right = variable2,\n",
    "                                how=\"left\", left_index = True , right_index = True) \n",
    "                \n",
    "                # correlation analysis\n",
    "                correlation = df.corr()**2\n",
    "                correlation_analysis    = correlation.reset_index().sort_values(by=Top1, ascending=False)\n",
    "                correlation_analysis    = correlation_analysis[['index',Top1]].set_index('index')\n",
    "\n",
    "                if isinstance(Top2, int):\n",
    "                    best_model = correlation_analysis.reset_index().iloc[Top2,0]\n",
    "                    best_r2    = correlation_analysis.reset_index().iloc[Top2,1]\n",
    "                    print('The %s higer correlated variable with %s is %s with R^2 = %.4f'%(Top2, Top1, best_model,best_r2))\n",
    "                else:\n",
    "                    best_model = Top2 \n",
    "                    best_r2    = correlation_analysis.loc[Top2, Top1]\n",
    "                    print('The correlation of the %s with %s is R^2 = %.4f'%(Top1, best_model,best_r2))\n",
    "                \n",
    "                if best_r2>0.5:\n",
    "                    print('There is a high correlation between the independant variables %s and %s \\n' %(Top1, best_model))\n",
    "                else:\n",
    "                    print('There is a low correlation between the independant variables %s and %s \\n' %(Top1, best_model))\n",
    "                \n",
    "                # plot correlation betwen the selected variable from variable1 and the variable with higher correlation from variable2\n",
    "                #%matplotlib inline\n",
    "                #df.plot(kind = \"scatter\", x = \"%s\"%best_model, y =\"%s\"%Top1, c='green')\n",
    "\n",
    "                return correlation_analysis\n",
    "        \n",
    "        # function that automatically derives the analysis of interdependecy between gpp, environmental variables and vegetation indices as performed by Cai et al. 2021\n",
    "        def correlation_cai(gpp,VI,EV,Top):\n",
    "\n",
    "                df_gpp = gpp.copy().dropna()\n",
    "\n",
    "                X = VI.columns.to_list()\n",
    "                Y = EV.columns.to_list()\n",
    "                \n",
    "                # loop to create VI*EV variables \n",
    "                for i in X:\n",
    "                    for ii in Y:\n",
    "                        stri = i+'_x_'+ii\n",
    "                        df_gpp[stri] = VI[i]*EV[ii]\n",
    "                \n",
    "                # correlation analysis\n",
    "                correlation = df_gpp.corr()**2\n",
    "                correlation_analysis    = correlation.reset_index().sort_values(by='GPP_observations', ascending=False)\n",
    "                correlation_analysis    = correlation_analysis[['index','GPP_observations']].set_index('index')\n",
    "\n",
    "                if isinstance(Top, int):\n",
    "                    best_model = correlation_analysis.reset_index().iloc[Top,0]\n",
    "                    best_r2    = correlation_analysis.reset_index().iloc[Top,1]\n",
    "                    print('The %s best model with Shubert et al. (2012) structure is GPP = m (%s) + b with R^2 = %.4f'%(Top, best_model,best_r2))\n",
    "                else:\n",
    "                    best_model = Top \n",
    "                    best_r2    = correlation_analysis.loc[Top, 'GPP_observations']\n",
    "                    print('The performance of the GPP = m (%s) + b model with Shubert et al. (2012) structure is R^2 = %.4f'%(best_model,best_r2))\n",
    "\n",
    "                # produce de linear model with the variable with higher correlation\n",
    "                lm_2    = smf.ols(formula=\"GPP_observations~%s\"%best_model, data = df_gpp).fit()\n",
    "\n",
    "                pvalues = lm_2.pvalues\n",
    "                params  = lm_2.params\n",
    "                summary = lm_2.summary()\n",
    "                print('The linear coefficients are b = %.4f with p = %.4e and m= %.4f with p = %.4e ' %(params[0],pvalues[0],params[1],pvalues[1]))\n",
    "                \n",
    "                # plot correlation between gpp and the variable VI*EV with higher correlation\n",
    "                #%matplotlib inline\n",
    "                #df.plot(kind = \"scatter\", x = \"%s\"%best_model, y =\"GPP_observations\", c='green')\n",
    "\n",
    "                return correlation_analysis, params, pvalues, summary, best_model, best_r2, df_gpp\n",
    "\n",
    "        # function that presents the VI and VI*EV variables with higher correlation with gpp observations\n",
    "        def full_correlation_analysis(df_EV,TopVI,TopCai):\n",
    "            \n",
    "            # selects de VI with higher correlation with gpp\n",
    "            correlation_analysis_VI,params_VI,pvalues_VI,summary_VI,best_model_VI, best_r2_VI                = correlation_single(df_gpp_tower,df_VI_export  ,TopVI)\n",
    "            \n",
    "            df_VI_best = pd.DataFrame(df_VI_export[best_model_VI])\n",
    "            correlation_analysis_cai,params_cai,pvalues_cai,summary_cai,best_model_cai, best_r2_cai, df_cai  = correlation_cai(df_gpp_tower,df_VI_best,df_EV ,TopCai)\n",
    "\n",
    "            best_model_EV = best_model_cai.split('_x_')[1]\n",
    "            correlation = multicollinearity(df_VI_export,df_EV, best_model_VI, best_model_EV)\n",
    "            \n",
    "            return best_model_cai, params_cai, pvalues_cai, best_r2_cai, summary_cai, df_cai, correlation_analysis_VI\n",
    "\n",
    "        print('\\n')\n",
    "        print('     Using:', bands)\n",
    "        print('     Using:', EV_dataframes_names)\n",
    "        print('\\n')\n",
    "        \n",
    "        # applying the functions\n",
    "        EV_dataframes \n",
    "        best_model_cai_list = []\n",
    "        best_model_VI_cai_list = []\n",
    "        best_model_EV_cai_list = []\n",
    "        params_cai_m_list   = []\n",
    "        params_cai_b_list   = []\n",
    "        pvalues_cai_m_list  = []\n",
    "        pvalues_cai_b_list  = []\n",
    "        best_r2_cai_list    = []\n",
    "        summary_cai_list    = []\n",
    "\n",
    "\n",
    "        for m in range(1, len(df_VI_export.columns)+1):\n",
    "            for n in EV_dataframes:\n",
    "\n",
    "                best_model_cai,params_cai,pvalues_cai, best_r2_cai,summary_cai, df_cai, correlation_analysis_VI = full_correlation_analysis(n,m,1) \n",
    "                best_model_VI_cai_list.append(best_model_cai.split('_x_')[0])\n",
    "                best_model_EV_cai_list.append(best_model_cai.split('_x_')[1])\n",
    "                best_model_cai_list.append(best_model_cai)\n",
    "\n",
    "                params_cai_m_list.append(params_cai[1])\n",
    "                params_cai_b_list.append(params_cai[0])\n",
    "                pvalues_cai_m_list.append(pvalues_cai[1])\n",
    "                pvalues_cai_b_list.append(pvalues_cai[0])\n",
    "                best_r2_cai_list.append(best_r2_cai)\n",
    "                summary_cai_list.append(summary_cai)\n",
    "\n",
    "\n",
    "        models = {'best_model_cai_VI':best_model_VI_cai_list,\n",
    "                  'best_model_cai_EV':best_model_EV_cai_list,\n",
    "                  'best_model_cai':best_model_cai_list, \n",
    "                  'params_cai_m':params_cai_m_list ,\n",
    "                  'params_cai_b':params_cai_b_list  , \n",
    "                  'pvalues_cai_m':pvalues_cai_m_list, \n",
    "                  'pvalues_cai_b':pvalues_cai_b_list, \n",
    "                  'best_r2_cai': best_r2_cai_list  }\n",
    "        \n",
    "        df_correlation         = pd.DataFrame(models)\n",
    "        df_correlation_ranked  = df_correlation.reset_index().sort_values(by='best_r2_cai', ascending=False).reset_index(drop=True)\n",
    "          \n",
    "            \n",
    "        # introduce in the upper section\n",
    "        m_parameter        = df_correlation_ranked.loc[:0,'params_cai_m'][0]             #cte along the period\n",
    "        b_parameter        = df_correlation_ranked.loc[:0,'params_cai_b'][0]             #cte along the period\n",
    "\n",
    "        best_model_cai_EV  = df_correlation_ranked.loc[:0,'best_model_cai_EV'][0] \n",
    "        best_model_cai_VI  = df_correlation_ranked.loc[:0,'best_model_cai_VI'][0] \n",
    "        \n",
    "        t112a   = ptime.time()\n",
    "        strin = ( '{:.1f} [minutes]'.format((t112a - t111a) / 60.)                                                      \n",
    "                  if (t112a - t111a) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t112a - t111a))\n",
    "                )\n",
    "        print('     Computed correlation analysis in ', strin)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #*********************************************************************************************************************************************************************     \n",
    "    if correlation_analysis_simple:\n",
    "        \n",
    "        print('13b) Correlation analysis')\n",
    "        t111b = ptime.time()\n",
    "        \n",
    "        def correlation_cai_simple(gpp,VI,EV_list,Top):\n",
    "            \n",
    "                df_gpp = gpp.copy().dropna()\n",
    "                \n",
    "                # loop to create VI*EV variables \n",
    "                for EV in EV_list:\n",
    "\n",
    "                    X = VI.columns.to_list()\n",
    "                    Y = EV.columns.to_list()\n",
    "\n",
    "                    for i in X:\n",
    "                        for ii in Y:\n",
    "                            stri = i+'_x_'+ii\n",
    "                            df_gpp[stri] = VI[i]*EV[ii]\n",
    "                \n",
    "                data_mix    = df_gpp.copy()\n",
    "                correlation = df_gpp.corr()**2\n",
    "                correlation_analysis    = correlation.reset_index().sort_values(by='GPP_observations', ascending=False)\n",
    "                correlation_analysis    = correlation_analysis[['index','GPP_observations']].set_index('index')\n",
    "\n",
    "                if isinstance(Top, int):\n",
    "                    best_model = correlation_analysis.reset_index().iloc[Top,0]\n",
    "                    best_r2    = correlation_analysis.reset_index().iloc[Top,1]\n",
    "                    print('The %s best model with Shubert et al. (2012) structure is GPP = m (%s) + b with R^2 = %.4f'%(Top, best_model,best_r2))\n",
    "                else:\n",
    "                    best_model = Top \n",
    "                    best_r2    = correlation_analysis.loc[Top, 'GPP_observations']\n",
    "                    print('The performance of the GPP = m (%s) + b model with Shubert et al. (2012) structure is R^2 = %.4f'%(best_model,best_r2))\n",
    "                \n",
    "                return data_mix, best_model, correlation_analysis\n",
    "        \n",
    "        # applying function\n",
    "        df_cai_simple, best_model_cai_simple, correlation_analysis_cai_simple =correlation_cai_simple(df_gpp_tower,df_VI_export,EV_dataframes,1)      \n",
    "        correlation_analysis_cai_simple.to_excel(outputdir + '/Model_output/' + ID + \"_Interdependency_analysis.xlsx\")\n",
    "        \n",
    "        t112b   = ptime.time()\n",
    "        strin = ( '{:.1f} [minutes]'.format((t112b - t111b) / 60.)                                                      \n",
    "                  if (t112b - t111b) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t112b - t111b))\n",
    "                )\n",
    "        print('     Computed simple correlation analysis in ', strin)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7016186",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_analysis_cai_simple.iloc[:20,:]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2424f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    #*********************************************************************************************************************************************************************     \n",
    "    if calibration_validation:\n",
    "        \n",
    "        print('14)  Calibration and validation')\n",
    "        t121 = ptime.time()\n",
    "              \n",
    "        # function for the computation of the model with the calibration dataset\n",
    "        def linearmodel_analysis(dependant, independant, training_df, testing_df):\n",
    "            \n",
    "            # calculate linear model and predictions with testing dataset\n",
    "            #lm_cal       = smf.ols(formula=\"GPP_observations~%s\"%best_model_cai_simple, data=training).fit()\n",
    "            lm_cal        = smf.ols(formula=\"%s~%s\"%(dependant, independant), data=training_df).fit()\n",
    "            r2_cal        = lm_cal.rsquared\n",
    "            pvalues_cal   = lm_cal.pvalues\n",
    "            params_cal    = lm_cal.params\n",
    "            conf_int_cal  = lm_cal.conf_int(alpha=0.05, cols=None)\n",
    "            summary_cal   = lm_cal.summary()\n",
    "            model_pred    = lm_cal.predict(testing_df)\n",
    "            model_pred_df = pd.DataFrame(data=model_pred, columns=['GPP_predictions'])\n",
    "            \n",
    "            # print analysis\n",
    "            print('      The linear coefficients with independent data set are b = %.4f with p = %.4e and m= %.4f with p = %.4e' %(params_cal[0],pvalues_cal[0],params_cal[1],pvalues_cal[1]))\n",
    "            print('      R^2:    %.4f ' %r2_cal)\n",
    "            \n",
    "            return model_pred_df,conf_int_cal\n",
    "        \n",
    "        # function that derives errors  \n",
    "        def error_model(testing_df, label_testing, predictions_df, label_predictions):\n",
    "            \n",
    "            MAE   = sum(abs(testing_df[label_testing]-predictions_df[label_predictions]))/len(testing)\n",
    "            SSD   = sum((testing_df[label_testing]-predictions_df[label_predictions])**2)\n",
    "            RSE   = np.sqrt(SSD/(len(testing_df)-2))\n",
    "            mean  = np.mean(testing_df[label_testing])\n",
    "            error = RSE/mean\n",
    "            RMSE =np.sqrt(SSD/(len(testing_df)))\n",
    "            \n",
    "            # pint analysis\n",
    "            print('      MAE:    %.4f ' %MAE)\n",
    "            print('      Error:  %.4f ' %error)\n",
    "            print('      RMSE:   %.4f ' %RMSE)\n",
    "            print('\\n')      \n",
    "\n",
    "        # function to plot r2 \n",
    "        def plot_r2(df,var):\n",
    "            fig, axes_0=plt.subplots(figsize=(8,8))\n",
    "\n",
    "            axes_0.plot(df[var], df['GPP_observations'], 'ro',color='orchid', label='GPP_observations vs '+ var)\n",
    "            axes_0.axis([0, 9, 0, 9])\n",
    "\n",
    "            axes_0.set_title('GPP_observations vs '+ var, fontsize=20)\n",
    "            axes_0.set_xlabel(var+'(gC m-2 d-1)', fontsize=20)\n",
    "            axes_0.set_ylabel('GPP_observations(gC m-2 d-1)', fontsize=20)\n",
    "\n",
    "            axes_0.grid(lw=0.5)\n",
    "            axes_0.legend(fontsize=14, loc='lower right')\n",
    "        \n",
    "        # function to plot r2 with predicted-observed relationship\n",
    "        def plot_r2_withline(df,var, color, alpha):\n",
    "            x   = df[var].to_numpy()\n",
    "            y   = df['GPP_observations'].to_numpy()\n",
    "\n",
    "            x_s = df[var].to_numpy()\n",
    "            y_s = df['GPP_observations'].to_numpy()\n",
    "\n",
    "            # Model: y ~ x + c\n",
    "\n",
    "            x = sm.add_constant(x) # constant intercept term\n",
    "            model  = sm.OLS(y, x)\n",
    "            fitted = model.fit()\n",
    "\n",
    "            x_pred  = np.linspace(x.min(), x.max(), 50)\n",
    "            x_pred2 = sm.add_constant(x_pred)\n",
    "            y_pred  = fitted.predict(x_pred2)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(8, 8))\n",
    "            ax.scatter(x_s, y_s, alpha=alpha, color=color, label='GPP from validation dataset')\n",
    "            fig.tight_layout(pad=2); \n",
    "            ax.grid(lw = 0.5)\n",
    "            ax.axis([0, 8, 0, 8])\n",
    "\n",
    "            ax.plot(x_pred, y_pred, '-', color='darkorchid', linewidth=8, alpha=0.3, label='predicted-observed relationship')\n",
    "            ax.set_title('GPP_observations vs '+ var, fontsize=20)\n",
    "            ax.set_xlabel(var+' (gC m-2 d-1)', fontsize=16)\n",
    "            ax.set_ylabel('GPP_observations (gC m-2 d-1)', fontsize=18)\n",
    "            ax.legend(fontsize=14, loc='lower right')\n",
    "        \n",
    "        # function to plot linear model\n",
    "        def plot_lm(df,var):\n",
    "            fig, axes_0=plt.subplots(figsize=(16,8))\n",
    "            axes_0.plot(df[var], df['GPP_observations'], 'ro',color='#888888', label='GPP_observations')\n",
    "            axes_0.plot(df[var], df['GPP_predictions'], lw=3, color='darkorchid', label='GPP_predictions')\n",
    "            axes_0.plot(df[var], df['confident_interval_minus'], lw=3, color='orchid', label='Confidence_intervals')\n",
    "            axes_0.plot(df[var], df['confident_interval_plus'], lw=3, color='orchid', label='Confidence_intervals')\n",
    "            axes_0.set_ylim(0, 9)\n",
    "\n",
    "            axes_0.set_title('GPP_observations vs GPP_predictions', fontsize=16)\n",
    "            axes_0.set_xlabel(var, fontsize=20)\n",
    "            axes_0.set_ylabel('GPP (gC m-2 d-1)', fontsize=20)\n",
    "            \n",
    "            axes_0.grid(lw=0.5)\n",
    "            axes_0.legend(fontsize=14, loc='lower right')\n",
    "            \n",
    "        # function to plot with intervals     \n",
    "        def plot_lm_withintervals(training, var):\n",
    "            x = training[\"%s\"%var].to_numpy()\n",
    "            y = training['GPP_observations'].to_numpy()\n",
    "\n",
    "            x_s = training[\"%s\"%var].to_numpy()\n",
    "            y_s = training['GPP_observations'].to_numpy()\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            ax.scatter(x_s, y_s, alpha=0.5, color='orchid', label='GPP_observations')\n",
    "            ax.grid(True)\n",
    "            fig.tight_layout(pad=2);\n",
    "            #fig.savefig('filename1.png', dpi=125)\n",
    "\n",
    "            # Model: y ~ x + c\n",
    "\n",
    "            x = sm.add_constant(x) # constant intercept term\n",
    "            model   = sm.OLS(y, x)\n",
    "            fitted  = model.fit()\n",
    "            x_pred  = np.linspace(x.min(), x.max(), 50)\n",
    "            x_pred2 = sm.add_constant(x_pred)\n",
    "            y_pred  = fitted.predict(x_pred2)\n",
    "\n",
    "            ax.plot(x_pred, y_pred, '-', color='darkorchid', linewidth=2, label='GPP_model')\n",
    "            #fig.savefig('filename2.png', dpi=125)\n",
    "\n",
    "            y_hat   = fitted.predict(x) # x is an array from line 12 above\n",
    "            y_err   = y - y_hat\n",
    "            mean_x  = x.T[1].mean()\n",
    "            n       = len(x)\n",
    "            dof     = n - fitted.df_model - 1\n",
    "            t = stats.t.ppf(1-0.025, df=dof)\n",
    "            s_err = np.sum(np.power(y_err, 2))\n",
    "            conf = t * np.sqrt((s_err/(n-2))*(1.0/n + (np.power((x_pred-mean_x),2) / \n",
    "                    ((np.sum(np.power(x_pred,2))) - n*(np.power(mean_x,2))))))\n",
    "            upper = y_pred + abs(conf)\n",
    "            lower = y_pred - abs(conf)\n",
    "\n",
    "            ax.fill_between(x_pred, lower, upper, color='#888888', alpha=0.4, label='Confidence interval')\n",
    "            #fig.savefig('filename3.png', dpi=125)\n",
    "\n",
    "            sdev, lower, upper = wls_prediction_std(fitted, exog=x_pred2, alpha=0.05)\n",
    "\n",
    "            ax.fill_between(x_pred, lower, upper, color='#888888', alpha=0.1, label='Prediction interval')\n",
    "            #fig.savefig('filename4.png', dpi=125)\n",
    "\n",
    "            ax.set_title('GPP_observations vs GPP_predictions', fontsize=20)\n",
    "            ax.set_xlabel(\"%s\"%var, fontsize=16)\n",
    "            ax.set_ylabel('GPP (gC m-2 d-1)', fontsize=18)\n",
    "            ax.legend(fontsize=14, loc='lower right')\n",
    "            \n",
    "        # function to plot time series \n",
    "        def plot_ts(df,var):\n",
    "            fig, axes_0=plt.subplots(figsize=(14,6))\n",
    "            axes_0.plot(df['GPP_observations'], marker=\"^\",markeredgewidth =2, linestyle=':',linewidth=2, ms = 6, color = 'orchid', alpha=0.5, label='GPP_observations')\n",
    "            axes_0.plot(df['GPP_predictions'], marker='.',linestyle='-',linewidth=4, ms=6, color='green',alpha=0.4, label='GPP_predictions')\n",
    "            axes_0.plot(df[var], marker='.',linestyle='-', ms=6,linewidth=4, color='yellow', alpha=0.4, label=var)\n",
    "            axes_0.set_ylim(-5,20)\n",
    "\n",
    "\n",
    "            axes_0.set_title('Gross Primary Productivity', fontsize=20)\n",
    "            axes_0.set_xlabel('Date', fontsize=20)\n",
    "            axes_0.set_ylabel('GPP (gC m-2 d-1)', fontsize=20)\n",
    "\n",
    "            axes_0.grid(lw=0.5)\n",
    "            axes_0.legend(fontsize=14, loc='upper left')  \n",
    "            \n",
    "            # save figure\n",
    "            #fig.savefig('plot_ts.png', dpi=125)\n",
    "            \n",
    "        # selecting best model dataframe\n",
    "        df_cai_simple = df_cai_simple.dropna(subset=[best_model_cai_simple])\n",
    "        experiment = df_cai_simple[['GPP_observations',\"%s\"%best_model_cai_simple]].rename(columns={\n",
    "            'GPP_observations' : 'GPP_observations',\n",
    "            \"%s\"%best_model_cai: \"%s\"%best_model_cai}).copy()\n",
    "        \n",
    "        # dividing dataframe in training and testing dataframe with a constant seed equal to 10\n",
    "        training, testing = train_test_split(experiment, test_size = 0.2, random_state=10)\n",
    "    \n",
    "        print('      Training dataset:%d ' %len(training))\n",
    "        print('      Testing dataset:%d ' %len(testing))\n",
    "        print('\\n') \n",
    "        \n",
    "        # ploting training and testing dataframes\n",
    "        #training.plot(kind = \"scatter\", x = \"%s\"%best_model, y =\"GPP_observations\", c='green')\n",
    "        #testing.plot(kind = \"scatter\", x = \"%s\"%best_model, y =\"GPP_observations\", c='green')\n",
    "  \n",
    "        # apply function to derive empirical model \n",
    "        print('      Analysis eshape model')\n",
    "        model_eshape, conf_int_cal_eshape  = linearmodel_analysis('GPP_observations', best_model_cai_simple, training, testing)\n",
    "        report_eshape = error_model(testing,'GPP_observations', model_eshape,'GPP_predictions')\n",
    "        \n",
    "        # prepare dataframe for testing-join predictions with testing dataset and MODIS data \n",
    "        df_testing  = pd.merge(left= testing , right = model_eshape,\n",
    "                            how=\"left\", left_on =\"date\", right_on =\"date\").reset_index().sort_values(by='date', ascending=True).set_index('date')  \n",
    "        df_testing  = pd.merge(left= df_testing, right = df_gpp_MODIS,\n",
    "                             how=\"left\", left_index = True , right_index = True)\n",
    "        \n",
    "        df_testing['confident_interval_minus'] = df_testing[\"%s\"%best_model_cai_simple]*conf_int_cal_eshape.iloc[1,0]+conf_int_cal_eshape.iloc[0,0]\n",
    "        df_testing['confident_interval_plus']  = df_testing [\"%s\"%best_model_cai_simple]*conf_int_cal_eshape.iloc[1,1]+conf_int_cal_eshape.iloc[0,1]\n",
    "        \n",
    "        # apply function to evaluate MODIS data \n",
    "        print('      Analysis MODIS')\n",
    "        model_MODIS, conf_int_cal_MODIS  = linearmodel_analysis('GPP_observations', 'GPP_MODIS', df_testing, df_testing)\n",
    "        report_MODIS = error_model(df_testing,'GPP_observations', df_testing,'GPP_MODIS')\n",
    "        \n",
    "        # apply function to derive plots of r2, linear model and time series between predictions and testing dataset\n",
    "        plot_r2(df_testing,'GPP_predictions')\n",
    "        plot_r2(df_testing,'GPP_MODIS')\n",
    "        plot_r2_withline(df_testing,'GPP_predictions', 'green',0.3)\n",
    "        plot_r2_withline(df_testing,'GPP_MODIS', 'yellow',1)\n",
    "        plot_lm(df_testing,best_model_cai_simple)\n",
    "        plot_lm_withintervals(training, best_model_cai_simple)\n",
    "        plot_ts(df_testing,'GPP_MODIS')\n",
    "        \n",
    "        \n",
    "        t122    = ptime.time()\n",
    "        strin   = ( '{:.1f} [minutes]'.format((t122 - t121) / 60.)                                                      \n",
    "                  if (t122 - t121) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t122 - t121))\n",
    "                )\n",
    "        print('     Computed calibration and validation in ', strin)  \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d007ce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    #*********************************************************************************************************************************************************************     \n",
    "    if timeseries_thirty:\n",
    "        \n",
    "        print('15)  Monthly timeseries')\n",
    "        t131 = ptime.time()\n",
    "        \n",
    "        # function to create monthly time series (monthly mean or aggregated)\n",
    "        def month_ts(df,years, mean=True):\n",
    "\n",
    "            # divide dataframe by year  \n",
    "            def group_by_month(df, year):\n",
    "\n",
    "                df = df.copy().reset_index()\n",
    "                df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "                df['Timestamp'] = pd.to_datetime(df['date']) \n",
    "                df['Year']      = pd.DatetimeIndex(df['Timestamp']).year\n",
    "                df['Month']     = pd.DatetimeIndex(df['Timestamp']).month\n",
    "\n",
    "                #def add_date_info(df):\n",
    "\n",
    "                #    df['Timestamp'] = pd.to_datetime(df['date']) \n",
    "                #    df['Year']      = pd.DatetimeIndex(df['Timestamp']).year\n",
    "                #    df['Month']     = pd.DatetimeIndex(df['Timestamp']).month\n",
    "\n",
    "                #    return df\n",
    "\n",
    "                #df = add_date_info(df)\n",
    "\n",
    "                df = df[df['Year']==year].copy(deep=True) \n",
    "\n",
    "\n",
    "                df_aux_0         = df.loc[:,['Month']]                                          # helps to count the number of days in a month\n",
    "                df_aux_0['days'] = 1\n",
    "                df_aux_0         = df_aux_0.groupby(df['Month']).sum().drop('Month', axis = 1)\n",
    "\n",
    "                df_aux_1       =  df.loc[:,['Month','Year']]\n",
    "                df_aux_1       =  df_aux_1.groupby(df['Month']).mean().drop('Month', axis = 1)\n",
    "\n",
    "                if mean:\n",
    "                    df= df.groupby(df['Month']).mean()\n",
    "                else:\n",
    "                    df= df.groupby(df['Month']).sum()\n",
    "\n",
    "                dff = df.drop('Year', axis = 1)\n",
    "\n",
    "                dff = pd.merge(left= dff, right = df_aux_0,\n",
    "                             how=\"left\", left_index = True , right_index = True)\n",
    "\n",
    "                dff = pd.merge(left= dff, right = df_aux_1,\n",
    "                             how=\"left\", left_index = True , right_index = True)\n",
    "                return dff\n",
    "\n",
    "            for i in years:\n",
    "\n",
    "                print('      Monthly timeseries for the year %s and %s ' %(i,df))\n",
    "                globals()['df_month_%s' % i] = group_by_month(df, i)\n",
    "\n",
    "            df_total = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "            for i in years:\n",
    "\n",
    "                df = globals()['df_month_%s' % i]\n",
    "                df_total = pd.concat([df_total, df]).dropna(axis=1,how ='all')\n",
    "\n",
    "            df = df_total.reset_index().rename(columns={'index' : 'Month'})\n",
    "\n",
    "            df['Year']  = df['Year'].astype('int').astype('str')\n",
    "            df['Month'] = df['Month'].astype('int').astype('str')\n",
    "            df['date']  = df['Year']+ '-'+ df['Month']\n",
    "            df['date']  = pd.to_datetime(df['date'], format='%Y-%m')\n",
    "            \n",
    "            df          =  df.set_index('date').drop(['Year','Month'], axis =1) \n",
    "\n",
    "            return df\n",
    "        \n",
    "        def date_tostring(date): \n",
    "            string_date =  date.strftime(\"%Y-%m\")\n",
    "            return string_date\n",
    "        \n",
    "        EV_dataframes_month = []\n",
    "        \n",
    "        # deriving time series\n",
    "        if df_rainfall_station_switch:\n",
    "            df_rainfall_station_m  = month_ts(df_rainfall_station,years,mean = False).drop('days', axis = 1)\n",
    "            EV_dataframes_month.append(df_rainfall_station_m)\n",
    "                \n",
    "        if df_meteo_station_switch:\n",
    "            df_meteo_station_m     = month_ts(df_meteo_station,years,mean = False).drop('days', axis = 1)\n",
    "            EV_dataframes_month.append(df_meteo_station_m)\n",
    "            \n",
    "        if df_rainfall_CHIRPS_switch:\n",
    "            df_rainfall_CHIRPS_m   = month_ts(df_rainfall_CHIRPS,years,mean = False).drop('days', axis = 1)\n",
    "            EV_dataframes_month.append(df_rainfall_CHIRPS_m)\n",
    "             \n",
    "        if df_temp_MODIS_switch:\n",
    "            df_temp_MODIS_m        = month_ts(df_temp_MODIS ,years,mean = False).drop('days', axis = 1)\n",
    "            EV_dataframes_month.append(df_temp_MODIS_m)\n",
    "            \n",
    "        if df_meteo_tower_switch:\n",
    "            df_meteo_tower_m       = month_ts(df_meteo_tower,years,mean = False).drop('days', axis = 1)\n",
    "            EV_dataframes_month.append(df_meteo_tower_m)\n",
    "            \n",
    "        df_gpp_MODIS_m         = month_ts(df_gpp_MODIS  ,years,mean = False).drop('days', axis = 1)\n",
    "        df_gpp_tower_m_sum     = month_ts(df_gpp_tower,years,mean = False)\n",
    "        df_gpp_tower_m_mean    = month_ts(df_gpp_tower,years,mean = True)\n",
    "\n",
    "        for n in EV_dataframes_month:\n",
    "            df_gpp_tower_m_sum = pd.merge(left= df_gpp_tower_m_sum, right = n,\n",
    "                                         how=\"left\", left_index = True , right_index = True)\n",
    "            \n",
    "        # deriving information of days in a month and adding variables by month \n",
    "        df_month_days          = df_gpp_tower_m_sum.loc[:,'days'].tolist()                    #Variable along the period \n",
    "        df_month_var           = df_gpp_tower_m_sum.loc[:,best_model_cai_EV].tolist()         #Variable along the period \n",
    "        df_month_dates         = df_gpp_tower_m_sum.reset_index()\n",
    "        df_month_dates['date'] = df_month_dates['date'].dt.date\n",
    "        df_month_dates['date_end'] = df_month_dates['date'] + relativedelta(months=+1)\n",
    "            \n",
    "        df_month_dates_beggining   = df_month_dates['date'].apply(date_tostring).tolist()\n",
    "        df_month_dates_end         = df_month_dates['date_end'].apply(date_tostring).tolist()\n",
    "\n",
    "        n_maps_30 = []\n",
    "        for n in range(len(df_month_dates)):\n",
    "            n_maps_30.append(n)\n",
    "            \n",
    "        t132    = ptime.time()\n",
    "        strin   = ( '{:.1f} [minutes]'.format((t132 - t131) / 60.)                                                      \n",
    "                  if (t132 - t131) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t132 - t131))\n",
    "                )\n",
    "        print('     Computed monthly time series in ', strin) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de6f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #*********************************************************************************************************************************************************************     \n",
    "    if timeseries_fifteen:\n",
    "        \n",
    "        print('16)  15-day timeseries')\n",
    "        t141 = ptime.time()\n",
    "\n",
    "        def fifteen_ts(df,best_model_cai_EV):\n",
    "            \n",
    "            def date_tostring(date): \n",
    "                string_date  =  date.strftime(\"%Y-%m-%d\")\n",
    "                return string_date\n",
    "\n",
    "            day_aux           = pd.date_range(start=start, end=end, freq ='d').strftime(\"%Y-%m-%d\") \n",
    "            day_aux           = pd.DataFrame(day_aux).rename(columns={0: 'Date'}) \n",
    "\n",
    "            day_aux['Date_delay'] = pd.to_datetime(day_aux['Date']) - dt.timedelta(days = 1)\n",
    "            start_minus_1day  = day_aux['Date_delay'].apply(date_tostring).tolist()\n",
    "            \n",
    "            df['days']       = 1.\n",
    "\n",
    "            day_15           = pd.date_range(start=start_minus_1day[0], end=end, freq ='SM').strftime(\"%Y-%m-%d\") \n",
    "            day_15           = pd.DataFrame(day_15).rename(columns={0: 'Date'}) \n",
    "\n",
    "            day_15['Date_beggining'] = pd.to_datetime(day_15['Date']) + dt.timedelta(days = 1)\n",
    "            day_15['Date_end']       = pd.to_datetime(day_15['Date']).shift(-1)\n",
    "            day_15_list              = day_15\n",
    "            day_15                   = day_15.drop(day_15.index[len(day_15)-1])\n",
    "\n",
    "            day_15_beggining = day_15['Date_beggining'].apply(date_tostring).tolist()\n",
    "            day_15_end       = day_15['Date_end'].apply(date_tostring).tolist()\n",
    "            day_15_list      = day_15_list['Date_beggining'].apply(date_tostring).tolist()\n",
    "\n",
    "            df_days = []\n",
    "            df_var  = []\n",
    "\n",
    "            for n  in range(len(day_15_end)):\n",
    "                days = df.loc[day_15_beggining[n]:day_15_end[n],'days'].sum()\n",
    "                var  = df.loc[day_15_beggining[n]:day_15_end[n],best_model_cai_EV].sum()\n",
    "                df_days.append(days)\n",
    "                df_var.append(var)\n",
    "                \n",
    "            n_maps_15 = []\n",
    "            for n in range(len(day_15_end)):\n",
    "                n_maps_15.append(n)\n",
    "\n",
    "            return df_days, df_var, n_maps_15, day_15_beggining, day_15_end \n",
    "        \n",
    "        df_gpp_tower_15 = df_gpp_tower\n",
    "    \n",
    "        for n in EV_dataframes:\n",
    "            df_gpp_tower_15 = pd.merge(left= df_gpp_tower_15, right = n,\n",
    "                                         how=\"left\", left_index = True , right_index = True)\n",
    "\n",
    "        df_15_days, df_15_var, n_maps_15, df_15_beggining, df_15_end = fifteen_ts(df_gpp_tower_15,best_model_cai_EV)\n",
    "        df_15_days_gpp, df_15_var_gpp, n_maps_15_gpp, df_15_beggining_gpp, df_15_end_gpp = fifteen_ts(df_gpp_tower_15,'GPP_observations')\n",
    "        \n",
    "        t142    = ptime.time()\n",
    "        strin   = ( '{:.1f} [minutes]'.format((t142 - t141) / 60.)                                                      \n",
    "                  if (t142 - t141) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t142 - t141))\n",
    "                )\n",
    "        print('     Computed 15-day time series in ', strin)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c32853",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #*********************************************************************************************************************************************************************     \n",
    "    if mapping_GPP:\n",
    "        \n",
    "        print('17)  Mapping function')\n",
    "        t151 = ptime.time()\n",
    "        \n",
    "        # Add custom basemaps to folium\n",
    "        basemaps = {\n",
    "            'Google Maps': folium.TileLayer(\n",
    "                tiles = 'https://mt1.google.com/vt/lyrs=m&x={x}&y={y}&z={z}',\n",
    "                attr = 'Google',\n",
    "                name = 'Google Maps',\n",
    "                overlay = True,\n",
    "                control = True\n",
    "            ),\n",
    "            'Google Satellite': folium.TileLayer(\n",
    "                tiles = 'https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
    "                attr = 'Google',\n",
    "                name = 'Google Satellite',\n",
    "                overlay = True,\n",
    "                control = True\n",
    "            ),\n",
    "            'Google Terrain': folium.TileLayer(\n",
    "                tiles = 'https://mt1.google.com/vt/lyrs=p&x={x}&y={y}&z={z}',\n",
    "                attr = 'Google',\n",
    "                name = 'Google Terrain',\n",
    "                overlay = True,\n",
    "                control = True\n",
    "            ),\n",
    "            'Google Satellite Hybrid': folium.TileLayer(\n",
    "                tiles = 'https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}',\n",
    "                attr = 'Google',\n",
    "                name = 'Google Satellite',\n",
    "                overlay = True,\n",
    "                control = True\n",
    "            ),\n",
    "            'Esri Satellite': folium.TileLayer(\n",
    "                tiles = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "                attr = 'Esri',\n",
    "                name = 'Esri Satellite',\n",
    "                overlay = True,\n",
    "                control = True\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # define a method for displaying Earth Engine image tiles on a folium map.\n",
    "        def add_ee_layer(self, ee_object, vis_params, name):\n",
    "\n",
    "            try:    \n",
    "\n",
    "                # display ee.Image()\n",
    "                if isinstance(ee_object, ee.image.Image):    \n",
    "                    map_id_dict = ee.Image(ee_object).getMapId(vis_params)\n",
    "                    folium.raster_layers.TileLayer(\n",
    "                    tiles = map_id_dict['tile_fetcher'].url_format,\n",
    "                    attr = 'Google Earth Engine',\n",
    "                    name = name,\n",
    "                    overlay = True,\n",
    "                    control = True\n",
    "                    ).add_to(self)\n",
    "\n",
    "                # display ee.ImageCollection()\n",
    "                elif isinstance(ee_object, ee.imagecollection.ImageCollection):    \n",
    "                    ee_object_new = ee_object.mosaic()\n",
    "                    map_id_dict = ee.Image(ee_object_new).getMapId(vis_params)\n",
    "                    folium.raster_layers.TileLayer(\n",
    "                    tiles = map_id_dict['tile_fetcher'].url_format,\n",
    "                    attr = 'Google Earth Engine',\n",
    "                    name = name,\n",
    "                    overlay = True,\n",
    "                    control = True\n",
    "                    ).add_to(self)\n",
    "\n",
    "                # display ee.Geometry()\n",
    "                elif isinstance(ee_object, ee.geometry.Geometry):    \n",
    "                    folium.GeoJson(\n",
    "                    data = ee_object.getInfo(),\n",
    "                    name = name,\n",
    "                    overlay = True,\n",
    "                    control = True,\n",
    "                    style_function=lambda x:vis_params\n",
    "                ).add_to(self)\n",
    "\n",
    "                # display ee.FeatureCollection()\n",
    "                elif isinstance(ee_object, ee.featurecollection.FeatureCollection):  \n",
    "                    ee_object_new = ee.Image().paint(ee_object, 0, 2)\n",
    "                    map_id_dict = ee.Image(ee_object_new).getMapId(vis_params)\n",
    "                    folium.raster_layers.TileLayer(\n",
    "                    tiles = map_id_dict['tile_fetcher'].url_format,\n",
    "                    attr = 'Google Earth Engine',\n",
    "                    name = name,\n",
    "                    overlay = True,\n",
    "                    control = True\n",
    "                ).add_to(self)\n",
    "\n",
    "            except:\n",
    "                print(\"Could not display {}\".format(name))\n",
    "\n",
    "        # add EE drawing method to folium.\n",
    "        folium.Map.add_ee_layer = add_ee_layer\n",
    "        \n",
    "        t152    = ptime.time()\n",
    "        strin   = ( '{:.1f} [minutes]'.format((t152 - t151) / 60.)                                                      \n",
    "                  if (t152 - t151) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t152 - t151))\n",
    "                )\n",
    "        print('     Computed mapping function in ', strin)  \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230090bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #*********************************************************************************************************************************************************************     \n",
    "    if classification_maps:\n",
    "    #https://code.earthengine.google.com/2b95fd6462c6c906d4ed9a74fae51bf4\n",
    "    \n",
    "        print('18)  Vegetation calssification maps')\n",
    "        t161 = ptime.time()\n",
    "        \n",
    "        bands_ml  = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12',best_model_cai_VI]\n",
    "        S2_VI_ml  = S2_VI.select(bands_ml)\n",
    "\n",
    "        Region    = point.buffer(ecosystem_extension)\n",
    "                \n",
    "        inputML   =  S2_VI_ml.median().clip(Region)\n",
    "\n",
    "        training  = inputML.sample(region= Region,scale= 100,numPixels= 1000)\n",
    "        clusterer = ee.Clusterer.wekaKMeans(number_clusters).train(training)\n",
    "\n",
    "        result          = inputML.cluster(clusterer)\n",
    "        results_colect  = ee.ImageCollection([result,result])\n",
    "        df_clus = results_colect.getRegion(point, 100).getInfo()\n",
    "        df_clus = pd.DataFrame(df_clus)\n",
    "        headers = df_clus.iloc[0]\n",
    "        df_clus = pd.DataFrame(df_clus.values[1:], columns=headers).set_index('id')\n",
    "        cluster_ecosystem = df_clus['cluster'][0]\n",
    "        \n",
    "        results_shp = result.reduceToVectors(scale=100, bestEffort=True)\n",
    "\n",
    "        def classification(weka, num):\n",
    "            class_vegetation = weka.select('label').filter(ee.Filter.eq('label', num))\n",
    "            return class_vegetation;\n",
    "        \n",
    "        cluster_name = []\n",
    "        for i in range(number_clusters):\n",
    "            globals()['cluster_%s'%i] = classification(results_shp, i).union(1).geometry()\n",
    "            cluster_name.append(globals()['cluster_%s'%i])\n",
    "            \n",
    "        cluster_ecosystem_geometry = cluster_name[cluster_ecosystem]\n",
    "        \n",
    "        # importing results from google earth engine\n",
    "        #results_shp_Germany        = ee.FeatureCollection(\"users/mafmonjaraz/results_shp_Germany\").geometry()\n",
    "        #c3_shp        = ee.FeatureCollection(\"users/mafmonjaraz/c3_shp\").geometry()\n",
    "\n",
    "        t162    = ptime.time()\n",
    "        strin   = ( '{:.1f} [minutes]'.format((t162 - t161) / 60.)                                                      \n",
    "                  if (t162 - t161) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t162 - t161))\n",
    "                )\n",
    "        print('     Computed vegetation classification maps in ', strin)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60cab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example mapping with folium\n",
    "# a) create a folium map object.\n",
    "my_map = folium.Map(location=[latitude,longitude], zoom_start=15, height=500)\n",
    "\n",
    "# b) add custom basemaps\n",
    "basemaps['Esri Satellite'].add_to(my_map)\n",
    "basemaps['Google Satellite Hybrid'].add_to(my_map)\n",
    "\n",
    "# c) set visualization parameters.\n",
    "vis_params = {\n",
    "  'min': 0,\n",
    "  'max': 4000,\n",
    "  'palette': ['006633', 'E5FFCC', '662A00', 'D8D8D8', 'F5F5F5']}\n",
    "\n",
    "# d) add the elevation model to the map object.\n",
    "#dem  = ee.Image('USGS/SRTMGL1_003')\n",
    "#xy   = ee.Geometry.Point([86.9250, 27.9881])\n",
    "#elev = dem.sample(xy, 30).first().get('elevation').getInfo()                                       #print the elevation of Mount Everest.\n",
    "#my_map.add_ee_layer(dem.updateMask(dem.gt(0)), vis_params, 'DEM')\n",
    "#print('Mount Everest elevation (m):', elev)\n",
    "\n",
    "# d) display ee.Image\n",
    "dataset        = ee.Image('JRC/GSW1_1/GlobalSurfaceWater')\n",
    "occurrence     = dataset.select('occurrence');\n",
    "occurrenceVis  = {'min': 0.0, 'max': 100.0, 'palette': ['ffffff', 'ffbbbb', '0000ff']}\n",
    "my_map.add_ee_layer(occurrence, occurrenceVis, 'JRC Surface Water')\n",
    "\n",
    "# d) display ee.Geometry\n",
    "holePoly = ee.Geometry.Polygon(coords = [[[-35, -10], [-35, 10], [35, 10], [35, -10], [-35, -10]]],\n",
    "                               proj= 'EPSG:4326',\n",
    "                               geodesic = True,\n",
    "                               maxError= 1.,\n",
    "                               evenOdd = False)\n",
    "my_map.add_ee_layer(holePoly, {}, 'Polygon')\n",
    "\n",
    "# d) display ee.FeatureCollection\n",
    "fc  = ee.FeatureCollection('TIGER/2018/States')\n",
    "my_map.add_ee_layer(fc, {}, 'US States')\n",
    "\n",
    "# d) display Geometry\n",
    "vis_params_geometry = dict(color='red', weight=2, opacity=10, fillColor='red')\n",
    "\n",
    "for i in years:\n",
    "    for n in range(len(rst_var)):\n",
    "            area = globals()['area_%s_%s' %(int(rst_var[n]),i)]\n",
    "            my_map.add_ee_layer(area, vis_params_geometry , 'area_%s_%s' %(int(rst_var[n]),i))\n",
    "    area = globals()['area_100_%s' %i]\n",
    "    my_map.add_ee_layer(area, vis_params_geometry , 'area_100_%s' %i)\n",
    "    \n",
    "    \n",
    "my_map.add_ee_layer(cluster_ecosystem_geometry,  vis_params_geometry , 'cluster_0')\n",
    "    \n",
    "#my_map.add_ee_layer(results_shp,  vis_params_geometry , 'results_shp')\n",
    "#my_map.add_ee_layer(c0,  vis_params_geometry , 'c0')\n",
    "#my_map.add_ee_layer(c1,  vis_params_geometry , 'c1')\n",
    "#my_map.add_ee_layer(c2,  vis_params_geometry , 'c2')\n",
    "#my_map.add_ee_layer(c3,  vis_params_geometry , 'c3')\n",
    "    \n",
    "#my_map.add_ee_layer(area_20_2020,  vis_params_geometry , 'area_20_2020')\n",
    "#my_map.add_ee_layer(area_40_2020,  vis_params_geometry , 'area_40_2020')\n",
    "#my_map.add_ee_layer(area_60_2020,  vis_params_geometry , 'area_60_2020')\n",
    "#my_map.add_ee_layer(area_80_2020,  vis_params_geometry , 'area_80_2020')\n",
    "#my_map.add_ee_layer(area_100_2020, vis_params_geometry , 'area_100_2020')\n",
    "\n",
    "#vis_params_geometry = dict(color='blue', weight=2, opacity=10, fillColor='red')\n",
    "#my_map.add_ee_layer(area_20_2021,  vis_params_geometry , 'area_20_2021')\n",
    "#my_map.add_ee_layer(area_40_2021,  vis_params_geometry , 'area_40_2021')\n",
    "#my_map.add_ee_layer(area_60_2021,  vis_params_geometry , 'area_60_2021')\n",
    "#my_map.add_ee_layer(area_80_2021,  vis_params_geometry , 'area_80_2021')\n",
    "#my_map.add_ee_layer(area_100_2021, vis_params_geometry , 'area_100_2021')\n",
    "    \n",
    "#visNDVI = {\"min\":0, \"max\":0.5 ,\"palette\":[\"ff4545\",\"fbffbe\",\"a7ff7a\",\"009356\",\"1f1e6e\"]}          #requires to define GPP rasters \n",
    "#my_map.add_ee_layer(GPP_2020_10, visNDVI , 'GPP_2020_10')                              \n",
    "#my_map.add_ee_layer(GPP_2020_11, visNDVI , 'GPP_2020_11')\n",
    "#my_map.add_ee_layer(GPP_2020_12, visNDVI , 'GPP_2020_12')\n",
    "    \n",
    "#for namer, img in zip(names, images_GPP):\n",
    "#    my_map.add_ee_layer(img,  visNDVI, namer) \n",
    "\n",
    "# e) add a layer control panel to the map.\n",
    "my_map.add_child(folium.LayerControl())\n",
    "plugins.Fullscreen().add_to(my_map)\n",
    "\n",
    "# f) display the map.\n",
    "display(my_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9542be19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    #*********************************************************************************************************************************************************************     \n",
    "    #generating maps\n",
    "    if mapping_GPP:\n",
    "        \n",
    "        print('19)  Mapping GPP')\n",
    "        t171 = ptime.time()\n",
    "        \n",
    "        def maps_GEE(timestep, geometry, n_maps_time, df_time_var, df_time_days,df_time_dates_beggining, df_time_dates_end):\n",
    "            \n",
    "            names_GPP_f    = []\n",
    "            images_GPP_f   = []\n",
    "            mean_GPP_f     = []\n",
    "\n",
    "            geometry_maps   = geometry\n",
    "            reducer         = ee.Reducer.mean()\n",
    "            \n",
    "            visGPP       = {\"min\":-50, \"max\":300 ,\"palette\":[\"ff4545\",\"fbffbe\",\"a7ff7a\",\"009356\",\"1f1e6e\"]}\n",
    "            visGPP_total = {\"min\":-50, \"max\":1000 ,\"palette\":[\"ff4545\",\"fbffbe\",\"a7ff7a\",\"009356\",\"1f1e6e\"]}\n",
    "\n",
    "            visGPP_scale           = {\"min\":0, \"max\":1,\"palette\":[\"ff4545\",\"fbffbe\",\"a7ff7a\",\"009356\",\"1f1e6e\"]}\n",
    "            visGPP_intercept       = {\"min\":0, \"max\":1,\"palette\":[\"ff4545\",\"fbffbe\",\"a7ff7a\",\"009356\",\"1f1e6e\"]}  \n",
    "            \n",
    "            # create a  map.\n",
    "            map_variables_f = folium.Map(location= [lon_lat[1], lon_lat[0]], zoom_start=12)\n",
    "            \n",
    "            for n ,v_m, d_m in zip(n_maps_time,df_time_var, df_time_days):\n",
    "\n",
    "                date  = df_time_dates_beggining[n]\n",
    "                start = df_time_dates_beggining[n]\n",
    "                end   = df_time_dates_end[n]\n",
    "                time  = [start,end]\n",
    "                #print('', time)\n",
    "                \n",
    "                if timestep==30:\n",
    "                    name  = date.split('-')[0]+'_'+date.split('-')[1]\n",
    "                else:\n",
    "                    name  = date.split('-')[0]+'_'+date.split('-')[1]+'_'+date.split('-')[2]\n",
    "            \n",
    "                def maskS2nonvegetation(image):\n",
    "\n",
    "                    qa    = image.select('QA60')\n",
    "                    scl   = image.select('SCL')\n",
    "                    ndvi  = image.select('NDVI')\n",
    "                    mndvi = image.select('MNDVI')\n",
    "\n",
    "                    cloudBitMask = 1 << 10\n",
    "                    cirrusBitMask = 1 << 11\n",
    "\n",
    "                    #vegetationMask1 = 4 # vegetation\n",
    "                    #vegetationMask2 = 5 # non-vegetated\n",
    "                    #vegetationMask3 = 6 # water\n",
    "                    #vegetationMask4 = 7 # unclassified\n",
    "\n",
    "                    #ndviMask  = -100;   \n",
    "                    #mndviMask = -100; \n",
    "\n",
    "                    mask = scl.eq(4).Or(scl.eq(5)).Or(scl.eq(6)).Or(scl.eq(7)).And(qa.bitwiseAnd(cloudBitMask).eq(0)).And(qa.bitwiseAnd(cirrusBitMask).eq(0)).And(ndvi.gte(ndviMask)).And(mndvi.gte(mndviMask))\n",
    "\n",
    "                    vegetation = image.updateMask(mask) \n",
    "\n",
    "                    return vegetation\n",
    "\n",
    "                def maskqa60(image):\n",
    "                    qa        = image.select('QA60')\n",
    "                    mask      = qa.eq(0)\n",
    "                    freecloud = image.updateMask(mask) \n",
    "                    return freecloud\n",
    "\n",
    "                def create_var_band(image):\n",
    "                    cte    = image.constant(v_m).rename('sum_var')\n",
    "                    image1 = image.addBands(cte) \n",
    "                    return image1\n",
    "\n",
    "                def create_day_band(image):\n",
    "                    cte    = image.constant(d_m).rename('days')\n",
    "                    image1 = image.addBands(cte) \n",
    "                    return image1\n",
    "                \n",
    "                # function to add gpp information to the maps by multiplying bands \n",
    "                def gpp_computation(image):\n",
    "\n",
    "                    VI = best_model_cai_VI                          \n",
    "\n",
    "                    bands_for_expressions = {\n",
    "                        'days'   : image.select('days'),\n",
    "                        'sum_var': image.select('sum_var'),\n",
    "                        'VI'     : image.select(VI),\n",
    "                        'QA60'   : image.select('QA60')}\n",
    "\n",
    "                    gpp  = image.expression('%s*VI*sum_var+%s*days'%(m_parameter,b_parameter), \n",
    "                            bands_for_expressions).rename(\"GPP\")\n",
    "\n",
    "                    #qa60       = image.expression('QA60/QA60', \n",
    "                    #             bands_for_expressions).rename(\"QA60_ratio\")\n",
    "\n",
    "                    #date       = dt.datetime.strptime(start,\"%Y-%m\")\n",
    "                    #timestamp  = dt.datetime.timestamp(date)\n",
    "                    #time       = image.constant(timestamp).rename('Time')\n",
    "\n",
    "                    image1 = image.addBands(gpp) #.addBands(qa60).addBands(time)\n",
    "\n",
    "                    return image1\n",
    "                \n",
    "                globals()['S2_%s' % name] = load_catalog(COPERNICUS_S2_L2A, time, point, COPERNICUS_S2_bands)\n",
    "                max_cloud_coverage = 100                                                                       \n",
    "                globals()['S2_%s' % name] = cloud_filter(globals()['S2_%s' % name], cloud_coverage_metadata_name, max_cloud_coverage)\n",
    "                globals()['S2_%s' % name] = globals()['S2_%s' % name].map(calculateVI)\n",
    "                globals()['S2_%s' % name] = globals()['S2_%s' % name].map(maskS2nonvegetation)\n",
    "                globals()['S2_%s' % name] = globals()['S2_%s' % name].map(create_var_band)\n",
    "                globals()['S2_%s' % name] = globals()['S2_%s' % name].map(create_day_band)\n",
    "                globals()['S2_%s' % name] = globals()['S2_%s' % name].mean()\n",
    "                globals()['S2_%s' % name] = globals()['S2_%s' % name].clip(geometry_maps)\n",
    "                globals()['%s_%s' % (best_model_cai_VI,name)] = globals()['S2_%s' % name].select([best_model_cai_VI,'sum_var','days','QA60'])\n",
    "                print('%s_%s' % (best_model_cai_VI,name))\n",
    "                globals()['GPP_%s' % name] =  gpp_computation(globals()['%s_%s' % (best_model_cai_VI,name)]).select(['GPP'])  #.select(['GPP',\"QA60_ratio\", \"Time\"])     \n",
    "            \n",
    "                GPP_var   = 'GPP_%s' % name\n",
    "                GPP_mean  =  globals()['GPP_%s' % name].select(['GPP']).reduceRegion(reducer= reducer,geometry=geometry_maps,scale=10,maxPixels=1e9).getInfo()\n",
    "\n",
    "                images_GPP_f.append(globals()['GPP_%s' % name])\n",
    "                names_GPP_f.append(GPP_var)\n",
    "                mean_GPP_f.append(GPP_mean)\n",
    "                \n",
    "            # add layers to the map using a loop.\n",
    "            for namer, img in zip(names_GPP_f, images_GPP_f):\n",
    "                if namer == 'GPP_total':\n",
    "                    map_variables_f.add_ee_layer(img.select('GPP'), visGPP_total, namer) \n",
    "                    print('Adding Map: %s' % namer)\n",
    "                    print('namer:' , namer)\n",
    "                else:\n",
    "                    map_variables_f.add_ee_layer(img.select('GPP'), visGPP, namer) \n",
    "                    print('Adding Map: %s' % namer)\n",
    "                    print('namer:' , namer)\n",
    "\n",
    "            folium.LayerControl(collapsed = False).add_to(map_variables_f)\n",
    "\n",
    "            # display the map.\n",
    "            display(map_variables_f)  \n",
    "            \n",
    "                \n",
    "            return map_variables_f, images_GPP_f, names_GPP_f, mean_GPP_f\n",
    "        \n",
    "        def export_maps(urlTh_list_f, url_list_f, names_f, maps):\n",
    "                for name, img in zip(names_f, maps):\n",
    "                    # writes url\n",
    "                    urlTh = img.select('GPP').getThumbUrl({ \"min\":0, \"max\":400, \n",
    "                                                         'dimensions': 512, \n",
    "                                                         'region': Region,\n",
    "                                                         'palette':[\"ff4545\",\"fbffbe\",\"a7ff7a\",\"009356\",\"1f1e6e\"]}) \n",
    "                    \n",
    "                    url = img.getDownloadUrl({'bands': 'GPP',\n",
    "                                              'region': Region,\n",
    "                                              'scale': 10,\n",
    "                                              'format': 'GEO_TIFF'})\n",
    "                    \n",
    "                    #task = ee.batch.Export.image.toDrive(**{\n",
    "                    #    'image': img,\n",
    "                    #    'description': name,\n",
    "                    #    'folder':'GPP',\n",
    "                    #    'scale': 10,\n",
    "                    #    'region': Region})\n",
    "                    \n",
    "                    #task.start()\n",
    "                    #while task.active():\n",
    "                    #  print('Polling for task (id: {}).'.format(task.id))\n",
    "                    #  time.sleep(5)\n",
    "                    \n",
    "                    #print(urlTh)\n",
    "                    #print(url)\n",
    "                    \n",
    "                    urlTh_list_f.append(urlTh)\n",
    "                    url_list_f.append(url)\n",
    "                    \n",
    "                    # creates url \n",
    "                    #print('\\nPlease wait while the thumbnail loads, it may take a moment...')\n",
    "                    #Image(url=urlTh)\n",
    "                    \n",
    "                    vlabs    = {'Name':names_f,'Maps':urlTh_list_f, 'GeoTIFF': url_list_f}\n",
    "                    #df_vlabs = pd.DataFrame(vlabs).set_index('Name')\n",
    "                    \n",
    "                return urlTh_list_f, url_list_f, vlabs\n",
    "        \n",
    "        if mapping_GPP_thirty:\n",
    "            print('19a)  Mapping monthly GPP')\n",
    "            dashboard_30, maps_GPP_30, names_GPP_30, mean_GPP_30 = maps_GEE(30,cluster_ecosystem_geometry, n_maps_30, df_month_var, df_month_days,df_month_dates_beggining, df_month_dates_end)\n",
    "            dashboard_30.save(outputdir + '/Maps_output/' + ID + '_Dashboard_30.html')\n",
    "            \n",
    "            urlTh_list_30 = []\n",
    "            url_list_30   = []\n",
    "            \n",
    "            urlTh_list_30_test, url_list_30_test, vlabs_test_30 = export_maps(urlTh_list_30, url_list_30, names_GPP_30, maps_GPP_30)\n",
    "            \n",
    "            df_vlabs_30 = pd.DataFrame(vlabs_test_30).set_index('Name')\n",
    "            df_vlabs_30.to_excel(outputdir + '/Maps_output/' + ID + \"_Maps_30.xlsx\")   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88916e27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "        if mapping_GPP_fifteen: \n",
    "            \n",
    "            print('19b)  Mapping 15-day GPP')\n",
    "            dashboard_15, maps_GPP_15, names_GPP_15, mean_GPP_15 = maps_GEE(15,cluster_ecosystem_geometry, n_maps_15, df_15_var, df_15_days,df_15_beggining, df_15_end)\n",
    "            dashboard_15.save(outputdir + '/Maps_output/' + ID \"_Dashboard_15.html')\n",
    "            \n",
    "            urlTh_list_15 = []\n",
    "            url_list_15   = []\n",
    "            \n",
    "            urlTh_list_15_test, url_list_15_test, vlabs_test_15 = export_maps(urlTh_list_15, url_list_15, names_GPP_15, maps_GPP_15)\n",
    "            \n",
    "            df_vlabs_15 = pd.DataFrame(vlabs_test_15).set_index('Name')\n",
    "            df_vlabs_15.to_excel(outputdir + '/Maps_output/' + ID + \"_Maps_15.xlsx\")   \n",
    "            \n",
    "        t172    = ptime.time()\n",
    "        strin   = ( '{:.1f} [minutes]'.format((t172 - t171) / 60.)                                                      \n",
    "                  if (t172 - t171) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t172 - t171))\n",
    "                )\n",
    "        print('     Computed GPP maps in ', strin) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
