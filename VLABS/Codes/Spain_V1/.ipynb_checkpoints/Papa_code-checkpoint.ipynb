{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c05e66a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n27/09/2021\\n\\nIntegration of Footprint predictor model and satellite images from google earth engine \\nto derive empirical remote sensing models and monthly and annual maps.\\n\\nWritten, Mario Alberto Fuentes Monjaraz, October 2021\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This scripts runs post-processing steps for Eddy covariance data coming\n",
    "in one file in the format of europe-fluxdata.eu. This format is very similar\n",
    "to the ICOS format (the only known difference is the unit of pressure,\n",
    "which is hPa in europe-fluxdata.eu and kPa in ICOS).\n",
    "\n",
    "The script covers the following steps:\n",
    "- spike / outlier detection with mean absolute deviation filter\n",
    "  after Papale et al. (Biogeosci, 2006)\n",
    "- ustar filtering after Papale et al. (Biogeosci, 2006)\n",
    "- carbon flux partitioning with the nighttime method\n",
    "  of Reichstein et al. (Global Change Biolo, 2005) and\n",
    "  the daytime method of Lasslop et al. (Global Change Biolo, 2010)\n",
    "- gap filling with marginal distribution sampling (MDS)\n",
    "  of Reichstein et al. (Global Change Biolo, 2005)\n",
    "- flux error estimates using MDS after Lasslop et al. (Biogeosci, 2008)\n",
    "\n",
    "The script is controlled by a config file in Python's standard configparser\n",
    "format. The config file includes all possible parameters of used routines.\n",
    "Default parameter values follow the package REddyProc where appropriate. See\n",
    "comments in config file for details.\n",
    "\n",
    "The script currently flags on input all NaN values and given *undefined*\n",
    "values. Variables should be set to *undefined* in case of other existing flags\n",
    "before calling the script. Otherwise it should be easy to set the appropriate\n",
    "flags in the pandas DataFrame dff for the flags after its creation around line\n",
    "160.\n",
    "\n",
    "The output file can either have all flagged variables set to *undefined*\n",
    "and/or can include flag columns for each variable (see config file).\n",
    "\n",
    "Note, ustar filtering needs at least one full year.\n",
    "\n",
    "Examples\n",
    "--------\n",
    "python postproc_europe-fluxdata.py hesseflux_example.cfg\n",
    "\n",
    "History\n",
    "-------\n",
    "Written, Matthias Cuntz, April 2020\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "27/09/2021\n",
    "\n",
    "Integration of Footprint predictor model and satellite images from google earth engine \n",
    "to derive empirical remote sensing models and monthly and annual maps.\n",
    "\n",
    "Written, Mario Alberto Fuentes Monjaraz, October 2021\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fda4dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python packages used in the code\n",
    "from __future__ import division, absolute_import, print_function\n",
    "import time as ptime\n",
    "import sys\n",
    "import configparser\n",
    "import os.path\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hesseflux as hf\n",
    "import math\n",
    "from pyproj import Proj\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import ee\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "import folium\n",
    "from folium import plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45a8efc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1)   Readding configuration file\n",
      "2)   Formatting data frames\n",
      "      Read data:  \\NEE\\NEE_EU_format_2020.txt, \\NEE\\NEE_EU_format_2021.txt\n",
      "      Formating data:  ['Data\\\\NEE\\\\NEE_EU_format_2020.txt', 'Data\\\\NEE\\\\NEE_EU_format_2021.txt']\n",
      "     Computation setting data frames in  0 [seconds]\n",
      "3)   Spike detection\n",
      "      Using: ['FC_2']\n",
      "     Computation outlier detection in  1 [seconds]\n",
      "4)   u* filtering (less than 1-year data)\n",
      "      Using: ['FC_2', 'USTAR', 'TA']\n",
      "      Using: ['FC_2']\n",
      "     Computation u* filtering detection in  0 [seconds]\n",
      "5)   Flux partitioning\n",
      "      Using: ['FC_2', 'SW_IN', 'TA', 'VPD']\n",
      "      Nighttime partitioning\n",
      "      Falge method\n",
      "      Daytime partitioning\n",
      "     Computation flux partitioning detection in  8 [seconds]\n",
      "6)   Gap-filling\n",
      "      Using: ['FC_2', 'GPP_FC_2_rei', 'RECO_FC_2_rei', 'GPP_FC_2_fal', 'RECO_FC_2_fal', 'GPP_FC_2_las', 'RECO_FC_2_las', 'SW_IN', 'TA', 'VPD']\n",
      "  Filling  FC_2\n",
      "  Filling  GPP_FC_2_rei\n",
      "  Filling  RECO_FC_2_rei\n",
      "  Filling  GPP_FC_2_fal\n",
      "  Filling  RECO_FC_2_fal\n",
      "  Filling  GPP_FC_2_las\n",
      "  Filling  RECO_FC_2_las\n",
      "     Computation filling gaps detection in  1.3 [minutes]\n",
      "8)   Outputfile\n",
      "      Write output  Outdir\\GPP\\GPP_corrections_with_flags.txt\n",
      "      Add flag columns for gap-filled variables.\n",
      "      Write.\n",
      "      Creating output file in  0 [seconds]\n",
      "9)   Daily GPP\n",
      "     Computed daily GPP in  0 [seconds]\n",
      "Total time of correction of data  1.4 [minutes]\n"
     ]
    }
   ],
   "source": [
    "#Function to identify columns with specific beggining \n",
    "def _findfirststart(starts, names):\n",
    "    \"\"\"\n",
    "    Function that finds variables on the head of a table indicating the name or label of the variable  \n",
    "    and creates a list of the variables located. The \n",
    "    \"\"\"\n",
    "    hout = []\n",
    "    for hh in starts:\n",
    "        for cc in names:\n",
    "            if cc.startswith(hh):\n",
    "                hout.append(cc)\n",
    "                break\n",
    "    return hout\n",
    "\n",
    "#The workflow start here\n",
    "if __name__ == '__main__':\n",
    "    t1 = ptime.time()\n",
    "    \n",
    "    #*********************************************************************************************************************************************************************\n",
    "    #1)   Read configuration file\n",
    "    print('1)   Readding configuration file')\n",
    "    \n",
    "    #1.a) Read from command-line interpreter (It must include in the cosole \"GPP.py Configs.cfg\" located in the same file)\n",
    "    #if len(sys.argv) <= 1:\n",
    "    #raise IOError('Input configuration file must be given.')\n",
    "    #configfile = sys.argv[1]                                                                          #Change 1. Read configuration (different methods)    \n",
    "                                                                                                       #For this option a Configs folder is required in the main directory\n",
    "    #1.b)Read from directory path\n",
    "    #configfile = 'C:/Users/Administrador/OneDrive/Documentos/MSc Thesis/Configs/DNP_e_shape_configuration.cfg'  \n",
    "    #configfile = 'C:/Users/Usuario/Documents/Pdrive/Final python codes Anna/Codes/'\n",
    "    configfile = 'Configs/Configs.cfg'                                                \n",
    "    \n",
    "    #1.c)Read from gui window\n",
    "    #configfile = hf.files_from_gui(initialdir='.', title='configuration file')\n",
    "    \n",
    "    config = configparser.ConfigParser(interpolation=None)                                             #Constructor\n",
    "    config.read(configfile)                                                                            #Read configuration file with the constructor\n",
    "    \n",
    "    # file path\n",
    "    datadir   = config['GENERAL'].get('datadir', \".\")                                                  #Change 2. Add datadir to read folder from Data folder\n",
    "    outdir    = config['GENERAL'].get('outdir', \".\")\n",
    "    \n",
    "    # meteorological data\n",
    "    meteo_file    = config['DB1FILES'].get('meteo_file', \".\")\n",
    "    \n",
    "    # program switches                                                                                 #Activates each module of the workflow)\n",
    "    #------------------------------------------------------------\n",
    "    outlier   = config['POSTSWITCH'].getboolean('outlier',   True)\n",
    "    ustar     = config['POSTSWITCH'].getboolean('ustar',     True)\n",
    "    ustar_noyear  = config['POSTSWITCH'].getboolean('ustar_noyear',     True)                          #Change 3. Add method ustar_noyear to compute the u* filter with a given threshold.\n",
    "    partition = config['POSTSWITCH'].getboolean('partition', True)                                     #ustar_noyear method has to be used instead of ustar when there is not data \n",
    "    fill      = config['POSTSWITCH'].getboolean('fill',      True)                                     #for a full date to calculate automatically a threshold\n",
    "    fluxerr   = config['POSTSWITCH'].getboolean('fluxerr',   True)\n",
    "    #------------------------------------------------------------\n",
    "    daily_values              =  config['POSTSWITCH'].getboolean('daily_values',                True)  #Change 4. All the modules \n",
    "    #------------------------------------------------------------\n",
    "    climatological_footprint  =  config['POSTSWITCH'].getboolean('climatological_footprint ',   True) \n",
    "    #------------------------------------------------------------\n",
    "    vegetation_indices        =  config['POSTSWITCH'].getboolean('vegetation_indices',          True)\n",
    "    #------------------------------------------------------------\n",
    "    environmental_variables_station     =  config['POSTSWITCH'].getboolean('environmental_variables_station',          True)\n",
    "    environmental_variables_satellite   =  config['POSTSWITCH'].getboolean('environmental_variables_satellite',          True)\n",
    "    tower_observations                  =  config['POSTSWITCH'].getboolean('tower_observations',          True)\n",
    "    #------------------------------------------------------------\n",
    "    correlation_analysis        =  config['POSTSWITCH'].getboolean('correlation_analysis',          True)\n",
    "    correlation_analysis_simple =  config['POSTSWITCH'].getboolean('correlation_analysis',          True)\n",
    "    calibration_validation      =  config['POSTSWITCH'].getboolean('calibration_validation',          True) \n",
    " \n",
    "    # input file format\n",
    "    eufluxfile  = config['POSTIO'].get('inputfile',  '')\n",
    "    timeformat  = config['POSTIO'].get('timeformat', '%Y%m%d%H%M')\n",
    "    sep         = config['POSTIO'].get('sep',        ',')\n",
    "    skiprows    = config['POSTIO'].get('skiprows',   '')\n",
    "    undef       = config['POSTIO'].getfloat('undef', -9999.)\n",
    "    swthr       = config['POSTIO'].getfloat('swthr', 10.)\n",
    "    outputfile  = config['POSTIO'].get('outputfile'  '')\n",
    "    outundef    = config['POSTIO'].getboolean('outundef',    False)\n",
    "    outflagcols = config['POSTIO'].getboolean('outflagcols', False)\n",
    "\n",
    "    # input file variables \n",
    "    carbonflux     = config['POSTVAR'].get('carbonflux',        'FC')                                  #Change 4. Add variable to identify the name of the carbon fluxes to compute \n",
    "                                                                                                       #Carbon flux name to process in the code (e.g. NEE, FC, FC_1)                                                                                              #This change can be done for all the column names of the input file \n",
    "    # mad parameters\n",
    "    nscan = config['POSTMAD'].getint('nscan', 15)\n",
    "    nfill = config['POSTMAD'].getint('nfill',  1)\n",
    "    z     = config['POSTMAD'].getfloat('z',    7)\n",
    "    deriv = config['POSTMAD'].getint('deriv',  2)\n",
    "    \n",
    "    # ustar parameters\n",
    "    ustarmin       = config['POSTUSTAR'].getfloat('ustarmin',    0.1)\n",
    "    nboot          = config['POSTUSTAR'].getint('nboot',         1)\n",
    "    plateaucrit    = config['POSTUSTAR'].getfloat('plateaucrit', 0.95)\n",
    "    seasonout      = config['POSTUSTAR'].getboolean('seasonout', False)                                #Change 5. Add these parameters in the configuration file                      \n",
    "    applyustarflag = config['POSTUSTAR'].getboolean('applyustarflag', True)\n",
    "\n",
    "    # gap-filling parameters\n",
    "    sw_dev  = config['POSTGAP'].getfloat('sw_dev',  50.)\n",
    "    ta_dev  = config['POSTGAP'].getfloat('ta_dev',  2.5)\n",
    "    vpd_dev = config['POSTGAP'].getfloat('vpd_dev', 5.0)\n",
    "    longgap = config['POSTGAP'].getint('longgap',   60)\n",
    "    \n",
    "    # partitioning parameters \n",
    "    nogppnight = config['POSTPARTITION'].getboolean('nogppnight', False)\n",
    "    \n",
    "    # climatological footprint parameters\n",
    "    altitude                        = config['CLIMATOLOGICAL'].getfloat('altitude',1.0)                                \n",
    "    latitude                        = config['CLIMATOLOGICAL'].getfloat('latitude', 36.9985)                        \n",
    "    longitude                       = config['CLIMATOLOGICAL'].getfloat('longitude', -6.4345)                        \n",
    "    canopy_height                   = config['CLIMATOLOGICAL'].getfloat('canopy_height ',  0.7)                           \n",
    "    displacement_height             = config['CLIMATOLOGICAL'].getfloat('displacement_height',  0.2)                          \n",
    "    roughness_lenght                = config['CLIMATOLOGICAL'].getfloat('roughness_lenght ',  -999)                           \n",
    "    instrument_height_anenometer    = config['CLIMATOLOGICAL'].getfloat('instrument_height_anenometer',  3.95)\n",
    "    instrument_height_gas_analyzer  = config['CLIMATOLOGICAL'].getfloat('instrument_height_gas_analyzer',  4.03)\n",
    "    projection_site                 = config['CLIMATOLOGICAL'].get('projection_site ', '+proj=utm +zone=29 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs') #Change 5.1 Projection of climatological footprint\n",
    "    \n",
    "    # vegetation indices parameters \n",
    "    max_cloud_coverage              = config['VI'].getint('max_cloud_coverage',         100)\n",
    "    crs                             = config['VI'].get('crs',                  'EPSG:4326')\n",
    "    ndviMask                        = config['VI'].getfloat('ndviMask',-100)\n",
    "    mndviMask                       = config['VI'].getfloat('mndviMask',-100)\n",
    " \n",
    "    #*********************************************************************************************************************************************************************\n",
    "    #2)   Setting data frames\n",
    "    print('2)   Formatting data frames')\n",
    "    t01 = ptime.time()\n",
    "    \n",
    "    #------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    #2.a)   Read eddy covariance files (eufluxfiles)\n",
    "    print('      Read data: ', eufluxfile)\n",
    "    \n",
    "    # Assert iterable                                                                                  #This process reads the names of the eufluxfiles and adds the directory information\n",
    "    if ',' in eufluxfile:\n",
    "        eufluxfile = eufluxfile.split(',')\n",
    "        \n",
    "        eufluxfile = [ datadir + ee.strip() for ee in eufluxfile ]                                     #Change 6. Add datadir                                                                                 \n",
    "    else:                                                                                              #If the datadir is not added in this section, the input file need to be in the main directory and not in the datadir file          \n",
    "        if eufluxfile:                                                                                   \n",
    "            eufluxfile = [datadir + eufluxfile]\n",
    "        else:\n",
    "            try:\n",
    "                eufluxfile = hf.files_from_gui(\n",
    "                    initialdir='.', title='europe-fluxdata.eu file(s)')\n",
    "            except:\n",
    "                raise IOError(\"GUI for europe-fluxdata.eu file(s) failed.\")\n",
    "\n",
    "    # Identify rows in the dataframe to skipt              \n",
    "    if skiprows == 'None':                                                                             #This process allows to identify the rows to skipt in the data frames\n",
    "        skiprows = ''\n",
    "    if skiprows:\n",
    "        import json  # to analyse int or list, tuple not working\n",
    "        skiprows = json.loads(skiprows.replace('(', '[').replace(')', ']'))\n",
    "        \n",
    "    # Read input files into Panda data frame and check variable availability\n",
    "    parser = lambda date: dt.datetime.strptime(date, timeformat)                               \n",
    "\n",
    "    infile = eufluxfile[0]                                                                             #Loads the first file in the eufluxfile list                                                                                          \n",
    "    df = pd.read_csv(infile, sep, skiprows=skiprows, parse_dates=[0], \n",
    "                     date_parser=parser, index_col=0, header=0)\n",
    "    if len(eufluxfile) > 1:                                                                            #Iterate to integrate all the files in case of data for different years is available \n",
    "        for infile in eufluxfile[1:]:                    \n",
    "            df1 = pd.read_csv(infile, sep, skiprows=skiprows, parse_dates=[0],\n",
    "                              date_parser=parser, index_col=0, header=0)\n",
    "            df  = df.append(df1, sort=False)\n",
    "            \n",
    "    #------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    #2.b)   Formatting the input file    \n",
    "    print('      Formating data: ', eufluxfile)\n",
    "    \n",
    "    # Fill the undef values (e.g. -9999.) with null values (NaN)\n",
    "    df.fillna(undef, inplace=True)\n",
    "    #df.replace(undef, np.nan, inplace=True)\n",
    "            \n",
    "    # Flag.                                                                                             #Create file with flags\n",
    "    dff              = df.copy(deep=True)\n",
    "    dff[:]           = 0\n",
    "    dff[df == undef] = 2                                                                               #(Flag 2) for null values\n",
    "    #dff[df.isna()]   = 2\n",
    "\n",
    "    # day / night\n",
    "    #isday = df['SW_IN'] > swthr                                                                       #This column in the data frame indicates the short wave radiation which\n",
    "    hsw = ['SW_IN']                                                                                    #can be use to identify difference between day an night. Threshold is set in the configuration file\n",
    "    hout = _findfirststart(hsw, df.columns)                                                            #Change 7. Use _findfirststart method to look for the SW_IN column\n",
    "    isday = df[hout[0]] >= swthr\n",
    "    \n",
    "    # Remove 'SW_IN' data from the data frame. \n",
    "    df['SW_IN']=-9999.                                                                                 #Change 7.1 Remove SW_IN\n",
    "    df['SW_IN'].replace(-9999., np.nan, inplace=True)                                                                               #Change 7.1 Remove SW_IN. This change is just relevant for the study case of Doñana\n",
    "       \n",
    "    # Check Ta in Kelvin\n",
    "    hta = ['TA']                                                                                       #Change 8. Change TA_ for TA. Allows more flexibility in the column names of the input file\n",
    "    hout = _findfirststart(hta, df.columns)\n",
    "    if df[hout[0]].max() < 100.:\n",
    "        tkelvin = 273.15\n",
    "    else:\n",
    "        tkelvin = 0.\n",
    "        \n",
    "    # Add tkelvin only where not flagged\n",
    "    df.loc[dff[hout[0]] == 0, hout[0]] += tkelvin\n",
    "    \n",
    "    # Add vpd if not given\n",
    "    hvpd = ['VPD']\n",
    "    hout = _findfirststart(hvpd, df.columns)\n",
    "    if len(hout) == 0:\n",
    "        hvpd = ['TA', 'RH']                                                                            #Change 9. Change TA_ and RH_ for TA and RH                                                                               \n",
    "        hout = _findfirststart(hvpd, df.columns)\n",
    "        if len(hout) != 2:\n",
    "            raise ValueError('Cannot calculate VPD.')\n",
    "        ta_id = hout[0]\n",
    "        rh_id = hout[1]\n",
    "        if df[ta_id].max() < 100.:\n",
    "            tk = df[ta_id] + 273.15\n",
    "        else:\n",
    "            tk = df[ta_id]\n",
    "        if df[rh_id].max() > 10.:\n",
    "            rh = df[rh_id] / 100.\n",
    "        else:\n",
    "            rh = df[rh_id]\n",
    "        vpd = (1. - rh) * hf.esat(tk)\n",
    "        vpd_id = 'VPD_CALC'\n",
    "        df[vpd_id] = vpd\n",
    "        df[vpd_id].where((df[ta_id] != undef) | (df[rh_id] != undef),\n",
    "                         other=undef, inplace=True)\n",
    "        dff[vpd_id] = np.where((dff[ta_id] + dff[rh_id]) > 0, 2, 0)                                    #(Flag 2) in 'VPD_CALC'  where ta or rh is not available\n",
    "        df.loc[dff[vpd_id] == 0, vpd_id] /= 100.                                                       #Converts from \n",
    "\n",
    "    # Check VPD in Pa\n",
    "    hvpd = ['VPD']\n",
    "    hout = _findfirststart(hvpd, df.columns)\n",
    "    if df[hout[0]].max() < 10.:     # kPa\n",
    "        vpdpa = 1000.\n",
    "    elif df[hout[0]].max() < 100.:  # hPa\n",
    "        vpdpa = 100.\n",
    "    else:\n",
    "        vpdpa = 1.                  # Pa\n",
    "    df.loc[dff[hout[0]] == 0, hout[0]] *= vpdpa   \n",
    "    \n",
    "    # Time stepping                                                                                    #Derives the number of datapoints per day\n",
    "    dsec  = (df.index[1] - df.index[0]).seconds\n",
    "    ntday = np.rint(86400 / dsec).astype(np.int)\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    t02   = ptime.time()                                                                               #Change 9. Change legend of computation time\n",
    "    strin = ( '{:.1f} [minutes]'.format((t02 - t01) / 60.)                                           \n",
    "              if (t02 - t01) > 60.\n",
    "              else '{:d} [seconds]'.format(int(t02 - t01))\n",
    "            )\n",
    "    print('     Computation setting data frames in ', strin , end='\\n')    \n",
    "\n",
    "    #********************************************************************************************************************************************************************* \n",
    "    # 3)   Outlier detection\n",
    "\n",
    "    if outlier:\n",
    "        print('3)   Spike detection')\n",
    "        t11 = ptime.time()\n",
    "\n",
    "        # Finds carbon flux data (e.g. NEE or FC)\n",
    "        houtlier = [carbonflux]                                                                        #Change 10. Process only the carbonflux variable (H and LE could be processed in the same way)                                            \n",
    "        hout = _findfirststart(houtlier, df.columns)\n",
    "        print('      Using:', hout)\n",
    "        \n",
    "        # Applies the spike detection. Only one call to mad for all variables                         #carbonflux variable can be a list with NEE, H, LE, etc. and the .madspike() requires to be called only once for alll the variables\n",
    "        sflag = hf.madspikes(df[hout], flag=dff[hout], isday=isday,                                    #This function creates flags with value 2 for outliers that are translated to flags 3 in the dff file\n",
    "                             undef=undef, nscan=nscan * ntday,                                 \n",
    "                             nfill=nfill * ntday, z=z, deriv=deriv, plot=False)\n",
    "        \n",
    "        for ii, hh in enumerate(hout):\n",
    "            dff.loc[sflag[hh] == 2, hh] = 3                                                            #(Flag 3) for outlieres\n",
    "            dff.loc[df[hh] == undef , hh] = 2\n",
    "\n",
    "        t12   = ptime.time()                                                                           #Change 11. Change legend of computation time  \n",
    "        strin = ( '{:.1f} [minutes]'.format((t12 - t11) / 60.)                                                \n",
    "                  if (t12 - t11) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t12 - t11))\n",
    "                )\n",
    "        print('     Computation outlier detection in ', strin)  \n",
    "        \n",
    "    #********************************************************************************************************************************************************************* \n",
    "    # 4)   u* filtering (data for a full year)\n",
    " \n",
    "    if  ustar:                                                                                         #This method requires a data set with data for a full year\n",
    "        print('4)   u* filtering')\n",
    "        t21 = ptime.time()\n",
    "        \n",
    "        #Looking for carbonflux, u*, and temperature data\n",
    "        hfilt = [carbonflux, 'USTAR', 'TA']                                                            #Change 12. Change 'NEE' for carbonflux variable\n",
    "        hout  = _findfirststart(hfilt, df.columns)\n",
    "        assert len(hout) == 3, 'Could not find CO2 flux (NEE or FC), USTAR or TA in input file.'\n",
    "        print('      Using:', hout)\n",
    "        \n",
    "        #Saves a copy of the flags of the carbonflux data\n",
    "        ffsave = dff[hout[0]].to_numpy()\n",
    "        \n",
    "        #Sets a temporal flag \n",
    "        iic    = np.where((~isday) & (df[hout[0]] < 0.))[0]                        \n",
    "        dff.iloc[iic, list(df.columns).index(hout[0])] = 4                                             #(Flag 4). Temporal flag for data with negative values in the carbon fluxes during night\n",
    "        \n",
    "        # Applies the u* filtering\n",
    "        ustars, flag = hf.ustarfilter(df[hout], flag=dff[hout],                                        #Check method to identify why the temporal flag is required in the ustarfilter\n",
    "                                      isday=isday, undef=undef,                                        #ustarfilter function creates flags with value 2 for outliers that are translated to flags 3 in the dff file                 \n",
    "                                      ustarmin=ustarmin, nboot=nboot,\n",
    "                                      plateaucrit=plateaucrit,\n",
    "                                      seasonout=seasonout,\n",
    "                                      plot=True)\n",
    "        dff[hout[0]] = ffsave                                                                          #Return to original flags file without 4-flag\n",
    "        df  = df.assign(USTAR_TEST=flag)                                                               #Change 14. Change 'USTAR_TEST_1_1_1' column name for 'USTAR_TEST'                                                                \n",
    "        dff = dff.assign(USTAR_TEST=np.zeros(df.shape[0], dtype=np.int))                               #This line adds a column in the dataframe of flags \n",
    "        \n",
    "        if applyustarflag:\n",
    "        #if False:\n",
    "            hustar = [carbonflux]                                                                      #Change 15. Process only the carbonflux variable (H and LE could be processed in the same way)   \n",
    "            hout = _findfirststart(hustar, df.columns)\n",
    "            print('      Using:', hout)\n",
    "            for ii, hh in enumerate(hout):\n",
    "                dff.loc[flag [hh] == 2, hh] = 5                                                        #(Flag 5) for carbon fluxes with ustar(friction velocity) below calculated threshold\n",
    "                                \n",
    "        t22   = ptime.time()                                                                           #Change 16. Change legend of computation time.\n",
    "        strin = ( '{:.1f} [minutes]'.format((t22 - t21) / 60.)                                           \n",
    "                  if (t22 - t21) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t22 - t21))\n",
    "                )\n",
    "        print('     Computation u* filtering detection in ', strin) \n",
    "\n",
    "    #------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # 4)   u* filtering (data for partial year)\n",
    "\n",
    "    if  ustar_noyear:                                                                                  #Change 17. ustar_noyear is a copy of ustar without the 'ustarfilter'\n",
    "        print('4)   u* filtering (less than 1-year data)')                                             #The ustar_noyear method is simple approach to manually set a ustar threshold when \n",
    "        t21 = ptime.time()                                                                             #there is no data for a full year required to compute ustar\n",
    "        \n",
    "        #Looking for carbonflux, u*, and temperature data\n",
    "        hfilt = [carbonflux, 'USTAR', 'TA']                                                            \n",
    "        hout  = _findfirststart(hfilt, df.columns)\n",
    "        assert len(hout) == 3, 'Could not find CO2 flux (NEE or FC), USTAR or TA in input file.'\n",
    "        print('      Using:', hout)\n",
    "        \n",
    "        ffsave = dff[hout[0]].to_numpy()\n",
    "        flag = sflag.copy().multiply(0)\n",
    "        \n",
    "        #flag.loc[(df['USTAR'] < ustarmin) & (dff[carbonflux] == 2), carbonflux] = 2.\n",
    "        #flag.loc[(df['USTAR'] < ustarmin) & (dff[carbonflux] == 3), carbonflux] = 2.\n",
    "        #flag.loc[(df['USTAR'] < ustarmin), carbonflux] = 2.\n",
    "        flag.loc[(df['USTAR'] < ustarmin) & (dff['USTAR'] != 2) & (dff[carbonflux] != 2), carbonflux] = 2.\n",
    "        \n",
    "        dff[hout[0]] = ffsave                         \n",
    "        df  = df.assign(USTAR_TEST=flag)               \n",
    "        dff = dff.assign(USTAR_TEST=np.zeros(df.shape[0], dtype=np.int))\n",
    "\n",
    "        if applyustarflag:\n",
    "        #if False:\n",
    "            hustar = [carbonflux]\n",
    "            hout = _findfirststart(hustar, df.columns)\n",
    "            print('      Using:', hout)\n",
    "            for ii, hh in enumerate(hout):\n",
    "                dff.loc[flag[hh] == 2, hh] = 5 \n",
    "\n",
    "        t22   = ptime.time()                                                                           \n",
    "        strin = ( '{:.1f} [minutes]'.format((t22 - t21) / 60.)                                           \n",
    "                  if (t22 - t21) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t22 - t21))\n",
    "                )\n",
    "        print('     Computation u* filtering detection in ', strin) \n",
    "                \n",
    "    #********************************************************************************************************************************************************************* \n",
    "    # 5)   Flux partitioning\n",
    "\n",
    "    if partition:\n",
    "        print('5)   Flux partitioning')\n",
    "        t31 = ptime.time()\n",
    "        \n",
    "        #Looking for carbon flux, global radiation, temperature and vpd data\n",
    "        hpart = [carbonflux, 'SW_IN', 'TA', 'VPD']                                                     #Change 18. Change 'NEE' for carbonflux variable                                                                      \n",
    "        hout  = _findfirststart(hpart, df.columns)\n",
    "        assert len(hout) == 4, 'Could not find CO2 flux (NEE or FC), SW_IN, TA, or VPD in input file.'\n",
    "        print('      Using:', hout)\n",
    "\n",
    "        suff = hout[0]                                                                                 #Change 20. Rename with the carbonflux variable              \n",
    " \n",
    "        # nighttime method\n",
    "        print('      Nighttime partitioning')\n",
    "        dfpartn = hf.nee2gpp(df[hout], flag=dff[hout], isday=isday,\n",
    "                             undef=undef, method='reichstein',\n",
    "                             nogppnight=nogppnight)\n",
    "\n",
    "        dfpartn.rename(columns=lambda c: c + '_' + suff + '_rei', inplace=True)                          #Change 21. Add '_' before suff and change '1' with '_1'\n",
    "            \n",
    "        # falge method                                                                                 #Change 22. Falge method instead of lasslop method\n",
    "        print('      Falge method')\n",
    "        dfpartf = hf.nee2gpp(df[hout], flag=dff[hout], isday=isday,\n",
    "                             undef=undef, method='falge',         \n",
    "                             nogppnight=nogppnight)  \n",
    "        \n",
    "        dfpartf.rename(columns=lambda c: c + '_' + suff + '_fal', inplace=True)\n",
    "        \n",
    "        # daytime method                                                                               #Change 23. Day time method 'lasslop' can be integrated as a third method\n",
    "        print('      Daytime partitioning')\n",
    "        dfpartd = hf.nee2gpp(df[hout], flag=dff[hout], isday=isday,\n",
    "                             undef=undef, method='lasslop',\n",
    "                             nogppnight=nogppnight)\n",
    "        \n",
    "        dfpartd.rename(columns=lambda c: c  + '_' + suff + '_las', inplace=True) \n",
    "\n",
    "        df = pd.concat([df, dfpartn, dfpartf, dfpartd],  axis=1)\n",
    "\n",
    "        # take flags from NEE or FC same flag\n",
    "        for dn in ['rei', 'fal', 'las']:\n",
    "            for gg in ['GPP', 'RECO']:                                                                 #Change 24. Adds '_' between labels\n",
    "                dff[gg + '_' + suff + '_'+ dn] = dff[hout[0]]                                          #Takes flags from the carbonflux variable\n",
    "                \n",
    "        # flag GPP and RECO if they were not calculated\n",
    "        for dn in ['rei', 'fal', 'las']:\n",
    "            for gg in ['GPP', 'RECO']:\n",
    "                dff.loc[df['GPP' + '_' + suff + '_'+ dn] == undef, gg + '_' + suff + '_'+ dn ] = 2 \n",
    "\n",
    "        # flag RECO when GPP was not calculated\n",
    "        #for dn in ['1', '2']:                                                                          #Change 25. This method flags with 2 value the 'RECO' columns when 'GPP was not calculated  \n",
    "        #    for gg in ['GPP']:                                                                         #('GPP' == undef)\n",
    "        #        dff.loc[df[gg + '_' + suff + '_'+ dn] == undef, 'RECO' + '_' + suff + '_'+ dn ] = 2 \n",
    "          \n",
    "        t32   = ptime.time()\n",
    "        strin = ( '{:.1f} [minutes]'.format((t32 - t31) / 60.)                                         #Change 26. Change legend of computation time.         \n",
    "                  if (t32 - t31) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t32 - t31))\n",
    "                )\n",
    "        print('     Computation flux partitioning detection in ', strin)  \n",
    "        \n",
    "    #********************************************************************************************************************************************************************* \n",
    "    # 6)   Gap-filling\n",
    "\n",
    "    if fill:        \n",
    "        print('6)   Gap-filling')\n",
    "        t41 = ptime.time()\n",
    "        \n",
    "        #Looking for meteorological data\n",
    "        hfill = ['SW_IN', 'TA', 'VPD']\n",
    "        hout  = _findfirststart(hfill, df.columns)\n",
    "        assert len(hout) == 3, 'Could not find SW_IN, TA or VPD in input file.'\n",
    "\n",
    "        # if available\n",
    "        rei_gpp = 'GPP_'+carbonflux+'_rei'\n",
    "        rei_res = 'RECO_'+carbonflux+'_rei'\n",
    "        fal_gpp = 'GPP_'+carbonflux+'_fal'\n",
    "        fal_res = 'RECO_'+carbonflux+'_fal'\n",
    "        las_gpp = 'GPP_'+carbonflux+'_las'\n",
    "        las_res = 'RECO_'+carbonflux+'_las'\n",
    "        \n",
    "        hfill = [ carbonflux,                                                                          #Change 27. Change names of columns to process\n",
    "                  rei_gpp,rei_res,fal_gpp,fal_res,las_gpp,las_res,\n",
    "                  'SW_IN', 'TA', 'VPD']\n",
    "        \n",
    "        hout  = _findfirststart(hfill, df.columns)\n",
    "        print('      Using:', hout)\n",
    "        \n",
    "        df_f, dff_f = hf.gapfill(df[hout], flag=dff[hout],\n",
    "                                 sw_dev=sw_dev, ta_dev=ta_dev, vpd_dev=vpd_dev,\n",
    "                                 longgap=longgap, undef=undef, err=False,\n",
    "                                 verbose=1)\n",
    "        \n",
    "        #hdrop = ['SW_IN', 'TA', 'VPD']                           \n",
    "        #hout = _findfirststart(hdrop, df.columns)\n",
    "        #df_f.drop(columns=hout,  inplace=True)\n",
    "        #dff_f.drop(columns=hout, inplace=True)\n",
    "\n",
    "        \n",
    "        def _add_f(c):\n",
    "            return '_'.join(c.split('_')[:-3] + c.split('_')[-3:]  + ['f'])                            #Change 28. 'f' of fill till the end of the name of the column names\n",
    "        df_f.rename(columns=_add_f,  inplace=True)\n",
    "        dff_f.rename(columns=_add_f, inplace=True)    \n",
    "        \n",
    "        df  = pd.concat([df,  df_f],  axis=1)\n",
    "        dff = pd.concat([dff, dff_f], axis=1)\n",
    "        #df.replace(undef, np.nan, inplace=True)\n",
    "        \n",
    "        t42   = ptime.time()                                                                           #Change 29. Change legend of computation time.\n",
    "        strin = ( '{:.1f} [minutes]'.format((t42 - t41) / 60.)                                           \n",
    "                  if (t42 - t41) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t42 - t41))\n",
    "                )\n",
    "        print('     Computation filling gaps detection in ', strin) \n",
    "        \n",
    "    #********************************************************************************************************************************************************************* \n",
    "    # 7)   Error estimate\n",
    "\n",
    "    if fluxerr:\n",
    "        print('7)   Flux error estimates')\n",
    "        t51 = ptime.time()\n",
    "        \n",
    "        #Looking for meteorological data\n",
    "        hfill = ['SW_IN', 'TA', 'VPD']\n",
    "        hout  = _findfirststart(hfill, df.columns)\n",
    "        assert len(hout) == 3, 'Could not find SW_IN, TA or VPD in input file.'\n",
    "\n",
    "        # if available \n",
    "        rei_gpp = 'GPP_'+carbonflux+'_rei'\n",
    "        rei_res = 'RECO_'+carbonflux+'_rei'\n",
    "        fal_gpp = 'GPP_'+carbonflux+'_fal'\n",
    "        fal_res = 'RECO_'+carbonflux+'_fal'\n",
    "        las_gpp = 'GPP_'+carbonflux+'_las'\n",
    "        las_res = 'RECO_'+carbonflux+'_las'\n",
    "        \n",
    "        hfill = [ carbonflux,                                                                                  #Change 30. Change names of columns to process\n",
    "                  rei_gpp,rei_res,fal_gpp,fal_res,las_gpp,las_res,\n",
    "                  'SW_IN', 'TA', 'VPD']\n",
    "        \n",
    "        hout  = _findfirststart(hfill, df.columns)\n",
    "        print('      Using:', hout)\n",
    "        \n",
    "        df_f = hf.gapfill(df[hout], flag=dff[hout],\n",
    "                          sw_dev=sw_dev, ta_dev=ta_dev, vpd_dev=vpd_dev,\n",
    "                          longgap=longgap, undef=undef, err=True, \n",
    "                          verbose=1)\n",
    "        \n",
    "        hdrop = ['SW_IN', 'TA', 'VPD']\n",
    "        hout = _findfirststart(hdrop, df.columns)\n",
    "        df_f.drop(columns=hout, inplace=True)\n",
    "\n",
    "        colin = list(df_f.columns)\n",
    "\n",
    "        def _add_e(c):                                                                                 #Change 31. Create _add_e instead of reusing _add_f\n",
    "            return '_'.join(c.split('_')[:-3] + c.split('_')[-3:] + ['e'])\n",
    "\n",
    "        # rename the variables with e (error)\n",
    "        df_f.rename(columns=_add_e,  inplace=True)\n",
    "        colout = list(df_f.columns)\n",
    "        df = pd.concat([df, df_f], axis=1)\n",
    "        \n",
    "        # take flags of non-error columns with the same label\n",
    "        for cc in range(len(colin)):\n",
    "            dff[colout[cc]] = dff[colin[cc]]\n",
    "\n",
    "        t52   = ptime.time()                                                                           #Change 32. Change legend of computation time.\n",
    "        strin = ( '{:.1f} [minutes]'.format((t52 - t51) / 60.)                                           \n",
    "                  if (t52 - t51) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t52 - t51))\n",
    "                )\n",
    "        print('     Computation flux error estimates in ', strin) \n",
    "\n",
    "    #********************************************************************************************************************************************************************* \n",
    "    # 8)   Output\n",
    "    \n",
    "    print('8)   Outputfile')\n",
    "    t61 = ptime.time()\n",
    "\n",
    "    if not outputfile:\n",
    "        try:\n",
    "            outputdir = hf.directory_from_gui(initialdir='.',\n",
    "                                              title='Output directory')\n",
    "        except:\n",
    "            raise IOError(\"GUI for output directory failed.\")\n",
    "            \n",
    "        outputfile = configfile[:configfile.rfind('.')]                                                #Takes the name from the configurtion file\n",
    "        outputfile = outputdir + '/' + os.path.basename(outputfile + '.csv')                           #Change 33. Change outdir for outputdir to select directly the output folder\n",
    "    else:\n",
    "        outputfile = outdir + outputfile                                                               #Change 34. Create outputfile in case outputfile and outputdir are available \n",
    "        \n",
    "    print('      Write output ', outputfile)\n",
    "\n",
    "    # Back to original units\n",
    "    hta = ['TA']\n",
    "    hout = _findfirststart(hta, df.columns)\n",
    "    df.loc[dff[hout[0]] == 0, hout[0]] -= tkelvin\n",
    "    hvpd = ['VPD']\n",
    "    hout = _findfirststart(hvpd, df.columns)\n",
    "    df.loc[dff[hout[0]] == 0, hout[0]] /= vpdpa\n",
    "\n",
    "    if outundef:\n",
    "        print('      Set flags to undef.')\n",
    "        for cc in df.columns:\n",
    "            if cc.split('_')[-1] != 'f' and cc.split('_')[-1] != 'e':  # exclude gap-filled columns    #Change 35. Change [-4] for [-1] and exclude error 'e' columns\n",
    "                df[cc].where(dff[cc] == 0, other=undef, inplace=True)                                  #This line writes undef (-9999.) for all the flagged data\n",
    "\n",
    "    if outflagcols:\n",
    "        print('      Add flag columns.')\n",
    "\n",
    "        def _add_flag(c):\n",
    "            return 'flag_' + c\n",
    "        dff.rename(columns=_add_flag, inplace=True)\n",
    "        \n",
    "        # no flag columns for flags\n",
    "        dcol = []\n",
    "        for hh in dff.columns:\n",
    "            if '_TEST' in hh:                                                                          #Change 36. Change '_TEST_' for '_TEST'\n",
    "                dcol.append(hh)\n",
    "        if dcol:\n",
    "            dff.drop(columns=dcol, inplace=True)                                                       #Remove the TEST columns\n",
    "        df = pd.concat([df, dff], axis=1)\n",
    "    else:\n",
    "        print('      Add flag columns for gap-filled variables.')\n",
    "        occ = []\n",
    "        for cc in df.columns:\n",
    "            if cc.split('_')[-1] == 'f' or cc.split('_')[-1] == 'e':                                  #Change 37. Add the error columns 'e' in the condition\n",
    "                occ.append(cc)\n",
    "        dff1 = dff[occ].copy(deep=True)\n",
    "        dff1.rename(columns=lambda c: 'flag_' + c, inplace=True)\n",
    "        df = pd.concat([df, dff1], axis=1)\n",
    "    print('      Write.')\n",
    "    \n",
    "    \n",
    "    df.to_csv(outputfile, sep=sep, na_rep=str(undef), index=True,\n",
    "              date_format=timeformat)\n",
    "    \n",
    "    \n",
    "    t62   = ptime.time()\n",
    "    strin = ( '{:.1f} [minutes]'.format((t62 - t61) / 60.)                                             #Change 37. Change legend of computation time.          \n",
    "              if (t62 - t61) > 60.\n",
    "              else '{:d} [seconds]'.format(int(t62 - t61))\n",
    "            )\n",
    "    print('      Creating output file in ', strin) \n",
    "\n",
    "    #********************************************************************************************************************************************************************* \n",
    "   # Next elements are complement modules to compute Remote Sensing empirical models of GPP            #Change 39. All below code is extra code to derive empirical models\n",
    "    #********************************************************************************************************************************************************************* \n",
    "    # 9)   Daily estimations \n",
    "\n",
    "    if daily_values:                                                                                    ####Parameter\n",
    "        \n",
    "        print('9)   Daily GPP')\n",
    "        t71 = ptime.time()\n",
    "\n",
    "        # Daily GPP and enviromental drivers\n",
    "        gpp = df.copy()\n",
    "        gpp = gpp[(gpp[carbonflux+'_f'] < 20) & (gpp[carbonflux+'_f'] > -20)]                           ####Parameter\n",
    "        gpp = gpp[(gpp[rei_res+'_f'] < 15) & (gpp[rei_res+'_f'] > -15)] \n",
    "\n",
    "        gpp_mean = gpp[['TA_f','VPD_f','SW_IN_f']]\n",
    "        gpp_sum  = gpp[[carbonflux+'_f',rei_gpp+'_f',rei_res+'_f',fal_gpp+'_f',fal_res+'_f',las_gpp+'_f',las_res+'_f']] * 12 * 30 * 60 /1000000\n",
    "\n",
    "        gpp_mean = gpp_mean.reset_index()\n",
    "        gpp_sum  = gpp_sum.reset_index()\n",
    "\n",
    "        gpp_mean['date']  =  gpp_mean['TIMESTAMP_START'].dt.date\n",
    "        gpp_sum ['date']  =  gpp_sum['TIMESTAMP_START'].dt.date\n",
    "\n",
    "        gpp_mean.replace(-9999, np.nan, inplace=True)\n",
    "        gpp_sum.replace(-9999, np.nan, inplace=True) \n",
    "\n",
    "        gpp_mean_daily = gpp_mean.groupby('date').mean()\n",
    "        gpp_sum_daily  = gpp_sum.groupby('date').sum()\n",
    "\n",
    "        df_gpp = pd.concat([gpp_mean_daily, gpp_sum_daily], axis=1)\n",
    "\n",
    "        # identify beggining and end of the time series\n",
    "        df_time = df_gpp.reset_index()\n",
    "        time1 = df_time.iloc[0, 0]\n",
    "        time2 = df_time.iloc[df_gpp.shape[0] -1,0]\n",
    "\n",
    "        # create time series with daily frequency (Not needed if usinf gap filled variables)\n",
    "        time_series = pd.date_range(time1, time2, freq=\"D\")\n",
    "        time_series = pd.DataFrame(time_series).rename(columns={0: 'date'}).set_index('date')\n",
    "        df_gpp_time = pd.merge(left= time_series, right = df_gpp,\n",
    "                                 how=\"left\", left_index = True , right_index = True)\n",
    "\n",
    "        # smoth time series\n",
    "        df_gpp_smoth  = df_gpp_time.rolling(3).mean()                                                ####Parameter\n",
    "        df_gpp_smoth  = df_gpp_smoth.interpolate(method='akima', order=1, limit_direction ='forward')\n",
    "        \n",
    "        # save file of daily GPP\n",
    "        df_gpp_smoth.to_csv(outdir + \"/GPP/GPP_daily.txt\")\n",
    "\n",
    "        t72   = ptime.time()\n",
    "        strin = ( '{:.1f} [minutes]'.format((t72 - t71) / 60.)                                                      \n",
    "                  if (t72 - t71) > 60.\n",
    "                  else '{:d} [seconds]'.format(int(t72 - t71))\n",
    "                )\n",
    "        print('     Computed daily GPP in ', strin) \n",
    "        \n",
    "    #********************************************************************************************************************************************************************* \n",
    "    # Finish Correction of Data with the Hesseflux package \n",
    "\n",
    "    t2   = ptime.time()                                                                                 #Change 38. Change legend of computation time.\n",
    "    strin = ( '{:.1f} [minutes]'.format((t2 - t1) / 60.)                                            \n",
    "              if (t2 - t1) > 60.\n",
    "              else '{:d} [seconds]'.format(int(t2 - t1))\n",
    "            )\n",
    "    print('Total time of correction of data ', strin) \n",
    "\n",
    "    #********************************************************************************************************************************************************************* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
